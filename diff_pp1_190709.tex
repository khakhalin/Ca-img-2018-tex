\title{Khakhalin. Graph analysis of collision detection networks}
%DIF LATEXDIFF DIFFERENCE FILE



%\documentclass[twocolumn]{article}
\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[top=0.85in,left=1.5in,right=1.5in,footskip=0.75in]{geometry} % For one-column version
%\usepackage[top=0.85in,left=1in,right=1.0in,footskip=0.75in]{geometry} % For two-column version
% marginparwidth=2in


\usepackage[round, numbers, authoryear]{natbib} % Reference manager, round citations, unsorted
%\usepackage[super]{natbib} % Nature-style citations
%\setcitestyle{citesep={,}} % For nature-style, comma instead of ;

%\usepackage[switch,pagewise]{lineno} % Numbered lines for two columns
\usepackage[pagewise]{lineno} % Numbered lines for one column

%\usepackage{adjustbox} % To change margins around a table. Doesn't work?
\usepackage{tabularx} % To set table column width, if needed
\usepackage{rotating} % To write text sidewise
\usepackage{xcolor}
\renewcommand{\linenumberfont}{\normalfont\bfseries\small\color{lightgray}}
\definecolor{linkcolor}{rgb}{0.2,0.6,0.7} % Neuron-style link color
\usepackage[colorlinks=true,citecolor=linkcolor,urlcolor=blue]{hyperref}%
%\usepackage{url}

% improves typesetting in LaTeX
\usepackage{microtype}
%\DisableLigatures[f]{encoding = *, family = * }

% this package is supposed to give access to upright mu symbol via \micro
%\usepackage{siunitx} % didn't work for some reason
\usepackage{amsmath} % to enable \text tag
\usepackage{amssymb} % to enable \leqslant

% text layout
\raggedright
%\textwidth 6in 
%\textheight 9in
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

% adjust caption style
\usepackage[aboveskip=5mm,labelfont=bf,labelsep=period,singlelinecheck=off]{caption}

% this is required to include graphics
\usepackage{graphicx}

% Multicolumns
\setlength{\columnsep}{1cm}

% Titles
\usepackage{titlesec}
\titlespacing{\section}{0pc}{0.5pc}{0pc}

% Headers and footers
\usepackage{fancyhdr}
\pagestyle{fancy}
\rhead{Khakhalin AS. Graph analysis of looming-selective networks. Page \thepage}
\cfoot{} % To kill footer page numbers

% Set font:
\usepackage{helvet} % Helvetica: most conservative
%\usepackage{tgheros} % TEX Gyre Heros: a bit rounder, but poor
%\usepackage{lmodern} % Fancy, nice, but unusual
\renewcommand{\familydefault}{\sfdefault} % Force this font
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF UNDERLINE PREAMBLE %DIF PREAMBLE
\RequirePackage[normalem]{ulem} %DIF PREAMBLE
\RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1} %DIF PREAMBLE
\providecommand{\DIFaddtex}[1]{{\protect\color{blue}{#1}}} %DIF PREAMBLE
\providecommand{\DIFdeltex}[1]{{\protect\color{red}\sout{#1}}}                      %DIF PREAMBLE
%DIF SAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddbegin}{} %DIF PREAMBLE
\providecommand{\DIFaddend}{} %DIF PREAMBLE
\providecommand{\DIFdelbegin}{} %DIF PREAMBLE
\providecommand{\DIFdelend}{} %DIF PREAMBLE
%DIF FLOATSAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddFL}[1]{\DIFadd{#1}} %DIF PREAMBLE
\providecommand{\DIFdelFL}[1]{\DIFdel{#1}} %DIF PREAMBLE
\providecommand{\DIFaddbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFaddendFL}{} %DIF PREAMBLE
\providecommand{\DIFdelbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFdelendFL}{} %DIF PREAMBLE
%DIF HYPERREF PREAMBLE %DIF PREAMBLE
\providecommand{\DIFadd}[1]{\texorpdfstring{\DIFaddtex{#1}}{#1}} %DIF PREAMBLE
\providecommand{\DIFdel}[1]{\texorpdfstring{\DIFdeltex{#1}}{}} %DIF PREAMBLE
\newcommand{\DIFscaledelfig}{0.5}
%DIF HIGHLIGHTGRAPHICS PREAMBLE %DIF PREAMBLE
\RequirePackage{settobox} %DIF PREAMBLE
\RequirePackage{letltxmacro} %DIF PREAMBLE
\newsavebox{\DIFdelgraphicsbox} %DIF PREAMBLE
\newlength{\DIFdelgraphicswidth} %DIF PREAMBLE
\newlength{\DIFdelgraphicsheight} %DIF PREAMBLE
% store original definition of \includegraphics %DIF PREAMBLE
\LetLtxMacro{\DIFOincludegraphics}{\includegraphics} %DIF PREAMBLE
\newcommand{\DIFaddincludegraphics}[2][]{{\color{blue}\fbox{\DIFOincludegraphics[#1]{#2}}}} %DIF PREAMBLE
\newcommand{\DIFdelincludegraphics}[2][]{% %DIF PREAMBLE
\sbox{\DIFdelgraphicsbox}{\DIFOincludegraphics[#1]{#2}}% %DIF PREAMBLE
\settoboxwidth{\DIFdelgraphicswidth}{\DIFdelgraphicsbox} %DIF PREAMBLE
\settoboxtotalheight{\DIFdelgraphicsheight}{\DIFdelgraphicsbox} %DIF PREAMBLE
\scalebox{\DIFscaledelfig}{% %DIF PREAMBLE
\parbox[b]{\DIFdelgraphicswidth}{\usebox{\DIFdelgraphicsbox}\\[-\baselineskip] \rule{\DIFdelgraphicswidth}{0em}}\llap{\resizebox{\DIFdelgraphicswidth}{\DIFdelgraphicsheight}{% %DIF PREAMBLE
\setlength{\unitlength}{\DIFdelgraphicswidth}% %DIF PREAMBLE
\begin{picture}(1,1)% %DIF PREAMBLE
\thicklines\linethickness{2pt} %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,0){\framebox(1,1){}}}% %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,0){\line( 1,1){1}}}% %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,1){\line(1,-1){1}}}% %DIF PREAMBLE
\end{picture}% %DIF PREAMBLE
}\hspace*{3pt}}} %DIF PREAMBLE
} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbegin}{\DIFaddbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddend}{\DIFaddend} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbegin}{\DIFdelbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelend}{\DIFdelend} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbegin}{\DIFOaddbegin \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbegin}{\DIFOdelbegin \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbeginFL}{\DIFaddbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddendFL}{\DIFaddendFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbeginFL}{\DIFdelbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelendFL}{\DIFdelendFL} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbeginFL}{\DIFOaddbeginFL \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbeginFL}{\DIFOdelbeginFL \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF

\begin{document} % --------------------- Document start

%TC:ignore 
% The comment above is for textcount to ignore this text.
% It will ignore everything until the endignore pair, and so on.
% We need to get down to 4500 words in the main text.

% \linenumbers % Comment to suppress line numbers

% title goes here:
%\twocolumn[
\begin{flushleft}
{\Large
\textbf\newline{Graph analysis of looming-selective networks in the tectum, and its replication in a simple computational model}
}
\newline
% authors go here:
\\
Arseny S. Khakhalin\textsuperscript{1,*}
\\
\bigskip
{1} Biology Program, Bard College, Annandale-on-Hudson, NY. 

* Correspondence: khakhalin@bard.edu


% TODO

% Fix figures

% Change Fig. to Figure everywhere

% Sort references by year, to make citations correct

\section*{Abstract}
% 195 words
Looming stimuli evoke behavioral responses in most \DIFdelbegin \DIFdel{sighted }\DIFdelend animals, yet the mechanisms of looming detection in vertebrates are poorly understood. Here we hypothesize that looming detection in the tectum may rely on spontaneous emergence of synfire chains: groups of neurons connected to each other in the same sequence in which they are activated during a loom. We then test some specific consequences of this hypothesis. First, we use high-speed calcium imaging to reconstruct \DIFaddbegin \DIFadd{functional }\DIFaddend connectivity of small networks within the tectum of \DIFdelbegin \textit{\DIFdel{Xenopus}} %DIFAUXCMD
\DIFdelend \DIFaddbegin \DIFadd{Xenopus }\DIFaddend tadpoles. We report that reconstructed directed graphs are clustered and hierarchical, that their modularity increases in development, and that looming-selective cells tend to \DIFdelbegin \DIFdel{act as activation sinks }\DIFdelend \DIFaddbegin \DIFadd{collect activation }\DIFaddend within these graphs. Second, we describe spontaneous emergence of looming selectivity in a computational developmental model of the tectum, governed by both synaptic and intrinsic plasticity, and driven by structured visual inputs. We show that synfire chains contribute to looming detection in the model; that structured inputs are critical for the emergence of selectivity, and that biological tectal networks follow most, but not all predictions of the model. Finally, we propose a conceptual scheme for understanding the emergence and fine-tuning of collision detection in developing aquatic animals.
%TC:ignore 
\bigskip

\end{flushleft} % Only relevant for two-column documents, but doesn't hurt
%] % End of one-column region

\section*{Introduction}

% Create tension. Build value. Why are you reading this? What is the point? How will reading this benefit the community you belong to, or you personally? Instability (tension). Don't describe the status quo; don't build a foundation, describe the problem. Invest in tension (use: but, however, although, despite, paradox, inconsistency, unexpected, surprising, contradiction). Why does it matter? Gap in knowledge is good, but really we only care about problems that cost something.

Few sensory stimuli are as ill boding for the animal as a visual loom. A retinal projection that is small, but is quickly growing in size, may promise a painful collision, or a meeting with a predator, so it inherently calls for \DIFdelbegin \DIFdel{an }\DIFdelend action: an avoidance maneuver, or a defensive response. Moreover, to be meaningful, looming detection has to be fast. Not surprisingly, it is described in virtually every animal that uses vision, from insects to primates \citep{Pereira2016}. And yet, while our understanding of looming detection in insects has recently improved \citep{rind2016locust,von2017fruitfly}, the mechanisms that underlie collision avoidance in vertebrates are still unclear.

For vertebrates, it is well established that looming detection primarily happens in a midbrain region known as superior colliculus in mammals, and optic tectum in all other clades \citep{frost2004review, liu2011cat, khakhalin2014, dunn2016escapesZF}. It is not known however how midbrain circuits perform the computations required for collision avoidance, and it is \DIFdelbegin \DIFdel{not clear }\DIFdelend \DIFaddbegin \DIFadd{still unclear }\DIFaddend whether looming avoidance \DIFdelbegin \DIFdel{and collision avoidance in vertebrates are }\DIFdelend \DIFaddbegin \DIFadd{in vertebrates is }\DIFaddend innate (and so possibly hardwired), or whether \DIFdelbegin \DIFdel{they need }\DIFdelend \DIFaddbegin \DIFadd{it needs }\DIFaddend to be learned. Finally, to navigate, animals need to calculate not just \textit{whether} a collision is about to happen, but \textit{where} a looming stimulus is coming from. While it is known that the tectum harbors a retinotopic map of the visual field \citep{mclaughlin2003retinotopic, ruthazer2004map}, there are several competing theories about how looming detectors may be distributed within this map \citep{frost2004review}.

What calculations may \DIFdelbegin \DIFdel{underlie collision avoidance, in principle }\DIFdelend \DIFaddbegin \DIFadd{in principle underlie looming detection}\DIFaddend ? Across species, looming detection relies on a diverse set of mechanisms that include dimming detectors \DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\citep{ishikane2005, munch2009}}\hspace{0pt}%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citep{ishikane2005, munch2009, heap2018dimming}}\hspace{0pt}%DIFAUXCMD
}\DIFaddend , integration of opponent motion \citep{klapoetke2017looming}, and competitive spike-frequency adaptation \citep{peron2009adaptation, fotowat2011multiplexing}. Even within a single clade of anuran amphibians (frogs), animals seem to employ at least two competing approaches to loom detection: a non-linear response to dimming-induced retinal oscillations \citep{baranauskas2012}, and a rebound of recurrent activity during edge expansion \citep{khakhalin2014, jang2016}. Moreover, at least in some cases, competing mechanisms may lead to different motor responses, as described in insects \citep{card2008tradeoffs, chan2013avoidance}, fish \DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\citep{budick2000repertoire, burgess2007twoescapes, portugues2009behaviors, temizer2015pathway, bhattacharyya2017assessment}}\hspace{0pt}%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citep{budick2000repertoire, burgess2007twoescapes, portugues2009behaviors, temizer2015pathway, bhattacharyya2017assessment, heap2018dimming}}\hspace{0pt}%DIFAUXCMD
}\DIFaddend , and tadpoles \citep{khakhalin2014}.

It may seem puzzling that the brain would use several conflicting approaches to solve one practical problem, but this arrangement may make sense developmentally. Simple, crude ways of \DIFdelbegin \DIFdel{identifying }\DIFdelend \DIFaddbegin \DIFadd{detecting }\DIFaddend dangerous stimuli can be used to train more sophisticated \DIFdelbegin \DIFdel{and efficient networks, capable of nuanced }\DIFdelend \DIFaddbegin \DIFadd{networks, to support efficient }\DIFaddend sensory analysis at later stages of development \DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\citep{marblestone2016deeplearning,zador2019critique}}\hspace{0pt}%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citep{marblestone2016deeplearning, zador2019critique}}\hspace{0pt}%DIFAUXCMD
}\DIFaddend . We argue that young aquatic animals may use "hardwired" dimming receptors \DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\citep{baranauskas2012} }\hspace{0pt}%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citep{baranauskas2012, heap2018dimming} }\hspace{0pt}%DIFAUXCMD
}\DIFaddend both to avoid collisions early in development \citep{dong2009}, and to "bootstrap" motion-dependent networks in the tectum. In older animals, nuanced motion-detecting networks could serve as a first line of defense, identifying early phases of looming, and allowing \DIFdelbegin \DIFdel{for fine }\DIFdelend \DIFaddbegin \DIFadd{precise }\DIFaddend course corrections \citep{khakhalin2014, bhattacharyya2017assessment}, while dimming detectors remain as a backup, mediating last-moment, less coordinated responses. Moreover, every time collision avoidance is not performed perfectly, sensorimotor networks can be refined, based on the mechanosensory inputs from the lateral line and from the skin \citep{felch2016, helmbrecht2018topography}.

In this study, we use high-speed calcium imaging and functional connectivity reconstruction to search for looming detectors within recurrent networks in the developing tectum of \textit{Xenopus} tadpoles. Tadpole tectum is uniquely suitable for studies of sensory integration, as it is excessively plastic \citep{pratt2007intrinsic, busch2019}, strongly interconnected \citep{james2015}, and develops reliable looming selectivity within about a week \citep{dong2009, khakhalin2014}, as tadpoles mature from Nieuwkoop stage \DIFdelbegin \DIFdel{46 }\DIFdelend \DIFaddbegin \DIFadd{45 }\DIFaddend to stage 49. We hypothesize that looming detectors may emerge in development, when connections between tectal cells are reshaped by patterned visual stimulation. Synapses in the tectum of young tadpoles exhibit spike-time-dependent plasticity (STDP; \citealt{zhang1998stdp, mu2006stdp, vislay2006rf, richards2010stdp}) that is \DIFdelbegin \DIFdel{known }\DIFdelend \DIFaddbegin \DIFadd{expected }\DIFaddend to promote the development of \DIFaddbegin \DIFadd{so-called }\DIFaddend synfire chains: groups of neurons, sequentially connected to one another \citep{fiete2010chains, zheng2014synfire}. Synfire chains are selective for inputs that activate neurons in the same sequence in which they are connected \citep{clopath2010stdpcoding}, turning them into temporal pattern detectors. We hypothesize that it is this ability to "resonate" with specific temporally patterned inputs that underlies looming behavior in late pre-metamorphic tadpoles. 

There are two potential ways to test this hypothesis. First, we can look for synfire chains in the tectum; see whether they get selectively activated in response to looming stimuli, and check whether their connectivity and selectivity patterns change with development. Luckily, signal transmission in \textit{Xenopus} tectal neurons is relatively slow, with synaptic transmission and spiking initiation taking up to 10 ms \citep{ciarleglio2015, jang2016, busch2019}, so if synfire chains in the tectum exist, they may be directly observable with fast calcium imaging, operating at rates of $\sim$100 frames/s. Second, if synfire chains can serve as a basis for looming detection, we can expect looming selectivity to spontaneously emerge in a model of a developing tectum governed by spike-time-dependent plasticity \citep{gao2015simplicity, pietri2017emergence}. We can then use this model to \DIFdelbegin \DIFdel{build verifiable predictions about looming detectors in biologicall networks, and }\DIFdelend explore the range of developmental rules that enable looming selectivity \citep{linderman2017constrain, bassett2018models}, similar to \DIFdelbegin \DIFdel{how computational models recently helped to study the emergence }\DIFdelend \DIFaddbegin \DIFadd{recent advances in modeling the development }\DIFaddend of visual receptive fields \citep{bashivan2018neural}, grid cells \citep{banino2018grid}, and decision circuits \citep{haesemeyer2018convergent}. 

With this in mind, \DIFdelbegin \DIFdel{here we ask five }\DIFdelend \DIFaddbegin \DIFadd{we ask three }\DIFaddend specific questions. \DIFdelbegin \DIFdel{In the first half of the paper}\DIFdelend \DIFaddbegin \DIFadd{First}\DIFaddend , we test whether it is possible to reconstruct functional connectivity in tectal networks from calcium imaging recordings. \DIFdelbegin \DIFdel{We then }\DIFdelend \DIFaddbegin \DIFadd{Then we }\DIFaddend investigate the topology of \DIFdelbegin \DIFdel{connectivity graphs in the tectum, }\DIFdelend \DIFaddbegin \DIFadd{reconstructed connectivity graphs, and }\DIFaddend compare these graphs between younger and older tadpoles\DIFdelbegin \DIFdel{, and try to assess whether these graphs may support collision detection}\DIFdelend . Finally, \DIFdelbegin \DIFdel{in the second half of the paper, }\DIFdelend we check whether looming detection emerges in a computational model, and whether the statistics of simulated \DIFdelbegin \DIFdel{looming-selective }\DIFdelend networks matches that of biological networks.
\DIFdelbegin \DIFdel{We show that our model predicts some, but not all aspects of topology, functionality, and development of looming-sensitive networks, making it hard to draw simple conclusions. Nevertheless, we report several new findings, including some surprising positive and negative results.
}\DIFdelend 

\section*{Results}

For all \DIFdelbegin \DIFdel{types of analysis, in the main text we provide their names and ways to interpret their results}\DIFdelend \DIFaddbegin \DIFadd{analyses, we name and interpret them in the text}\DIFaddend , but leave \DIFaddbegin \DIFadd{precise }\DIFaddend definitions for the Methods section. \DIFdelbegin \DIFdel{For statistical tests, p-values }\DIFdelend \DIFaddbegin \DIFadd{P-values }\DIFaddend are reported without correction, and \DIFdelbegin \DIFdel{we interpret them }\DIFdelend \DIFaddbegin \DIFadd{are interpreted }\DIFaddend according to Fisher, rather than Neyman-Pearson philosophy \citep{greenland2016, amrhein2019pval}\DIFdelbegin \DIFdel{. At the interpretation step, we pay more attention }\DIFdelend \DIFaddbegin \DIFadd{, with more attention payed }\DIFaddend to hypotheses supported by several alternative analyses. All code and summary data are available at:  \url{https://github.com/khakhalin/Ca-Imaging-and-Model-2018} .

%DIF <  Todo: once we know where the data is hosted, update
%DIF >  Todo: once we know where the raw data is hosted, update

We performed calcium imaging experiments \DIFaddbegin \DIFadd{with cell-permeant Oregon Green Bapta }\DIFaddend in 14 stage 45-46, and 16 stage 48-49 tadpoles, recording responses from 128$\pm$40 tectal cells \DIFaddbegin \DIFadd{per experiment }\DIFaddend (between 84 and 229\DIFdelbegin \DIFdel{in individual experiments). Here }\DIFdelend \DIFaddbegin \DIFadd{; here }\DIFaddend and below “$\pm$” after the mean denotes standard deviation\DIFaddbegin \DIFadd{)}\DIFaddend . Unless stated otherwise, sample sizes $n=$ 14 and 16 animals for stage 46 and 49 tadpoles apply to all analyses between younger and older animals in this study. To each tadpole, we presented a sequence of three different stimuli, always in the same order: a dark-on-light "Looming" stimulus, followed by a full-field dark "Flash", followed by a spatially "Scrambled" looming stimulus (Figure 1A). \DIFaddbegin \DIFadd{For a looming stimulus, a dark circle on a light background took 1 s to grow to an angular size of $\sim$70$^\circ$ \mbox{%DIFAUXCMD
\citep{khakhalin2014}}\hspace{0pt}%DIFAUXCMD
. }\DIFaddend Scrambled stimuli were \DIFaddbegin \DIFadd{dynamically }\DIFaddend identical to looming, except that the visual field was split into a 7x7 grid of square tiles, and these tiles were randomly rearranged\DIFdelbegin \DIFdel{in space}\DIFdelend . In total\DIFaddbegin \DIFadd{, to every animal }\DIFaddend we presented 60$\pm$11 stimuli \DIFdelbegin \DIFdel{to every animal, which means that a stimulus of every type was presented }\DIFdelend \DIFaddbegin \DIFadd{(}\DIFaddend 20$\pm$4 \DIFdelbegin \DIFdel{times}\DIFdelend \DIFaddbegin \DIFadd{stimuli of each of three types)}\DIFaddend . We recorded high speed (84 frames/s) calcium imaging signals \citep{xu2011, truszkowski2017} from one layer of “deep” principal tectal neurons in the tectum (Figure 1B,C); extracted fluorescence traces (Figure 1D,E), and inferred instantaneous spiking rate for each neuron within every frame (Figure 1F,G).

\begin{figure*}[t!]
\includegraphics[width=\linewidth]{fig1.pdf}
\caption{
Experimental design overview. \textbf{A}. Visual stimulation: four representative frames from each stimulus type. \textbf{B}. Schematic of the preparation, with stimulation fiber on the left, and microscope objective on top. \textbf{C}. View of the optic tectum during calcium imaging recording. \textbf{D}. Regions of interest (cells) for brain shown in C, with darker markers representing cells with stronger responses. Labels in the right top corner mark Rostro-Caudal and Lateral-Medial axes. \textbf{E}. Typical fluorescence responses to Flash (F), Scrambled (S) and Looming (L) stimuli from three cells in the tectum. \textbf{F}. Spiking estimations \DIFaddbeginFL \DIFaddFL{(via deconvolution) }\DIFaddendFL for these fluorescence traces. \textbf{G}. Average full-brain responses to stimuli of every type, for one representative experiment, shown with a 95\% confidence interval band. }
\end{figure*}

\subsection*{Responses and stimulus selectivity}

As previously reported for electrophysiology experiments \citep{khakhalin2014}, responses to flashes were fast, with a sharp peak, and little recurrent activation after the peak, while responses to looming stimuli were slower, and were followed by a strong recurrent activation (Figure 1G). Responses to both looming and scrambled stimuli were highly variable from one animal to another (Figure 2A), which may indicate either an inherent variability of network configurations, or different levels of inhibition across preparations.

The \textbf{total \DIFaddbegin \DIFadd{spiking }\DIFaddend output} of observed networks \DIFaddbegin \DIFadd{(the sum of all spikes of all neurons during a visual response) }\DIFaddend tended to be higher \DIFdelbegin \DIFdel{in response to }\DIFdelend \DIFaddbegin \DIFadd{for }\DIFaddend looming stimuli than \DIFdelbegin \DIFdel{to }\DIFdelend \DIFaddbegin \DIFadd{for }\DIFaddend flashes (Figure 2B; on average, 39$\pm$29\% higher for younger; 25$\pm$25\% higher for older tadpoles; $p_{t1}=$ 2e-4 and 1e-3 respectively)\DIFdelbegin \DIFdel{, with no change in this preference }\DIFdelend \DIFaddbegin \DIFadd{; this preference did not change }\DIFaddend in development ($p_t$=0.15). There was no discernible difference in response amplitudes between looming and scrambled stimuli (average difference of $-$0.03$\pm$0.15 and $-$0.01$\pm$0.20; $p_{t1}=$ 0.45 and 0.78 for younger and older animals respectively; no difference in development $p_t=$ 0.80). These results \DIFdelbegin \DIFdel{support our prior observation that total }\DIFdelend \DIFaddbegin \DIFadd{match our prior reports that the overall strength of }\DIFaddend tectal responses in tadpoles \DIFdelbegin \DIFdel{depend }\DIFdelend \DIFaddbegin \DIFadd{depends }\DIFaddend mostly on the dynamics of visual stimuli, rather than on their geometry \citep{khakhalin2014, jang2016}.

\begin{figure*}[t!]
\includegraphics[width=\linewidth]{fig2.pdf}
\caption{
Selectivity analysis. \textbf{A}. Average brain responses (sum of activity of all recorded cells over time), for each stimulus type, in each experiment, superimposed. Black lines show grand averages across all experiments, separately for younger and older tadpoles. \textbf{B}. Cumulative full brain responses (an integral under the curves shown in A) for each experiment, across three stimulus types. \textbf{C}. A sample selectivity map, with hue coding preference for Flash (more red), Scrambled (more green), and Looming (more blue) stimuli. \textbf{D}. Average histograms of cell selectivity distributions, for younger (violet) and older (black) tadpoles; uncertainty bars show 95\% confidence intervals, for clarity given in only one direction. Younger tadpoles had more highly selective cells than older tadpoles. \textbf{E}. A correlation between Flash-Scramble and Flash-Looming selectivity of individual cells in a typical experiment. \textbf{F}. A correlation between Scramble-Looming and Flash-Looming selectivity of individual sells in the same sample experiment. \textbf{G}. Stimulus encoding in different experiments, for two developmental stages.}
\end{figure*}

Were there specialized looming detectors in the tectum? To quantify \textbf{stimulus selectivity} (Figure 2C), for each tectal cell we calculated Cohen’s effect size $d$ between cumulative responses to different stimuli. We considered two \DIFdelbegin \DIFdel{measures }\DIFdelend \DIFaddbegin \DIFadd{types }\DIFaddend of selectivity: that for "Looming over Flash" (a type of selectivity that may rely on both stimulus dynamics and its spatial organization), and "Looming over Scrambled" (that can only rely on spatial properties of stimuli, as by design \DIFdelbegin \DIFdel{they }\DIFdelend \DIFaddbegin \DIFadd{Looming and Scrambled stimuli }\DIFaddend had the same dynamics). Looking at cumulative responses was of course a simplification, as in real life \DIFdelbegin \DIFdel{, }\DIFdelend animals respond to stimuli \DIFdelbegin \DIFdel{even as }\DIFdelend \DIFaddbegin \DIFadd{while }\DIFaddend they are still unrolling \citep{peron2009adaptation, khakhalin2014}, and not after a timed 1 second-long presentation. We however had no way to justify \DIFdelbegin \DIFdel{any }\DIFdelend \DIFaddbegin \DIFadd{a }\DIFaddend specific way of dynamic thresholding, and so opted for the simplest \DIFdelbegin \DIFdel{possible }\DIFdelend approach.

On average, tectal cells were selective for looming stimuli (Figure 2D; \DIFdelbegin \DIFdel{$d$ }\DIFdelend \DIFaddbegin \DIFadd{mean Cohen's $\bar d$ }\DIFaddend of 0.67$\pm$0.50 and 0.46$\pm$0.47 in younger and older tadpoles respectively\DIFaddbegin \DIFadd{)}\DIFaddend . Here we first calculated selectivity $d$ for every cell, then found average $\bar d$ within each brain\DIFdelbegin \DIFdel{, and finally compared values of $\bar d$ between experiments in younger and older tadpoles}\DIFdelend . There was no difference \DIFdelbegin \DIFdel{in cell selectivity }\DIFdelend between stages in terms of \DIFdelbegin \DIFdel{both mean cell selectivity and variance }\DIFdelend \DIFaddbegin \DIFadd{either per-experiment mean selectivity $\bar d$, nor within-experiment variances }\DIFaddend in selectivity ($p_t=$ 0.3, 0.3). The share of cells that responded to looming stimuli stronger than to flashes also did not change in development (84$\pm$23\%, 77$\pm$21\%; $p_t=$ 0.4). Compared to younger brains, older brains had fewer highly selective cells (Figure 2D, right side of the curve). The gap between top-selective (90th percentile) and median selective cells was larger in younger (0.75$\pm$0.26) than in older tadpoles (0.53$\pm$0.27; $p_t=$ 0.03), as selectivity distributions in younger tadpoles were more positively skewed ($p_t=$ 0.01). These results were unexpected, as older tadpoles perform better in collision avoidance tests \citep{dong2009}, and so we expected them to develop a subset of \DIFaddbegin \DIFadd{specialized }\DIFaddend looming-selective cells, as described in adult frogs \citep{nakagawa2010otneurons, baranauskas2012}, and other vertebrates \citep{wang1992pigeon, wu2005pigeon, liu2011cat}. Yet in our experiments, a subpopulation of strongly selective cells not only did not expand in older animals, but \DIFdelbegin \DIFdel{actually }\DIFdelend became less prominent\DIFaddbegin \DIFadd{, somewhat similar to developmental changes described in the visual cortex \mbox{%DIFAUXCMD
\citep{rochefort2009sparsification}}\hspace{0pt}%DIFAUXCMD
}\DIFaddend .

We then considered a second, more computationally demanding definition of selectivity: a preference for spatially organized looming \DIFaddbegin \DIFadd{stimuli }\DIFaddend over scrambled stimuli. On average, tectal cells did not \DIFdelbegin \DIFdel{have }\DIFdelend \DIFaddbegin \DIFadd{show }\DIFaddend a preference between these two stimuli (average selectivity of $-$0.07$\pm$0.33 in younger tadpoles, $-$0.04$\pm$0.49 in older ones; no change in development $p_t=$ 0.9). The share of cells that responded to looming stronger than to scrambled was at a chance level for both developmental stages (46$\pm$31\%, 48$\pm$37\%, $p_t=$ 0.9), and there was no change in either within-brain variance of this selectivity ($p_t=$ 0.9), or the 90$-$50 percentile asymmetry of values ($p_t=$ 0.8).

We found that selectivity for scrambled stimuli over flashes correlated with selectivity for looming stimuli over flashes (Figure 2E) in both developmental groups: average within-brain correlation coefficients $r=$ 0.82$\pm$0.13, $p_{1t}=$3e-12 for younger animals, and 0.75$\pm$0.18, $p_{t1}=$ 3e-11 older ones, with no change in development ($p_t=$ 0.3). \DIFdelbegin \DIFdel{On the contrary}\DIFdelend \DIFaddbegin \DIFadd{In contrast to that}\DIFaddend , the preference for looming over flashes did not correlate with preference for looming over scrambled (Figure 2F; $r=$ 0.03$\pm$0.29, $p_{1t}=$ 0.7 for stage 46; 0.13$\pm$.30, $p_{1t}=$ 0.1 for stage 49). This further suggests that \DIFdelbegin \DIFdel{most }\DIFdelend \DIFaddbegin \DIFadd{the majority of }\DIFaddend cells in the tectum responded to stimulus dynamics only, and did not specifically process its geometry.

Finally, as a holistic way to quantify tectal network selectivity, we looked at our ability to predict stimulus identity from \DIFdelbegin \DIFdel{recorded tectal responses in all }\DIFdelend \DIFaddbegin \DIFadd{all recorded responses in tectal }\DIFaddend cells \citep{avitan2016limitations}: a measure known as "\textbf{stimulus encoding}". We ran a multivariate logistic regression on \DIFdelbegin \DIFdel{one }\DIFdelend \DIFaddbegin \DIFadd{first }\DIFaddend half of the data, linking values of total responses of each cell in each trial to the type of stimulus used in this trial. Then we measured the quality of this linkage on the second half of recorded data (Figure 2G). \DIFdelbegin \DIFdel{The quality of }\DIFdelend \DIFaddbegin \DIFadd{We found that with the dynamics of responses disregarded (with predictions based only on the total spiking output of each cell), the quality of these }\DIFaddend prediction was rather low: 59$\pm$12\% for younger, and 62$\pm$13\% for older tadpoles, with no change in development ($p_t=$ 0.6).

To assess the \textbf{variability of response shapes} from one tectal cell to another within each brain, we performed exploratory factor analysis (principal component analysis \DIFdelbegin \DIFdel{, }\DIFdelend followed by a promax rotation) of responses to looming stimuli within each preparation. The first and second principal components explained on average 19$\pm$7\% and 4$\pm$1\% of variance in younger tadpoles, and 24$\pm$14\% and 3$\pm$1\% in older tadpoles, with second component encoding response timing (Figure 3A). Cells with early responses to looming stimuli were reliably grouped together somewhere within the recorded field (Figure 3B) in each one out of 30 experiments (see Methods). We assumed that early-responding cells grouped together because of the known retinotopic arrangement of receptive fields in the tectum \citep{ruthazer2004map}, that made tectal activation follow and reproduce the gradual unrolling of looming stimuli on the retina. Across cells, the latency of average response correlated with distance from the retinotopy center (Figure 3C; $r=$ 0.35 $\pm$ 0.24; correlations individually $p_r<$ 0.05 in 25/30 experiments), despite our latency estimations being quite noisy for low-amplitude cells (see Methods). Curiously, while visual projections to the tectum are known to be actively refined in development \citep{sakaguchi1985refinement, ruthazer2004map, munz2014hebbian}, it seems that the precision of functional retinotopic maps did not differ between younger and older tadpoles, as the values of correlation coefficients between cell position and early component prominence did not discernibly change in development ($r=$ 0.63 $\pm$ 0.21 and 0.57 $\pm$ 0.25 respectively, $p_t=$ 0.5), which is similar to prior reports in Zebrafish \citep{avitan2016limitations}.

Knowing where the center of a looming stimulus was projected within the tectum, we could check whether looming-selective cells tended to be found in the center of the expanding activation area (as it would be expected if \DIFdelbegin \DIFdel{collision }\DIFdelend \DIFaddbegin \DIFadd{looming }\DIFaddend detectors formed a meta-retinotopic map; \citealt{frost2004review}), or at the periphery. We found that selectivity for looming over flash tended to decrease with distance from the projection center (Figure 3C) for both stage 45 (average $r=-$0.37$\pm$0.27; individual correlations $p_r<$0.05 in 12/14 experiments), and stage 49 tadpoles (average $r=-$0.09$\pm$0.35; $p_r<$0.05 in 12/16 experiments), indicating that looming-selective were often found in the center of the emerging spatial response. Similarly, at both developmental stages, selectivity decreased with response latency (stage 46: $p_r<$0.05 in 11/14 animals, average $r=-$0.29$\pm$0.11; stage 49: $p_r<$0.05 in 10/16 animals, average $r=-$0.16$\pm$0.21). Both correlations were weaker in older tadpoles ($p_t=$ 0.02 for a link between distance and selectivity, $p_t=$ 0.03 for a link between response latency and selectivity), suggesting a more spatially even distribution of looming-selective cells in older tectal network.

\begin{figure*}[t!]
\includegraphics[width=\linewidth]{fig3.pdf}
\caption{
Spatiotemporal organization of responses. \textbf{A}. Two first components (Principal Component Analysis with promax rotation) identified in tectal responses in a typical experiment.  \textbf{B}. Neurons from the same sample experiment as in A, shown as they were located in the tectum, and colored by the contribution of early (green) and late (purple) response components. Black cross shows the estimated position of the retinotopy center. \textbf{C}. Average change of Flash-Looming selectivity with distance from the retinotopy center, for stage 46 (left) and stage 49 (right) tadpoles; uncertainty bars show 95\% confidence intervals. \textbf{D}. Adjusted correlation matrix for instantaneous activation of different neurons across all stimuli in a sample experiment (same as in B). \textbf{E}. Ensembles identified from the correlation matrix. \textbf{F}. The number of ensembles, identified in younger and older tadpoles. \textbf{G}. Maximal modularity of ensemble partition. \textbf{H}. Ensemble spatial compactness.}
\end{figure*}

\subsection*{Neuronal ensembles}

We then asked whether certain tectal cells tended to respond or stay silent together, potentially indicating distributed processing in network ensembles \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citep{orger2016review}}\hspace{0pt}%DIFAUXCMD
}\DIFaddend . As a simplistic way to assess that, we performed factor analysis of trial-to-trial \textbf{population response variability}, and checked whether \DIFdelbegin \DIFdel{it }\DIFdelend \DIFaddbegin \DIFadd{its results }\DIFaddend changed in development, as it \DIFdelbegin \DIFdel{is known to happen }\DIFdelend \DIFaddbegin \DIFadd{happens }\DIFaddend for tectal spontaneous activity \citep{xu2011}. The total number of principle components needed to describe 80\% of variance across cells \citep{avitan2017spontaneous} was similar in stage 46 and 49 tadpoles \DIFdelbegin \DIFdel{, with a possibility of mild increase in response richness in older animals ($p>$0.05 for each stimulus type alone, but consistent across stimuli: }\DIFdelend \DIFaddbegin \DIFadd{(}\DIFaddend 51$\pm$14 and 65$\pm$28, $p_t$=0.1 for looming; 49$\pm$12 and 62$\pm$32, $p_t$=0.2 for flashes; 51$\pm$14 and 64$\pm$26, $p_t$=0.1 for scrambled).

To better describe and visualize network activation variability, we looked for \textbf{ensembles} of tectal cells using spectral clustering \citep{thompson2016ensembles}. Unlike for spontaneous activity, we could not easily aggregate activity states into clusters \citep{avitan2017spontaneous}, as activation in our networks was driven by shared sensory inputs. Instead, we subtracted average responses of each cell from its responses in individual trials, and calculated pairwise correlations on the remaining “anomalies” of trial-by-trial activation (Figure 3D). We turned these correlations into pairwise distances in a multidimensional space, ran a series of spectral clustering partitions \citep{ng2002spectral}, and of all possible partitions, picked the one that maximized spectral modularity on a similarity graph \citep{newman2006modularity, gomez2009community} (Figure 3E; see Methods for details). We found that the number of ensembles did not differ between younger and older tadpoles (Figure 3F; 10$\pm$5 in stage 45, 11$\pm$11 in stage 49; $p_t$=0.9), \DIFdelbegin \DIFdel{but }\DIFdelend \DIFaddbegin \DIFadd{although }\DIFaddend in older tadpoles, ensembles were more coordinated with each other,  producing lower values of network modularity (Figure 3G; 0.14$\pm$0.05 to 0.09$\pm$0.06, $p_t$=0.03). Tectal ensembles also tended to be spatially localized (Figure 3H), with cells within each ensemble located closer to each other on average than to cells from different ensembles, both in younger (29$\pm$9\% closer) and older animals (25$\pm$10\% closer; no difference in development $p_t$=0.3).

\subsection*{Network reconstruction}

\begin{figure*}[t!]
\includegraphics[width=\linewidth]{fig4.pdf}
\caption{
Connectivity reconstruction.  \textbf{A}. Delayed correlations between activities of different neurons in a sample experiment. \textbf{B}. Adjusted transfer entropy estimations for the same experiment. \textbf{C}. Estimated weighted adjacency matrix. \textbf{D}. Connectivity reconstruction, with location of each cell preserved. Node color indicates its selectivity. \textbf{E}. Same graph as in D, in visually optimized layout. Positions of individual nodes no longer represent the position of tectal cells. \textbf{F}. In-degree distribution for younger (violet) and older (black) tadpoles, presented in log-scale, with uncertainty bars showing 95\% confidence intervals. Observable changes for individual degrees ($p_t<$ 0.05) are marked with asterisks. \textbf{G}. Same as C, but for out-degree distributions. \textbf{H}. Network power $\gamma$ increased in development, as older tadpoles had a slightly sharper slope to their degree distributions. }
\end{figure*}

The high speed of imaging used in this study (84 frames/s) allowed us to look not just at instantaneous correlations between activation of individual neurons, but at the \textit{propagation} of signals through the network. To reconstruct network connectivity, we calculated pairwise transfer entropy \citep{gourevitch2007te, stetter2012te} between activity traces of individual cells. Intuitively, for each pair of neurons $i$ and $j$ we quantified the amount of additional information that past activity of neuron $i$ could offer to predict current activity of neuron $j$. This is conceptually similar to calculating a cross-correlation between the activity of neuron $i$ at each frame $t$, and the activity of neuron $j$ at each \DIFaddbegin \DIFadd{consecutive }\DIFaddend frame $t+1$ (Figure 4A), except that transfer entropy calculation \DIFaddbegin \DIFadd{(Figure 4B) }\DIFaddend does not make assumptions about the type of influences neuron $i$ may have over neuron $j$, and offers a lower inference noise \DIFdelbegin \DIFdel{(\mbox{%DIFAUXCMD
\citealt{stetter2012te}}\hspace{0pt}%DIFAUXCMD
; Figure 4B)}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citep{stetter2012te}}\hspace{0pt}%DIFAUXCMD
}\DIFaddend .

In our experiments, all tectal neurons received shared inputs from the eye, that recruited them in a similar manner in every trial. This \DIFdelbegin \DIFdel{complicated connectivity inference }\DIFdelend \DIFaddbegin \DIFadd{made the connectivity inference more complicated}\DIFaddend , as neurons could spike in a sequence both because they were connected, and because they received innervation from sequentially activated areas of the retina \citep{mehler2018lure}. To compensate for shared inputs, we randomly reshuffled trials for every neuron, calculated average transfer entropy on reshuffled data, and subtracted it from the value obtained on trial-matched data \citep{gourevitch2007te, wollstadt2014te}. In essence, it is similar to working with deviations from the average response, and quantifying whether these deviations tended to propagate through the network, from one neuron to another. The reshuffling step also allowed us to calculate $p$-values for each pair of neurons, and quantify how unusual the value of transfer entropy was, compared to a value arising from shared inputs alone.

We interpreted transfer entropy values as approximations of weights in a connectivity matrix $\mathbf{W}$, with $w_{ji}$ describing the strength of connection from neuron $i$ to neuron $j$ (but see \citealt{mehler2018lure}). We calculated $\mathbf{W}$ and corresponding $p$-values independently on looming, flash, and scrambled stimuli, and used these independent estimations to ensure some level of internal replication within each experiment (see Methods). Of all possible edges, only 1.6$\pm$1.4\% were found to be non-zero in all three independent analyses, which was discernibly higher than 0.6$\pm$0.09\% expected if edges were assigned at random (paired t-test $p_{tp}=$ 9e-9). In each experiment, we then introduced cut-offs on edge $p$-values, to exclude weak noisy edges from the connectivity graph (Figure 4C). As noise levels varied across preparations, we adjusted these cut-offs \citep{stetter2012te}, making the number of non-zero edges in each reconstruction equal to the number of recorded cells (Figure 4D,E). The effective $p$-value cut-offs were between 0.001 and 0.007 depending on the experiment (median of 0.004). \DIFaddbegin \DIFadd{Note also that the results described below are relatively insensitive to the precise assumptions of this approach to weigh thresholding.
}\DIFaddend 

The simplest statistical property of a network is its \textbf{degree distribution}: the share of nodes with different \DIFdelbegin \DIFdel{number }\DIFdelend \DIFaddbegin \DIFadd{numbers }\DIFaddend of incoming ($k_{in}$) and outgoing ($k_{out}$) connections. We compared rounded degree distributions between networks detected in younger and older tadpoles (Figure 4F,G), and found that older networks contained fewer unconnected cells ($k_{in}=$ 0, $p_t<$0.03) and fewer cells with high number of connections ($k_{in}=$ 5, $k_{out}=$ 6, $p_t=$ 0.01 in both cases), but more cells with intermediate number of connections ($k_{in}=$ 2, $p_t=$ 0.001, and $k_{out}=$ 2, $p_t=$ 0.04). When we approximated degree distributions (excluding $k=$ 0) with a power law \DIFdelbegin \DIFdel{(Figure 4H)}\DIFdelend \DIFaddbegin \DIFadd{$P(k) \sim k^{-\gamma}$}\DIFaddend , the power constant $\gamma$ was smaller in younger (1.48$\pm$0.19) than in older tadpoles (1.82$\pm$0.25, $p_t=$ 2e-4\DIFaddbegin \DIFadd{; Figure 4H}\DIFaddend ), consistent with a steeper drop from the rate of occurrence of weakly connected to highly connected cells. It suggests that older tectal networks had more chains of connected neurons (out- or in-degree of $k=$ 1) and forks ($k=$ 2), while younger neurons had more hyperconnected hubs ($k>$5) and unconnected nodes ($k=$ 0), which matches expectations for STDP-driven networks \citep{fiete2010chains}.

An unusual feature of our \DIFaddbegin \DIFadd{high-speed imaging }\DIFaddend protocol, compared to \DIFdelbegin \DIFdel{most calcium imaging techniques }\DIFdelend \DIFaddbegin \DIFadd{more common techniques that rely on genetically encoded calcium sensors and confocal microscopy}\DIFaddend , was that the signal-to-noise ratio varied greatly from one cell to another, depending on how far \DIFdelbegin \DIFdel{it }\DIFdelend \DIFaddbegin \DIFadd{the cell }\DIFaddend was from the focal plane, and how much dye it absorbed through the partially exposed membrane during staining. As a result, the share of cells with weak signals that appeared unconnected to the rest of the network varied across experiments, and was dependent on extraneous circumstances, such as the physical curvature of the preparation. To ensure that poorly resolved cells did not bias our conclusions too strongly, we restricted further network analysis to the largest weakly connected component of each network. There was no difference in the number of weakly connected components detected in younger and older tadpoles (50$\pm$14 and 50$\pm$26 for stages 45 and 49 respectively, $p_t=$ 0.9), but in older animals the largest weakly connected component included a \DIFaddbegin \DIFadd{slightly }\DIFaddend higher share of observed cells (50$\pm$6\% and 64$\pm$12\% for younger and older networks respectively; $p_t=$ 4e-4), \DIFdelbegin \DIFdel{which was to be expected given }\DIFdelend \DIFaddbegin \DIFadd{consistent with }\DIFaddend the change in degree distributions described above.

A known prediction for networks dominated by spike-time-dependent plasticity (STDP) is that with time, neuronal connections tend to become highly asymmetric \citep{pratt2008recurrent, richards2010stdp}. Indeed, if cells $i$ and $j$ are reciprocally connected, every time $j$ spikes after $i$, STDP would increase the weight \DIFdelbegin \DIFdel{$w_{ij}$ }\DIFdelend \DIFaddbegin \DIFadd{$w_{ji}$ (for a connection leading from $i$ to $j$)}\DIFaddend , but decrease the reciprocal weight \DIFdelbegin \DIFdel{$w_{ji}$ }\DIFdelend \DIFaddbegin \DIFadd{$w_{ij}$ }\DIFaddend \citep{abbott1996ltpsequence, fiete2010chains}. We found that for our data, \textbf{the share of bidirectional edges} (\DIFaddbegin \DIFadd{those }\DIFaddend with both $w_{ij}$ and $w_{ji}>$0) among all detected edges was smaller (0.3$\pm$0.3\%) than expected for random edge assignment in graphs of our size (0.4$\pm$0.1\%, paired $p_t=$ 0.02), indicating \DIFaddbegin \DIFadd{an }\DIFaddend asymmetric information flow in the tectum. Moreover, the share of bidirectional edges decreased in development, from 0.4$\pm$0.3\% in younger animals to 0.2$\pm$0.2\% in older animals ($p_t=$ 0.03), suggesting that STDP was still shaping emerging network topology at these developmental stages.

We then looked at whether connected cells were more likely to be located spatially closer to each other. We found that the \textbf{average distance between connected cells} was indeed shorter than for randomly selected cells \DIFdelbegin \DIFdel{: }\DIFdelend \DIFaddbegin \DIFadd{(}\DIFaddend 18$\pm$10\% shorter for stage 45, and 17$\pm$8\% shorter for stage 49 tadpoles\DIFdelbegin \DIFdel{(individually discernible }\DIFdelend \DIFaddbegin \DIFadd{; connected neurons were closer to each other }\DIFaddend with $p_t<$0.05 \DIFdelbegin \DIFdel{for }\DIFdelend \DIFaddbegin \DIFadd{in }\DIFaddend 13/14 and 16/16 \DIFaddbegin \DIFadd{individual }\DIFaddend experiments respectively). \DIFdelbegin \DIFdel{Contrary to our expectations, and in contrast to what is known about }\DIFdelend \DIFaddbegin \DIFadd{In contrast to }\DIFaddend visual inputs to the tectum \citep{tao2005refinement}, the intra-tectal connectivity did not become more compact in development ($p_t$=0.7). This \DIFdelbegin \DIFdel{suggests }\DIFdelend \DIFaddbegin \DIFadd{may suggest }\DIFaddend that tectal networks rely on relatively far-reaching recurrent connections to integrate visual information across the visual field \citep{baginskas2009recurrent, liu2016jumbo, jang2016}.

\subsection*{Network properties}

The best (and perhaps the only possible) way to compare two sets of \DIFaddbegin \DIFadd{irregular }\DIFaddend connectivity graphs to each other is to quantify their properties using a diverse set of network measures, and compare these values across groups. We did this, \DIFdelbegin \DIFdel{and also looked }\DIFdelend \DIFaddbegin \DIFadd{while also checking }\DIFaddend whether our inferred connectivity graphs were statistically unusual, by comparing their properties to that of matching randomized graphs \citep{ansmann2012surrogate}. To do so, we randomly rewired edges between nodes, while keeping the distribution of edge weights $w_{ji}$, and the number of non-zero edges \DIFdelbegin \DIFdel{adjacent to }\DIFdelend \DIFaddbegin \DIFadd{leaving }\DIFaddend each node (node \DIFdelbegin \DIFdel{degree}\DIFdelend \DIFaddbegin \DIFadd{out-degree}\DIFaddend ) fixed, as a case of degree-preserving rewiring \citep{maslov2002} generalized for weighted directed graphs. 

\begin{figure*}[t!]
\includegraphics[width=\linewidth]{fig5.pdf}
\caption{
Global network properties. Filled markers show values from experiments; empty markers show averaged values for matching rewired networks; black squares show averages for each group. Network diagrams to the right of each plot show cartoon representation of graphs with low and high values of respective network measures. Asterisks show statistically discernible differences ($p<$ 0.05). \textbf{A}. Global network efficiency was lower than expected for stage 46 tadpoles. \textbf{B}. Global clustering was higher than expected by chance, for both developmental stages. \textbf{C}. Network modularity increased in development.  \textbf{D}. Network hierarchy was higher than expected by chance, for both developmental stages. }
\end{figure*}

The measure of average "connectedness" in the graph, known as \textbf{network efficiency}, \DIFdelbegin \DIFdel{is defined as the }\DIFdelend \DIFaddbegin \DIFadd{quantifies the length of the }\DIFaddend average shortest path connecting two random nodes in the network \citep{latora2001efficiency, rubinov2010toolbox}. This value is high when short paths made of high-weight edges tend to connect any two randomly chosen nodes in the graph, making it easy for signals to propagate within the network; the value is low when some pairs of nodes are far from each other on a graph (Figure 5A). We found that network efficiency (0.004$\pm$0.002 for stage 46, 0.002$\pm$0.002 for stage 49 tadpoles) was lower than expected for a randomized network with a matching degree distribution ($d=-$0.3, paired $p_t=$ 0.04 and $d=-$0.3, paired $p_t=$ 0.06 for younger and older tadpoles respectively). It was not clear whether efficiency changed with age ($d=-$0.8, $p_t=$ 0.06).

The \textbf{global clustering coefficient} describes the small-scale heterogeneity in the network \citep{fagiolo2007}, and is defined as the relative frequency of two neighboring nodes being a part a triangle with a third node connected to both of them (Figure 5B). The value of clustering coefficient in our networks was very small (2.4$\pm$2.5 e-3 for stage 46, 1.5$\pm$1.6 e-3 for stage 49 animals), but slightly larger than expected in a randomly rewired network with same degree distribution ($d=$ 0.5 and 0.6, paired $p_t=$ 0.01 and 0.02 for younger and older animals). This means that neurons with more than two connections were likely to form clusters. There was no change in clustering in development ($d$=$-$0.4, $p_t$=0.3, Fig.).

Network \textbf{modularity} is the most commonly used measure of mesoscale network heterogeneity \citep{newman2006modularity, leicht2008community}. A network with high modularity can be split into a set of sub-networks, with higher density of connections within each sub-network, and weaker connections between them (Figure 5C). In our experiments, reconstructed tectal networks had similar, or slightly higher modularity\DIFdelbegin \DIFdel{than }\DIFdelend \DIFaddbegin \DIFadd{, compared to }\DIFaddend matching randomly rewired networks ($d=$ 0.2 and 0.3, paired $p_t=$ 0.2 and 0.06, for stages 46 and 49 respectively), and network modularity clearly increased in development ($d=$ 1.0, $p_t =$ 0.01).

\textbf{Flow hierarchy} is a measure of structural hierarchy in the network \citep{mones2012hierarchy}, assessed through flows of activation that propagate through it. We based our measure of hierarchy on the distribution of Katz centrality values (\citealt{katz1953original, fletcher2018katz}; see next section for definitions). Intuitively, hierarchy is high when a network has groups of "input" and "output" nodes, with connections between them largely pointing in the same direction, as it happens in layered feed-forward networks; hierarchy is weak in random networks, or networks that consist of cycles with no clear inputs and outputs \citep{czegel2015hierarchy}. We hypothesized that a network of dedicated looming detectors may exhibit flow hierarchy, with more edges leading from "feeder neurons" to "detector neurons". Indeed, tectal networks were more hierarchical than randomized networks with matching \DIFdelbegin \DIFdel{degrees }\DIFdelend \DIFaddbegin \DIFadd{degree }\DIFaddend distributions (Figure 5D; $d=$ 1.5 and 1.1, paired $p_t=$ 1e-04 and 1e-03 for younger and older tadpoles respectively). There was no difference in flow hierarchy in development ($d=-$0.3, $p_t=$ 0.4).

\subsection*{Selectivity Mechanisms}

Even though the exact architecture of collision-detecting tectal networks is unknown, it is safe to assume that looming-selective neurons have to integrate streams of information coming from different parts of the visual field. We therefore hypothesized that the \DIFdelbegin \DIFdel{position }\DIFdelend \DIFaddbegin \DIFadd{topological placement }\DIFaddend of selective neurons within \DIFdelbegin \DIFdel{connectivity graphs }\DIFdelend \DIFaddbegin \DIFadd{each connectivity graph }\DIFaddend would not be random \citep{timme2016degree}. To verify that, for each reconstructed network we tested correlations between looming selectivity of each neuron and values that quantify its \DIFdelbegin \DIFdel{place }\DIFdelend \DIFaddbegin \DIFadd{placement }\DIFaddend within the network, known as measures of \textbf{node centrality}.

\begin{figure*}[t!]
\includegraphics[width=\linewidth]{fig6.pdf}
\caption{
Local network properties (centrality) for cells selective for looming stimuli. \textbf{A}. Cells with higher Katz centrality had a weak tendency to respond stronger to looming stimuli \DIFaddbeginFL \DIFaddFL{($r =$ 0.02, $p_{r} =$ 1e-6)}\DIFaddendFL . A diagram inset illustrates the concept of Katz centrality: \DIFdelbeginFL \DIFdelFL{for this graph, }\DIFdelendFL cells with low centrality (sources) are pale, while cells with higher centrality (sinks) are darker. \textbf{B}. Correlation coefficients for Katz centrality of each cell and its selectivity for looming stimuli, calculated for each experiment, and shown by developmental stage. \textbf{C}. Correlation coefficients for node in-degree and its selectivity for looming stimuli, in each experiment, shown by developmental stage. \textbf{D}. Correlation coefficients for average activation of each cell, and its selectivity for looming stimuli, in each experiment, shown by stage. }
\end{figure*}

To identify information sinks \DIFaddbegin \DIFadd{(neurons that tended to collect activation from the rest of the network)}\DIFaddend , for each cell we calculated its \textbf{Katz centrality} within the graph \citep{katz1953original, fletcher2018katz}. By definition, nodes with high Katz centrality have many paths leading to them, so a spike originating at random within a graph is more likely to eventually cause activation of these nodes, compared to nodes with low Katz centrality. We found that when all cells from all experiments were considered (Figure 6A), there was a very weak but \DIFaddbegin \DIFadd{statistically }\DIFaddend discernible correlation between the looming selectivity of cells and their Katz centrality ($r =$ 0.02, $p_{r} =$ 1e-6, $n=$2487). More convincingly, correlation coefficients between Katz centrality and looming selectivity were \DIFaddbegin \DIFadd{weak but }\DIFaddend positive in 19/30 experiments (Figure 6B; average $r=$ 0.09$\pm$0.20 $p_{t1}=$ 0.03). There was no \DIFdelbegin \DIFdel{obvious }\DIFdelend change of this effect in development ($p_t=$ 0.5).

A node may have high Katz centrality for two reasons: it may receive a higher number of incoming connections (higher \textbf{in-degree}), or have chains of directed edges leading to it. To see which of these patterns may be at work here, we checked whether looming-selective cells tended to receive more incoming connections, compared to non-selective cells \citep{litwin2014assemblies}. When all points were considered, there was no correlation between in-degree and cell selectivity ($p_{r}=$0.3, $n=$ 2487), but for individual experiments\DIFaddbegin \DIFadd{, }\DIFaddend correlation coefficients between in-degree and selectivity were positive in 20/30 of cases (Figure 6C, $p_{t1}=$ 0.03)\DIFaddbegin \DIFadd{, which suggests that selective cells may tend to receive more incoming connections than non-selective cells}\DIFaddend . There was no \DIFdelbegin \DIFdel{obvious }\DIFdelend change of this correlation in development ($p_t=$ 0.5).

As cells with higher Katz centrality tend to be activated more often \citep{fletcher2018katz}, we checked whether selectivity for looming stimuli correlated with cell \textbf{average spiking activity} in our recordings. We found that both in younger and older animals, actively spiking cells tended to have stronger looming selectivity (for stage 46 average $r_S=$ 0.34$\pm$0.20, $r>$ 0 with $p_{t1}=$ 2e-5; for stage 49 $r_S=$ 0.14$\pm$0.26, $p_{t1}=$ 0.04, indicating a discernible decrease with development $p_t=$ 4e-4; across all cells $r_S=$ 0.07, $p_r=$ 4e-4\DIFaddbegin \DIFadd{; Figure 6D}\DIFaddend ). This \DIFdelbegin \DIFdel{matches }\DIFdelend \DIFaddbegin \DIFadd{result also matches the }\DIFaddend predictions from our studies of intrinsic excitability in the tectum \citep{busch2019}\DIFdelbegin \DIFdel{. For this analysis }\DIFdelend \DIFaddbegin \DIFadd{, where we show that higher spiking cells tend to be selective for slower synaptic inputs. For the analysis above }\DIFaddend we used Spearman rather than Pearson correlation, as 4 experiments had single neurons (one per experiment) with extremely high activity levels that acted as influential points. Excluding these 4 neurons \DIFaddbegin \DIFadd{out of 2487 total }\DIFaddend and using Pearson correlation yielded \DIFdelbegin \DIFdel{very }\DIFdelend similar results.

If looming-selective cells gather information from the network, it was \DIFdelbegin \DIFdel{also }\DIFdelend plausible to expect that they \DIFdelbegin \DIFdel{would }\DIFdelend \DIFaddbegin \DIFadd{could }\DIFaddend be connected to other selective cells more often than to non-selective ones. To test this, we looked into \textbf{assortativity of selectivity}: a weighted correlation between selectivity scores of cells connected by edges, with strength of these edges taken as weights (see Methods). We found that in both younger and older tadpoles, selectivity values for connected cells correlated (for stage 46: $r=$ 0.07$\pm$0.13, $p_{t1}=$ 0.04, individual $r>$ 0 in 9/14 experiments; for stage 49: $r=$ 0.07$\pm$0.10, $p_{t1}=$ 0.02; individual $r>$ 0 in 10/16 experiments). This suggests that similarly selective cells indeed tended to be connected to each other. There was no change in this effect over development ($p_t=$ 0.9).

% Plot for assortativity shows perfectly no change. Is it worth showing it here?

Finally, we checked whether higher average activity of looming-selective cells linked them into tight clusters, or "rich clubs". We calculated \textbf{local clustering coefficient} for each cell, and checked whether it correlated with cell selectivity. We found that local clustering coefficient did not correlate with cell selectivity both when all cells were pooled together ($p_r=$ 0.6, n=2487), or across individual reconstructions (correlation coefficients across experiments were not different from zero; $p_t1=$ 0.25, n=30). This shows that while selective cells tended to be connected to each other, they did not form \DIFdelbegin \DIFdel{tight }\DIFdelend clusters. Together these results suggest that the distribution of selective cells in tectal networks was not random.

%DIF < We also hypothesized that selectivity for looming stimuli may gradually “accumulate” as the signal propagates through the network, and so “downstream” cells on the receiving end of a strong edge within a graph would, on average, be more selective than “upstream” cells. This hypothesis proved to be wrong in both younger and older tadpoles: across all experiments, 52$\pm$7\% of strong edges (top 25\% of $w_{ji}$ values) led from cells with less to more selective cells, which is not different from chance rate ($p_{t1}$=0.1 for analysis across experiments, $n$=30). A weighted average increase in selectivity between two cells connected by an edge was 0.02$\pm$0.07 (not different from zero: $p_{t1}$=0.2, $n$=30), and there was no change in this value in development ($p_t$=0.2, $n$=14, 16).
%DIF > We also hypothesized that selectivity for looming stimuli may gradually “accumulate” as the signal propagates through the network, and so “downstream” cells on the receiving end of a strong edge within a graph would, on average, be more selective than “upstream” cells. This hypothesis proved to be wrong for both younger and older tadpoles: across all experiments, 52$\pm$7\% of strong edges (top 25\% of $w_{ji}$ values) led from cells with less to more selective cells, which is not different from chance rate ($p_{t1}$=0.1 for analysis across experiments, $n$=30). A weighted average increase in selectivity between two cells connected by an edge was 0.02$\pm$0.07 (not different from zero: $p_{t1}$=0.2, $n$=30), and there was no change in this value in development ($p_t$=0.2, $n$=14, 16).

\subsection*{Developmental Model}

\DIFdelbegin \DIFdel{As our recordings were noisy, and sample sizes were relatively small, it was hard to expect that we would uncover clear links between selectivity and connectivity of tectal cells in experimental results alone. To compensate for this limitation, and }\DIFdelend \DIFaddbegin \DIFadd{To }\DIFaddend provide a counterpoint to \DIFaddbegin \DIFadd{our }\DIFaddend experimental results, we built a mathematical model of the developing tectum, and ran simulated responses from this model through \DIFaddbegin \DIFadd{the  }\DIFaddend \textit{exactly same} set of analyses that we used for \DIFdelbegin \DIFdel{our }\DIFdelend \DIFaddbegin \DIFadd{biological }\DIFaddend experiments. The model consisted of 81 artificial neurons, arranged in a 9$\times$9 grid (Figure 7A), that were all originally connected to each other (every neuron to every neuron) with random positive (excitatory) synaptic weights (Figure 7B). The model operated in discrete time, in 10 ms increments, and we interpreted the output of each neuron at each time step as its instantaneous firing rate. At each time step we looked at the network activity at the previous step, calculated total synaptic inputs to each neuron, and used a sliding logistic function to translate these synaptic inputs to postsynaptic spiking (see Methods for a full detailed description).

\begin{figure*}[!t]
\includegraphics[width=\linewidth]{fig7.pdf}
\caption{
Developmental model. \textbf{A}. Model diagram (see the text). \textbf{B}. Typical adjacency matrix before simulation (random seed). \textbf{C}. Typical adjacency matrix by the end of simulation. \textbf{D}. Visualization of neuronal connections in space, with cell positions properly represented. Green and yellow nodes are selective for looming stimuli. \textbf{E}. Same graph as in C, but rearranged to reveal its structure. \textbf{F}. Evolution of all input weights to one sample cell, over the course of simulation. \textbf{G}. Evolution of spiking threshold for 9 sample cells, over the course of simulation. \textbf{H}. Cumulative responses of every cell to Flash, Scrambled, and Looming stimuli in a sample simulation. }
\end{figure*}

We introduced three simple developmental rules: spike-time-dependent plasticity (STDP), homeostatic plasticity, and synaptic competition. Our implementation of STDP approximated biological STDP observed in the tectum \citep{zhang1998stdp, mu2006stdp}: if two cells were active one after another in two consecutive time frames, and they were connected with a synapse, the weight of this synapse was increased. Conversely, if two cells were active within the same time frame, the \DIFdelbegin \DIFdel{weight of a synapse connecting them was }\DIFdelend \DIFaddbegin \DIFadd{weights of a synapses connecting them were }\DIFaddend decreased, as it means that \DIFdelbegin \DIFdel{one cell }\DIFdelend \DIFaddbegin \DIFadd{cells }\DIFaddend would try to activate \DIFdelbegin \DIFdel{another cell }\DIFdelend \DIFaddbegin \DIFadd{each other }\DIFaddend immediately \textit{after} \DIFdelbegin \DIFdel{it had }\DIFdelend \DIFaddbegin \DIFadd{they }\DIFaddend spiked (see Figure 7F for a typical evolution of synaptic inputs to one sample cell). The homeostatic plasticity rule adjusted excitability thresholds, trying to keep spiking output of each neuron constant on average (\citealt{pratt2007intrinsic, turrigiano2011}; Figure 7G). Synaptic competition attempted to keep close to a constant both the sum of synaptic inputs to each neuron, and the sum of outputs from each neuron. In practice, it means that every time a synapse connecting neurons $i$ and $j$ increased in strength, all output synapses of neuron $i$ and all input synapses of neuron $j$ were scaled down a bit \citep{cohen2002synreview, munz2014hebbian, hamodi2016nmda}.

With these three rules at play, we exposed the model to patterned sensory stimulation that imitated retinotopic inputs from the eye (Figure 7A). We hypothesized that during collision avoidance in real tadpoles, STDP-driven changes in the tectum may be controlled and amplified by a global learning signal that arrives if collision avoidance was unsuccessful \DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\citep{savin2014stdpreward, aswolinskiy2015stdpreward}}\hspace{0pt}%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citep{savin2014stdpreward, aswolinskiy2015stdpreward, heap2018hypothalamic}}\hspace{0pt}%DIFAUXCMD
}\DIFaddend , and originates either in dimming receptors in the retina \citep{baranauskas2012}, or in mechanosensory systems of the hindbrain \citep{pratt2009multisens, felch2016, truszkowski2017}. This approach is known as the eligibility trace model, in which changes in synaptic weights do not happen immediately, but are first "remembered" by each cell as potential changes, and are only implemented in response to a timely reinforcement signal \citep{seung2003trace}. To reflect this assumption, in the main set of simulations we only exposed our model to looming stimuli, as if STDP-driven changes were only implemented during \DIFdelbegin \DIFdel{looming responses }\DIFdelend \DIFaddbegin \DIFadd{collisions }\DIFaddend (but see sensitivity analyses below). The network was allowed to develop for 12500 time steps (at least 500 looming stimuli), and we saved its topology at five equally spaced time points during this process (Figure 7D,E), from a naive network (\DIFaddbegin \DIFadd{adjacency matrix shown in }\DIFaddend Figure 7B), to its final form (Figure \DIFdelbegin \DIFdel{7C}\DIFdelend \DIFaddbegin \DIFadd{7C-E}\DIFaddend ). We ran the simulation 50 times, and for each connectivity snapshot analyzed its weighted graph, as well as network responses to \DIFdelbegin \DIFdel{model }\DIFdelend \DIFaddbegin \DIFadd{modeled }\DIFaddend visual stimuli: a looming stimulus, a scrambled stimulus, and a full-field flash (same as in biological experiments; Figure 7H).

\begin{figure*}[t!]
\includegraphics[width=\linewidth]{fig8.pdf}
\caption{
Model results. Each plot shows how one network measure changed as the model developed. Lines of different colors represent different model types: \DIFdelbeginFL \DIFdelFL{red }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{bold black }\DIFaddendFL is for the default model; other colors show sensitivity analyses. See the text for details. Plots B and C have a gap in Y axis \DIFdelbeginFL \DIFdelFL{, }\DIFdelendFL to accommodate an outlier \DIFaddbeginFL \DIFaddFL{curve}\DIFaddendFL . }
\end{figure*}

The \textbf{summary of modeling results} is shown in Figure 8 (black line in each plot). In development, the network became selective for looming stimuli, both in terms of \textbf{total response} (by the end of development network responded to looming 99$\pm$9\% \DIFaddbegin \DIFadd{(about 2 times) }\DIFaddend stronger than to flash; Figure 8A; "no selectivity" corresponds to the value of 0) and \textbf{average selectivity} (mean Cohen $d$ of responses across all cells = 1.09$\pm$0.10; Figure 8B). The \textbf{share of cells} selective for looming stimuli also increased (Figure 8C), and plateaued at $\sim$98\% level, as did the selectivity of the top 10\% of most selective cells (Figure 8D).

We then analyzed the quality of \textbf{stimulus encoding}, or whether \DIFdelbegin \DIFdel{in our model the identity of visual stimuli }\DIFdelend \DIFaddbegin \DIFadd{stimulus identity }\DIFaddend could be reconstructed from \DIFdelbegin \DIFdel{network activation }\DIFdelend \DIFaddbegin \DIFadd{the total activation of every cell in the network, similar to how we did it for biological experiments}\DIFaddend . For a dataset consisting of equal shares of looming and non-looming stimuli, stimulus encoding increased in development, and reached prediction accuracy of $\sim$95\% (Figure 8E). The fact that with multivariate logistic regression we could identify looming stimuli \DIFdelbegin \DIFdel{in 95\% of cases }\DIFdelend \DIFaddbegin \DIFadd{so well }\DIFaddend suggests that a retinotopic STDP-driven network can achieve reliable looming detection, as long as it is equipped with a properly tuned output layer. We assume that in biological tadpoles, this output layer is represented by the projections from the tectum to the reticulospinal neurons in the hindbrain \citep{helmbrecht2018topography}, with weights of these projections tuned in development via reinforcement learning.

Unlike for biological experiments, the model was selective for \textbf{looming over scrambled stimuli} at the full network level (Figure 8F): by the end of the training, responses to looming stimulus were 43$\pm$7\% stronger than to scrambled. Mean selectivity of individual cells was 0.48$\pm$0.07 (Figure 8G); and 84\% of cells were selective for looming stimuli (Figure 8H), which was also different from what we observed in real animals. \DIFdelbegin \DIFdel{The selectivity for looming over flash correlated with selectivity for scrambled over flash on a cell by cell basis }\DIFdelend \DIFaddbegin \DIFadd{Across cells within each network, looming-over-flash selectivity only weakly correlated with scrambled-over-flash selectivity }\DIFaddend ($r$= 0.17$\pm$0.10)\DIFdelbegin \DIFdel{. }\DIFdelend \DIFaddbegin \DIFadd{, which was unlike high correlation (r$\sim$0.8) observed in imaging experiments. On the contrary, looming-over-flash selectivity strongly correlated with looming-over-scramble selectivity in the model ($r$=0.91$\pm$0.02), while it was not present in imaging experiments (to r$\sim$0.03). This suggests that while in the biological tectum selectivity was strongly influenced by activation dynamics, in the model it was largely driven by the stimulus geometry. }\DIFaddend A subset of highly selective cells did not emerge in the model, and the difference between 90th and 50th percentiles of selectivity were rather low ($\sim$ 0.9 ; Figure 8I).

The \textbf{position of selective cells} within the \DIFdelbegin \DIFdel{network }\DIFdelend \DIFaddbegin \DIFadd{visual field }\DIFaddend was different in the model \DIFdelbegin \DIFdel{, }\DIFdelend compared to biological experiments. While in tadpoles selective cells tended to sit in the middle of the retinotopic field, in the model they tended to be on the periphery, and selectivity for looming stimuli positively (rather than negatively as in calcium imaging experiments) correlated with the distance from the network center (Figure 8J). Similar to tadpoles, however, this correlation decreased in development (from $r\sim$ 0.75 in a naive network, to $r\sim$0.25 in a trained network). Similarly to biological networks, selective cells tended to be closer to each other (down to 71$\pm$3\% of distance expected for random connections; Figure 8K), yet unlike for biological networks, this locality of connections was refined in development.

We then looked at topological and functional correlates of looming selectivity in model networks (\textbf{centrality measures}). Selective cells tended to be more spiky ($r$=0.34$\pm$0.12; Figure 8L), and usually were not a part of a cluster (Figure 8M; final correlation with local clustering coefficient $r$=$-$0.26$\pm$0.12). Unlike in biological networks, in the base model (red line) selective sells did not have higher Katz centrality ($r$=0.02$\pm$0.13; Figure 8N), and they did not tend to receive an unusual number of incoming connections ($r=$0.00$\pm$0.12; Figure 8O). As in biological experiments, selective cells tended to be connected to each other (weighted assortativity of 0.24$\pm$0.07; Figure 8P). %DIF < In naive networks, the majority ($\sim$85\%) of strong edges (top 50\% of edges by weight) tended to lead from less selective sells to more selective ones, but in developed networks this share was reduced to chance value (51$\pm$0.03), matching our results from biological networks.
\DIFaddbegin \DIFadd{In naive networks, the majority ($\sim$85\%) of strong edges (top 50\% of edges by weight) tended to lead from less selective sells to more selective ones, but in developed networks this share was reduced to chance value (51$\pm$0.03), matching results from biological networks.
}\DIFaddend 

The \textbf{variability of responses} to looming stimuli over time, quantified as the number of principal components required to describe 80\% of response variability, mildly increased with network development, from 28$\pm$1 to 37$\pm$1 (Figure 8Q). The number of detected network \textbf{ensembles} did not change in development, staying around 15 (Figure 8R), but the share of variance in network responses explained by the involvement of different ensembles increased from $\sim$35\% in naive networks, to $\sim$50\% by the end of learning. As in biological networks, cells that formed an ensemble were slightly (by ca. 20\%) more likely to be connected to each other, and were about 40\% closer to each other than any two random cells in the network.

The \textbf{distribution of degrees} in the model was similar to that in biological experiments: the share of weakly connected cells (weighted in-degree $<$ 0.5) plummeted from $\sim$50\% in naive networks to 9$\pm$2\% by the end of training. On the contrary, the share of cells with degrees of 1 and 2 increased from $\sim$40\% to 83$\pm$2\%. With these changes, the power constant for the degree distribution changed from $\gamma \sim -$1.5 before training to $-$1.96$\pm$0.03 after training for both in- and out-degrees (Figure 8S), which qualitatively matched the changes observed in biological experiments. The share of reciprocally connected cells also decreased in development (Figure 8T).

To assess whether the model developed \textbf{synfire chains} synchronized with looming stimuli, we calculated a correlation between the synaptic weights connecting different cells in the model network (coefficients of the adjacency matrix), and the frequency at which these pairs of cells were activated in a sequence (one immediately after another) during looming stimulation. We found that looming-encoding edges were overrepresented in our connectivity graphs, and their share increased in development (Figure 8U). To estimate how strongly synfire chains contributed to looming detection we looked \DIFaddbegin \DIFadd{at }\DIFaddend whether edges that led from less looming-selective to more selective cells tended to be the ones activated during looms (see Methods). We found (Figure 8V) that the contribution of looming-aligned edges to selectivity increase along the graph was non-zero, and tended to grow, but very mildly, indicating that synfire resonances were not the sole mechanism behind looming detection.

Finally, we observed that most \textbf{global network measures} changed as the network matured: efficiency (Figure 8W) and modularity (Figure 8X) increased, while clustering (Figure 8Y) decreased. In all three cases, the changes were mainly due to changes in weight and degree distribution, as they persisted if calculated on networks randomized with degree-preserving rewiring. The flow hierarchy (Figure 8Z) increased mildly in development, which was entirely due to structured changes in network topology, as the effect disappeared in rewired graphs.
%DIF < A robust increase in modularity may seem to be in contradiction with a stable number of activation ensembles in the network, as network modules may be expected to form activity ensembles. We argue that the reason for this difference is that, as in biological experiments, we identified ensembles from activity during highly structured sensory stimulation, rather than from long recordings of spontaneous activity \citep{triplett2018emergence}.

To conclude whether predictions of our model matched biological experiments overall, we formulated a \textbf{list of “atomic”, easily verifiable elementary statements} about different measures we analyzed (Table 1, first two columns). The model and the experiments showed similar selectivity for looming over flash, but different selectivity to looming over scrambled stimuli. The interplay between cell position and connectivity was similar, except for the spatial distribution of looming-selective cells within the retinotopic map, which was peripheral in the model, but central in tadpoles. Changes in degree distributions were well matched, yet, with a possible exception of modularity, none of other network measures matched. When correlations between different types of node centrality and cell selectivity were considered, some of them matched, but some did not.

%\newgeometry{left=1in} % This page will have smaller margins
\begin{table}
    \input{table1.tex}
    \caption{A summary of network phenomena observed in biological experiments, in comparison with simulation results for the \textbf{base} model, and several reduced models (see main text). In the table, we $\checkmark$ is used for "yes", $\times$ for "no", $\land$ for "increase", $\lor$ for "decrease", $\land \lor$ for "increase followed by decrease", and $=$ for "no change". FL stands for "Flash-Looming" selectivity; SL - for Scrambled-Looming selectivity; "cor" abbreviates "correlation".}
\end{table}
%\restoregeometry

\subsection*{Sensitivity analysis}

While a comparison with one faithfully constructed model is important, a better approach is to consider a family of models, and see which assumptions are critical for the replication of biological results, and which ones are not essential \citep{linderman2017constrain, pauli2018repro}. For example, we wondered whether it was important to assume that plasticity was stronger during actual collisions, or whether looming selectivity would develop if instead of looming stimuli we used more general visual stimuli. We also wondered whether structured sensory flow is necessary for the emergence of selectivity \citep{triplett2018emergence}. To answer these questions, we rerun our model, excluding different parts of it one by one (but not in a cumulative fashion). First we removed explicit synaptic competition, by replacing it with synapsic weight decay (Figure 8, red lines). In a different set of simulations we greatly decreased the amount of intrinsic plasticity present in the system (Figure 8, \DIFaddbegin \DIFadd{dotted }\DIFaddend green lines), and in yet another set we replaced STDP with simple symmetrical Hebbian plasticity (Figure 8, blue lines). Finally, in last two series of simulations, we let the model develop either while exposed to random visual noise (Figure 8, \DIFdelbegin \DIFdel{pink dashed line}\DIFdelend \DIFaddbegin \DIFadd{dashed pink lines}\DIFaddend ), or a randomized mix of translating, receding, and oblique looming stimuli (Figure 8, \DIFdelbegin \DIFdel{gray dashed line}\DIFdelend \DIFaddbegin \DIFadd{dashed gray lines}\DIFaddend ).

Disruption of individual developmental rules led to very different changes in network properties, and their trajectories. When STDP was replaced by a simple Hebbian plasticity (Table 1, column "No STDP"; blue lines in Figure 8), looming selectivity was similar \DIFdelbegin \DIFdel{or better than with }\DIFdelend \DIFaddbegin \DIFadd{to that for }\DIFaddend STDP (top lines in Figure8A,B,F,G), and the network generally developed similarly, except \DIFdelbegin \DIFdel{for modularity that was higher (Figure }\DIFdelend \DIFaddbegin \DIFadd{that reciprocal connections were not eliminated (Figure 8T); connections were more local (Figure 8K) and modular (disjointed, Figure }\DIFaddend 8X), and \DIFdelbegin \DIFdel{neuronal ensembles that were }\DIFdelend \DIFaddbegin \DIFadd{with }\DIFaddend very strongly interconnected \DIFaddbegin \DIFadd{ensembles }\DIFaddend (connections within ensembles were \DIFdelbegin \DIFdel{up to 4 }\DIFdelend \DIFaddbegin \DIFadd{7 }\DIFaddend times more likely than between ensembles, compared to \DIFaddbegin \DIFadd{only }\DIFaddend $\sim$1.2 \DIFdelbegin \DIFdel{times }\DIFdelend \DIFaddbegin \DIFadd{probability ratio }\DIFaddend in base experiments).
\DIFdelbegin \DIFdel{With Hebbian plasticity, and the model trained on looming stimuli, looming-resonant synfire chains constituted more than 40\% of all network edges (Figure 8U), and contributed to looming selectivity more strongly than in the base, STDP-driven model (Figure 8V).
}\DIFdelend 

When synaptic competition was replaced with synaptic strength decay (Table 1, column "No Syn. Comp."), the degree distribution was very different (got flatter rather than sharper; Figure 8U, red lines); selectivity for looming stimuli was weakened (Figure 8C); the pattern of interactions between selectivity and node centrality became unlike what was observed in the base series of experiments (Figure 8M-O), and the network became strongly hierarchical (Figure 8Z). The main reason for these differences appears to be that without synaptic competition, chains of connected neurons were allowed to lead to "dead-ends" within the graph, while with competition the total output of each neuron remained constant, leading to the development of cycles. 

When intrinsic plasticity was weakened (Table 1, column "No Intrinsic"; Figure 8, \DIFaddbegin \DIFadd{dotted }\DIFaddend green lines), looming detection did not emerge (\DIFdelbegin \DIFdel{green }\DIFdelend \DIFaddbegin \DIFadd{bottom lines }\DIFaddend in Figure 8A-C,F-H), and networks had simpler response structure, both in terms of PCA results (Figure 8Q), and ensemble analysis (Figure 8R). This shows that agile intrinsic plasticity is critical for learning; without it the interaction between neuronal activity and network connectivity is disrupted, as the network cannot process sensory stimuli, yet is prone to spontaneous activity.

Finally, training exclusively on looming stimuli was not critical for most measurements, in terms of replicating predictions of a full model, but training on structured stimuli was. When looming stimuli (black line in Figure 8) were replaced by non-colliding visual stimuli (dashed gray line in Figure 8), most network parameters developed similarly to how they did in the base model (Table 1, column "Visual"). Stimulus encoding was similar or better; measures of looming selectivity showed similar trajectories (Figure 8A-I), except that the values of selectivity were 20-50\%, and changes in many network properties were indistinguishable (Figure 8W-Z). In contrast, when patterned stimuli were replaced with uncorrelated noise (dashed pink line in Figure 8; Table 1, column "Noise"), most measures of selectivity did not improve with time (dashed pink lines are flat in Figure 8A-B and D-I), even though network topology still changed in development (Figure 8W-Z). It suggests that looming detection should not emerge in enucleated or \DIFdelbegin \DIFdel{dark-rared }\DIFdelend \DIFaddbegin \DIFadd{dark-reared }\DIFaddend aquatic animals, unless the development of their tecta is guided by retinal waves with temporal statistics similar to that of behaviorally relevant visual stimuli \citep{huberman2008waves}. This prediction matches experimental reports from tadpoles \citep{xu2011}, but curiously seems to contradict observations in Zebrafish \citep{pietri2017emergence}.

\section*{Discussion}

In the first half of this study, we reconstruct functional connectivity in the tectum of \textit{Xenopus} tadpoles from high-speed calcium imaging recordings, and describe novel aspects of tectal network topology. We show that tectal networks become more \DIFdelbegin \DIFdel{openwork }\DIFdelend \DIFaddbegin \DIFadd{delicate and open-work }\DIFaddend with development, approaching scale-free statistics \DIFaddbegin \DIFadd{(with degrees following a power distribution with $\gamma >$ 2)}\DIFaddend , and non-random network structure. We also show that looming-selective cells tend to be located in the middle of the receptive field for a looming stimulus they respond to, and that they tend to serve as "information sinks", collecting more inputs from the rest of the network, compared to non-selective cells.

For the second part of this paper, we hypothesized that a simulated developing network governed by the spike-time-dependent plasticity, synaptic competition, and stimulated by patterned visual inputs, would 1) spontaneously acquire selectivity for looming stimuli, and 2) develop a non-random network structure, which 3) would be similar to that observed in biological experiments. The support for this hypothesis is mixed. The model did develop selectivity for looming stimuli (both in terms of average preference, and stimulus encoding), and this increase in selectivity was resilient to changes in model assumptions. Yet model result were not truly replicated in biological experiments, as in tadpoles there was no improvement in looming detection, and neither average selectivity, stimulus encoding, nor cell specialization increased with development. This was particularly surprising in view of a known improvement in collision avoidance in tadpoles with age \citep{dong2009}. Moreover, while spatiotemporally tuned synfire chains clearly emerged in \DIFdelbegin \DIFdel{thee }\DIFdelend \DIFaddbegin \DIFadd{the }\DIFaddend model (Figure 8U), we could not clearly tease out their quantitative contribution to looming selectivity.

As predicted, our model networks developed non-random structure, with a scale-free degree distribution, low clustering, and high modularity. Several results related to network structure were replicated in biological experiments (Table 1, compare columns "Imaging" and "Model / Base"): most notably, changes in network degree distribution, a decrease in bidirectional connections, and statements related to neuronal ensembles. This match between the model and the experiment suggests that at the very least, our model captured the nature of network development under the influence of synaptic competition and spike-time-dependent plasticity (STDP). Synaptic competition promoted connectivity in weakly connected neurons, while "punishing" overconnected cells, which created light-frame, openwork graph structures \citep{fiete2010chains}, and STDP coordinated activity within sub-networks, increasing modularity \citep{stam2010modular, litwin2014assemblies}, similar to how it was previously described for other types of plasticity \citep{damicelli2018topomod, triplett2018emergence}. We did not observe changes in the \textit{number} of neuronal ensembles \citep{avitan2017spontaneous, pietri2017emergence}, but we believe this is because our experiments were not suited for ensemble detection, as we worked with strong shared inputs that reliably activated almost every neuron in the network. This made our experiments very much unlike the case of spontaneous activity, when different sub-networks get activated randomly, with activity propagating within modules more readily than between them \citep{avitan2017spontaneous}.

At the same time, most model predictions about how network properties were supposed to change in development did not replicate in biological experiments. There are four possible explanations for this discrepancy. First, while neurons from stage 46 and 49 tadpoles have different synaptic and intrinsic properties \citep{ciarleglio2015}, and while retinal inputs to the tectum are known to be refined at these developmental stages \citep{tao2005refinement, munz2014hebbian}, the patterns of internal tectal connectivity may be relatively settled by stage 46. In our model, most network measures plateaued, or even reversed late in development (Figure 8), which means that even for a qualitative comparison between the model and the experiment we have to make a critical assumption about whether developmental stage 46 corresponds to a mid-point of network maturation, or falls on the developmental plateau. The absence of improvement in stimulus encoding in older tadpoles (Figure 2G), as well as a known difference in STDP between very young (stage 42) and older (stage 48) tadpoles \citep{richards2010stdp, tsui2010developmental}, suggest that both stages included in this study may indeed fall on the “plateau”. If true, this would mean that the known improvement in collision avoidance between stage 46 and stage 49 tadpoles \citep{dong2009} is largely due to maturation of sensorimotor projections from the tectum to the hindbrain that we did not assess in this study.

Second, a poor fit between model predictions and biological experiments may be a consequence of low statistical power of this study. With, respectively, 14 and 16 networks reconstructed for each developmental stage, we could only hope to detect large changes in network parameters (Cohen $d \approx$ 1.0, assuming $p_t<$ 0.05 threshold and 80\% power). Moreover, based on available imaging studies, we can estimate that at stage 49, one half of a tadpole tectum contains about 10,000 neurons, as it measures about 40 cells across \citep{hiramoto2009}, and is packed 6-10 cells deep in its thickest part \citep{hewapathirane2008vivo}, while tapering towards the edges \citep{bollmann2009}. On the other hand, here we reconstructed connectivity within the top layer of 128$\pm$40 cells, in a field of about 12 by 12 neurons, which means that our reconstructions covered only about 1\% of a full tectal network, and 0.01\% of all connections. With a coverage so sparse, our parameter estimations were expected to be noisy, further lowering our test power.

Third, one can question the validity of our connectivity reconstructions, as we did not have an opportunity to compare these reconstructions to a "ground truth" connectivity (but see \citealt{xu2011}). The best way to address this concern would be to run a set of control \DIFdelbegin \DIFdel{experiment}\DIFdelend \DIFaddbegin \DIFadd{experiments}\DIFaddend , analyzing transfer entropy between pairs of cells proven to be either connected or disconnected, to estimate the power of graph reconstruction from calcium imaging recordings. Unfortunately, these experiments are currently beyond our technical ability, so we have to rely on indirect criteria of a successful network reconstruction. Two most important observations that support the validity of our results are the fact that the share of reciprocal connections decreased in development; and that we observed a consistent non-randomness of almost all network measurements of reconstructed networks, compared to rewired networks (Figure 5). Also, we were comforted by a good replication of tectal response shapes (Figure 2A,B, compared to \citealt{khakhalin2014}), a good internal replication of edge detection between stimuli types, and an observation of retinotopy during responses to looming stimuli (Figure 3A,B).

%DIF < Notably, the fact that we recorded network activity exclusively in response to shared inputs does not pose a big concern for this type of analysis, as during adjusted transfer entropy calculations we ignored all patterns that were shared across trials. This means that, if anything, we were at risk of not detecting strongest connections within the network if they were robust enough to be activated in each and every trial, rather than mistaking shared synaptic drive for within-network connectivity.
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend Finally, the fourth way to explain a relatively poor fit between our model and imaging experiments is to assume that the mechanisms of looming selectivity in the tectum are after all different from that in the model. In our simulations, looming selectivity was mediated by the emergent synfire chains \citep{cohen2002synreview, zheng2014synfire} that were shaped by the structured sensory activation \citep{vislay2006rf, clopath2010stdpcoding}, and thus encoded activation patterns typical for looming stimuli \citep{pratt2008recurrent, richards2010stdp}. When looming stimuli were presented to the model, they "resonated" with matching synfire chains, causing them to respond strongly. It may be that in the biological tectum, enhanced responses to looming stimuli are due to either delayed recurrent integration \citep{khakhalin2014, jang2016}, dynamic inactivation of neurons \citep{fotowat2011multiplexing}, or \DIFdelbegin \DIFdel{some }\DIFdelend other non-linear effects \DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\citep{baginskas2009recurrent}}\hspace{0pt}%DIFAUXCMD
. Two }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citep{baginskas2009recurrent, heap2018dimming}}\hspace{0pt}%DIFAUXCMD
. The two }\DIFaddend biggest discrepancies between the model and the experiments were the position of selective cells within the network (central in tadpoles, peripheral in the model; Figure 3C vs. Figure 8J), and the difference in \DIFdelbegin \DIFdel{centrality measurements related to local signal integration }\DIFdelend \DIFaddbegin \DIFadd{network signal integration in selective cells }\DIFaddend (slightly higher in-degree and Katz rank in biological experiments, but no similar effect in the base model; Figure 6A-C vs. Figure 8N,O). We \DIFaddbegin \DIFadd{however }\DIFaddend don't find these discrepancies too problematic\DIFdelbegin \DIFdel{however}\DIFdelend : the difference in spatial placement of looming-selective cells may be due to the explicit, developmentally controlled arrangement of output neurons in biological tecta that \DIFdelbegin \DIFdel{was not included in }\DIFdelend \DIFaddbegin \DIFadd{simply was not a part of }\DIFaddend our model, while the difference in Katz centrality may be explained by exaggerated synaptic competition in the model. Indeed, \DIFdelbegin \DIFdel{in simulations}\DIFdelend \DIFaddbegin \DIFadd{by introducing strong synaptic competition}\DIFaddend , we effectively forced \DIFdelbegin \DIFdel{most }\DIFdelend neurons to have outputs \textit{within} the tectal network, which led to the development of cycles, while in a real tectum selective cells may lack local \DIFdelbegin \DIFdel{intratectal }\DIFdelend outputs, projecting only to other brain regions. This hypothesis is supported by very high Katz centrality of looming-selective cells in simulations without synaptic competition (Figure 8N,O, red lines). We therefore believe that, with \DIFdelbegin \DIFdel{all }\DIFdelend limitations of both the model and the experimental data taken into account, they were largely in agreement with each other. 

At the same time, to actually \DIFdelbegin \DIFdel{find }\DIFdelend \DIFaddbegin \DIFadd{identify and use }\DIFaddend a subset of tectal \DIFdelbegin \DIFdel{outputs that can be used as a }\DIFdelend \DIFaddbegin \DIFadd{cells as }\DIFaddend functional looming detector, as we did while estimating stimulus encoding (Figure 8E), a developing brain would need access to a learning signal. As a working hypothesis, we propose that in aquatic vertebrates this learning signal may come from both dimming receptors in the retina \DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\citep{ishikane2005, baranauskas2012}}\hspace{0pt}%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citep{ishikane2005, baranauskas2012, heap2018dimming}}\hspace{0pt}%DIFAUXCMD
}\DIFaddend , and lateral line receptors in the \DIFdelbegin \DIFdel{body }\DIFdelend \DIFaddbegin \DIFadd{skin }\DIFaddend \citep{pratt2009multisens, felch2016, truszkowski2017}. These inputs can then facilitate plasticity in tectal projections to the reticulospinal neurons in the hindbrain, strengthening inputs from a subset of tectal cells that were most active immediately before a collision. Moreover, during random spatial encounters, different parts of the retina would be dimmed, and different segments of the lateral line would be activated in each individual collision, theoretically allowing animals to build several overlapping subnetworks \DIFaddbegin \DIFadd{within the tectum}\DIFaddend , selective for collisions of different geometry, and projecting to different subsets of motor neurons \DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\citep{frost2004review, helmbrecht2018topography}}\hspace{0pt}%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citep{frost2004review, barker2015sensorimotor, helmbrecht2018topography}}\hspace{0pt}%DIFAUXCMD
}\DIFaddend . This type of learning could lead to the development of spatially nuanced escape responses that are described in both tadpoles \citep{khakhalin2014} and fish \citep{bhattacharyya2017assessment}. Based on this hypothesis, we predict that if tadpoles are raised individually in empty arenas (devoid of objects to collide with), they should have normal vision and dark-startles, but they would not be able to perform proper collision avoidance.

To sum up, we show that a combination of simple developmental rules with patterned sensory stimulation can quickly shape a random network into a structured system \DIFdelbegin \DIFdel{, able to support collision detection}\DIFdelend \DIFaddbegin \DIFadd{that is selective for looming stimuli}\DIFaddend . We show that several predictions from our model are replicated in biological experiments, although we found no improvement in looming detection between stage 46 and 49 tadpoles. We demonstrate that functional connectivity of tectal networks can be reconstructed from fast calcium imaging experiments, and that the structure of these networks can support \DIFdelbegin \DIFdel{collision }\DIFdelend \DIFaddbegin \DIFadd{looming }\DIFaddend detection in small aquatic animals.

\section*{Methods}

% All code for his paper is available at: \url{https://github.com/khakhalin/Ca-Imaging-and-Model-2018}

\subsection*{Statistics and reporting}

Unless stated otherwise, all values are reported as mean $\pm$ standard deviation. For most common tests, test type is indicated by the subscript for its reported p-value: $p_t$ for a two-sample t-test with two tails and unequal variances; $p_{t1}$ for a one-sample two-tail t-test, and $p_r$ for a Pearson correlation test.

When working with weight matrices, we write them as it is usually done in computational neuroscience, where $w_{ji}$ is a weight of an edge coming from node $i$ to node $j$, which is different from how adjacency matrices are presented in graph theory, where $A_{ji}$ would typically mean an edge from node $j$ to node $i$ (so our $\mathbf{W} = \mathbf{A}^\top$).

\subsection*{Experiments}

Overall, we followed calcium imaging protocols previously described in \citep{xu2011, truszkowski2017}, but combined it with visual stimulation modeled after \citep{khakhalin2014}. Experiments were performed at Brown University, in accordance with university IACUC protocols. Unless noted otherwise, chemicals were purchased from Sigma. Tadpoles were kept in Steinberg’s solution, on a 12/12 light cycle, at 18$^\circ$ C for 10-20 days, until they reached Nieuwkoop-Faber developmental stages of either 45-46 or 48-49. In each experiment, we anesthetized a tadpole with 0.02\% tricainemethane sulfonate (MS-222) solution for 5 minutes, then paralyzed it by immersion in 20 mM solution of tubacurarine for 5 minutes, and pinned it down to a carved Sylgard block within the recording chamber filled with artificial cerebro-spinal fluid solution (ACSF: 115 mM NaCl, 4 mM KCl, 5 mM HEPES, 10 uM glycine, 10 mM glucose). The optic tectum was exposed, and ventricular membrane was removed on one side of the tectum. Tadpoles were pinned down tilted, at an angle of 10-20$^\circ$, to keep exposed tectal surface flat for imaging. We then surrounded the tadpole with a small circular enclosure 15 mm in diameter, made of a thicker part of a standard plastic transfer pipette, to achieve higher concentration of Ca-sensitive dye in the solution. We dissolved 50 ug of AM ester cell permeable Oregon Green 488 nm Bapta-1 (OGB1 $\#$06807, Molecular Probes, Waltham, MA) in 30 ul of medium consisting of 4\% F-127 detergent in 96\% DMSO by weight; agitated this solution in a sonicator for 15 minutes, then added 30 ul of ACSF to the vial, and sonicated for another 10 minutes. The solution was then mixed with 4 ml of ACSF to the final concentration of 10 uM, transferred to the chamber, and the chamber was placed in the dark for 1 hour. After staining, the circular enclosure was removed; the preparation was gently washed with 10 ml of ACSF 3 times; the chamber was filled with 10 ml of fresh ACSF, and transferred under the scope.

This staining protocol with a BAPTA-conjugated dye proved to be challenging, and had a high failure rate. As staining procedure involved a detergent, and called for high concentrations of dye, most successful preparations were those that received the highest possible exposure to dye that did not yet kill the cells. A large share of preparations however either fell short of optimal staining, and had a weak fluorescence signal, and so a low signal-to-noise ratio, or got overexposed, leading to strong fluorescence, but weak responses to stimulation, as neurons grew increasingly unhealthy. This variability in signal-to-noise ratio led to  differences in edge detection certainty from one experiment to another, which complicated our network analysis (see below).

Visual stimulation was provided with a previously described setup \citep{khakhalin2014}, consisting of an LCD screen (Kopin Corporation, Taunton, MA, USA) illuminated by a blue LED (LXHL-LB3C, 490 nm; Lumileds Lighting, USA), with image projected to an optic multifiber (600 um, Fujikura Ltd, Tokyo, Japan). The other end of the fiber was brought to the left eye of the tadpole, and placed 400 um away from the lens, on the axis of the eye, to have the image projected to the center of the retina. The stimulation sequence consisted of three stimuli: Looming stimulus (in which a circle appeared in the center of the field, its radius growing linearly from 0 to full-field within 1 second), full-field Flash, and spatially "Scrambled" stimulus. For Scrambled stimuli, we divided the field of view into a grid of 17 by 17 squares, and randomly reassigned these squares within the image. The result was a stimulus that was identical to looming in terms of its total brightness at every time step, and presented fragments of a moving edge locally (within every square in a reshuffling grid), but lacked high-level spatial organization. The permutation of squares was randomized for each experiment, but was consistent within all trials within every experiment. Stimuli were delivered every 20 s, in the same sequence of "Looming, Flash, Scrambled”, typically for the total of 60 or 72 stimuli. The stimuli were generated in Matlab (Mathworks), using Psychtoolbox \citep{kleiner2007psychtoolbox}. Excitation light for imaging was turned on one second before the onset of visual stimulation, and kept on for 5 seconds, which was shown not to interfere with responses \citep{xu2011}.

Fluorescent responses in the tectum were imaged using a Nikon Eclipse FN1 microscope with a 60x water-immersion objective and ANDOR 860 EM-CCD camera (Andor Technologies). NIS-elements software (Nikon) was used to record the activity. We used binning with 8x8 pixels per bin, resulting in a 130x130 image covering the field of view of 1130 um. The data was acquired with 10 ms auto-exposure, leading to actual frame rate of 84 frames per second (11.9 ms per frame). For each preparation, we found a focal plane that produced images of as many cells as possible, which usually meant a plane focused "in-between" topmost and bottom-most cells within the field of view. To keep the signal-to-noise ratio consistent throughout the experiment despite the ongoing bleaching of the Ca sensor, we started with relatively weak illumination (with neutral density filter ND4 engaged) and no signal amplification by the camera (EM gain of 0). We then increased the EM gain level gradually after every 12 stimuli, to keep the signal level approximately constant. Once EM gain setting reached the value of 7, we increased illumination strength by disengaging one of the density filters, reduced EM gain back to 0, and repeated the process.

Videos were processed offline; circular regions of interest of equal size (21 binned pixels per region) were manually positioned over neurons with well defined Ca responses (based on the visualization of fluorescence variability in time, as provided by NIS-elements software). Average fluorescence within each region of interest was quantified, and exported to Matlab. We then processed fluorescence traces using non-negative deconvolution algorithm \citep{vogelstein2010oopsi}, and used its output without thresholding, interpreting it as a probabilistic estimation of instantaneous spike rate for each cell. We chose not to threshold the signal, as depending on the overlap each cell body had with the focal plane, as well as the amount of dye sequestered by the cell during staining, different neurons had very different signal-to-noise ratios even within each preparation, which complicated the matter of finding a single threshold. This decision also shaped all further steps of our analysis, as in our dataset poorly resolved cells with low spiking activity were represented not by spike traces that were mostly silent, but by traces that were noisy, and approached uniform distribution of estimator values. %Reference cells, required for the deconvolution algorithm, were selected automatically, as the cell with 5th highest amplitude fluorescence response.

We did not attempt to match inferred spike trains to "ground-truth" electrophysiology recordings, as the validity of this calcium imaging protocol was justified previously \citep{xu2011, truszkowski2017}. We also did not perform background subtraction \citep{truszkowski2017}, as most effects of background fluorescence were expected to be cancelled out during analysis. The main risk of not subtracting the background is that unsubtracted traces may contain a superposition of axonal spiking and synaptic activation in the neuropil. Judging from the spatial distribution of fluorescence signals, in our experiments neuropil fibers were not stained, as sensitive dye had no access to structures below the top, exposed level of primary tectal cells. Moreover, our signal acquisition was focused on fluorescence sources within the focal plane, meaning that any  neuropil signals were both attenuated, and spatially averaged across regions of interest. Finally, neuropil activation was expected to be similar in each trial, as same stimuli were presented to the tadpole in each trial. As deconvolution operation is close to linear, and we did not perform spike thresholding, any shared neuropil signal would be deconvolved, “hidden” in inferred spike-trains, and later cancelled out during trial-reshuffling (see below). Similarly, we did not address motion artifacts, as in our preparation they were synchronous in all cells (manifested as parallel displacement of signal sources from fixed ROIs), and therefore only introduced a fixed bias to all connectivity estimations.

\subsection*{Analysis}

\textbf{Basic analysis} To quantify response amplitudes, we used reconstructed responses between 250 and 2000 ms into the recording, as this window included full visual responses, but excluded artifacts caused by the excitation light. As a measure of cell selectivity for stimuli of a specific type, and in some cases as a measure of total network response selectivity, we used Cohen’s $d$ effect size for the difference between responses to looming and flash, or looming and scrambled stimuli:

\[ d = (m_L-m_F)/ \sqrt{ \big((n_L-1) s^2_L + (n_F-1) s^2_F)/(n_L + n_F - 2)} = \]
\[ =(m_L-m_F)/\sqrt{\big(s^2_L+s^2_F\big)/2} \]

In case of equal sample sizes $n_L=n_F=n$. Here $m_L$ and $m_F$ are mean responses to looming and flash stimuli respectively, and $s_L$ , $s_F$ are standard deviations for both groups.

To find the \textbf{retinotopy center}, we concatenated all responses of every cell to looming stimuli into one vector, ran a principal component analysis on these vectors, then rotated two first components using promax rotation, and made sure that the 1st component $c^1$ is the one with shorter latency, and that it is positive (by swapping and flipping the components if necessary). We then ran a non-linear optimization, looking for a pair of coordinates $(x,y)$ within the field of view, that would maximize the absolute value of correlation between distances of each cell to this center and the relative prominence of the short-latency component for this cell:

\[ r = \text{cor}\big(\sqrt{(x_i-x)^2+(y_i-y)^2}\ ,\ c^1_i/(c^1_i + c^2_i), \big) \]

We interpreted these $(x,y)$ coordinates as our best guess for the possible position of the "retinotopy center" for each recording. The fits were robust, with $p<$ 0.05 observed in every experiment (30/30), and average achieved correlations of $r=$ 0.59$\pm$0.23. To assess possible overfitting, we performed identical optimization fitting  after reshuffling cell identities 5 times for each experiment, which yielded average $r$ values of only 0.13$\pm$0.06, and $p_r<$ 0.05 in 21\% of experiments. From this we concluded that cells with early responses to looming stimuli were indeed clustered together, and that this clustering was not an artifact of our analysis, even though the $r$-values were probably exaggerated due to overfitting.

For \textbf{response latency} calculations, we looked at each response $y(t)$, and found the position of its maximum $(x_M, y_M)$. We then used the least squares fit with non-linear solved to approximate the segment between the beginning of the response and $x_M$ with a piecewise linear function:

\[ f(x) = \left \{ \begin{array}{cll} 0 & \text{for} & 0 \leqslant x<x_L \\
a (x-x_L) & \text{for} & x_L\leqslant x < x_M \end{array} \right. \]

optimizing for $x_L$ and $a$, where $x_L$ is the response latency, and $a$ is an amplitude-like parameter we did not use for subsequent analysis. This approach worked well for isolated responses with low noise, but got increasingly noisy with weak signals. To quantify the retinotopy, we therefore used the results of factor analysis, as described above, and only used response latencies for results verification.

\textbf{Ensemble analysis}. To find ensembles of cells that tended to be co-active together, we used a modified spectral clustering procedure \citep{ng2002spectral} and the definitinon of spectral modularity \citep{newman2006modularity}, generalized to weighted oriented graphs. First, for each stimulus type, for each cell $i$, and separately for each experimental trial $k$, we unbiased and normalized each activity response $a^k_i(t)$, by subtracting its mean, and dividing the result over standard deviation:

\[ a^k_i(t)' = \big(a^k_i(t)-b^k_i\big)/\sigma^k_i \]

where $b^k_i = \sum_{t=1}^T{a^k_i(t)}$ and $\sigma^k_i = \frac{1}{T-1}\sum_{t=1}^T{(a^k_i(t) - b^k_i))}$ .

Then, for each cell, we calculated the average response across all trials of the same type: 

\[ \overline{a_i}(t) = \frac{1}{n}\sum_{k=1}^n{a^k_i(t)} \]

and subtracted these average responses from each trial, which resulted in a vector of a trial-by-trial deviations from the average response:

\[ a^k_i(t)'' = a^k_i(t)' - \overline{a_i}(t) \]

We then concatenated these vectors of deviations from the mean $a^k_i(t)''$ across all trials, and used them to calculate a cross-correlation matrix, to see which cells tended to be active and inactive together:

\[ c_{ij} = \text{cor}\big(a''_i(t)\, , \, a''_j(t)\big) \]

We calculated adjusted correlations $c_{ij}$ separately for each of three types of stimuli (flash, scramble, and looming), and averaged these three estimations $c_{ij}^s$, to arrive at a less noisy estimation of adjusted cross-correlation. We then removed negative correlations, replacing them with zeroes.

\[ c'_{ij} = \text{max}\big(0 \, , \, \frac{1}{3} \sum_{s}{c_{ij}^s}\big) \]

We then roughly followed the spectral clustering procedure by \citep{ng2002spectral}, with adjustments that seemed appropriate for ensemble detection. We first transformed our correlation matrix $c_{ij}$ into a matrix of pairwise Euclidean distances:

\[ \varphi_{ij} = 2(1-c_{ij}) \]

and then to affinity matrix $\mathbf{A}$:

\[ a_{ij} = \text{exp}(-\varphi_{ij}/\sigma) \]

where $\sigma$ is a free parameter that we set at 10000. We then calculated a diagonal degree matrix $\mathbf{D}$ such that $d_{ij} = 0$ for $i \neq j$ , and $d_{ii} = \sum_k{a_ik}$ otherwise. We used $\mathbf{D}$ to build a Laplacian matrix $L$, such that:

\[ L_{ij} = a_{ij}/\sqrt{d_{ii}\cdot d_{jj}} \]

and found eigenvectors $x_1$ .. $x_n$ of this matrix $L$. Then we started to look for a good number of ensembles $k$, by going through all values from 1 (no ensembles) and up to the number of cells (each cell as a separate ensemble). For each $k$, we found first $k$ largest eigenvectors of $L$, stacked them in columns, and renormalized each row of this matrix to give it unit length:

\[ u_{lm} = \frac{x_{lm}}{\sqrt{\sum_{z=1}^{k}{x_{lz}^2}}} \]

where $x_{lm}$ is an $m$-th element of $l$-th eigenvector of $L$. We then used k-means clustering on rows of $\textbf{U}$ as points in $\mathbb{R}^k$, looking for $k$ clusters. Once rows of $\textbf{U}$ (and so cells in the original data) were assigned to $k$ clusters, we calculated spectral modularity of this partition on the original matrix $w_{ij}$, using a weighted directed modification of classic formula from \citep{newman2006modularity}:

\[ Q_k = \frac{1}{4m}\sum_{ij}{\delta_{ij}\Big(w_{ij}-\frac{d^{out}_i d^{in}_j}{2m}}\Big) \]

Here $d^{out}_i$ and $d^{in}_j$ are weighted out- and in-degrees for nodes $i$ and $j$ respectively: $d^{out}_i = \sum_k{w_{ik}}$ , and $d^{in}_j = \sum_k{w_{kj}}$ ; $m$ is the total number of edges involved: $m = \sum_{ij}{w_{ij}}/2$ , and $\delta_{ij}$ is a signal matrix with $\delta_{ij}=1$ for nodes $i$ and $j$ that belong to the same cluster, and $\delta_{ij} = 0$ otherwise. 

Finally, we selected the number of clusters $K$ that, after spectral clustering, produced highest modularity $Q_K$ across all $Q_k$, and we used $K$ as an estimation of the number of ensembles in the network, and corresponding cluster allocation - as the allocation of cells to these ensembles.

\textbf{Network reconstruction}. For network reconstruction, we used a modified Transfer Entropy (TE) calculation, adapted from \citep{gourevitch2007te, stetter2012te}. Fast Ca imaging recordings, as used in this study, provide a middle ground between commonly used slower Ca imaging data and multielectrode recordings. In most Ca imaging recordings, the frame acquisition time (100 ms) is an order of magnitude longer than the transmission time between neurons (2 ms), which biases analyses towards co-activation analysis. In our data, the high rate of acquisition (12 ms per frame) was close to typical cell-to-cell activation transmission time in the tectum, so we restricted our analysis to interactions between the activity of each cell at a frame i and their activity at the next frame i+1, ignoring both longer (multiframe), and same-frame interactions.

For each cell, we binned its activity trace at 3 levels, classifying every frame as either a high, medium, or low activity frame. For each cell, we used 1/3 and 2/3 quantiles of its inferred activity values as thresholds, so that all three types of frames were equally frequent, to maximize information. Then for each pair of neurons $i$ and $j$ we calculated the probability $P(g_j^1,g_j^0,g_i^0)$, which showed the conditional probability of neuron $j$ being in state $g_j^1$ (either 1, 2, or 3) at moment $t$, if this neuron was in a state $g_j^0$ at the previous frame $t-1$, and  input neuron $i$ was in state $g_i^0$ at the same frame $t-1$. From this set of probabilities, we calculated conditional probabilities of $P(g_j^1 \mid g_j^0)$, and finally calculated the total transfer entropy as

\[ T_{ij} = \sum_{l,m,n=1}^3{P(g_j^1=l,g_j^0=m,g_i^0=n)}\cdot \log\left(\frac{P(g_j^1=l \, \mid \, g_j^0=m, \, g_i^0=n)}{P(g_j^1=l \, \mid \, g_j^0=m)}\right) \]

For this project, a common sensory drive (visual inputs from the retina) presented a unique problem. If the hypothesis of this paper is true, and the detection of looming stimuli in the tectum is actually mediated by sensory activation of matching synfire chains, we can expect the pattern of this sensory activation to be synchronized with causal transfer of excitation from one neuron to another. Because of that, common sensory inputs cannot be eliminated by methods that rely on the comparison of delays \citep{wollstadt2014te}. Instead, we eliminated the effects of common drive by randomly reshuffling our data within each stimulus type, and pairing activation history of each cell with activation history of other cells from non-matching trials. For each experiment, we calculated 1000 randomly reshuffled transfer entropy estimations, and then subtracted the average of these reshuffled TE estimations from raw TE estimation, arriving at adjusted TE \citep{gourevitch2007te}:

\[ T'_{ij} = T_{ij} - T^\text{shuffled}_{ij} \]

This approach is similar to the idea of analyzing subtle variations in activation from one response to another, as opposed to the analysis of activation traces themselves. As presented stimuli were same in every trial, the effect of common sensory drive over time was expected to be shared across all trials. This means that if a connection between cells $i$ and $j$ was suggested equally strongly by the analysis of real, and reshuffled data, these cells were probably sequentially driven by a common input, rather than by a true causal connection between them.

For each TE estimation, we also calculated a corresponding p-value that quantified whether actually observed TE was unusual enough (discernibly different), compared to TE estimations obtained on reshuffled data, that corresponded to a null hypothesis of no causal connections. With the computational power available to us, we could only generate 1000 surrogate reshuffled networks for every TE calculation, which made it impossible to use the false discovery rate correction, as it is sometimes recommended for large-scale studies of brain connectivity \citep{lindner2011trentool, vicente2011te}. With $\sim$10$^2$ neurons and 10$^4$ connections the smallest possible non-zero p-value of 0.001, corresponding to finding a more extreme TE value in one out of 1000 surrogate experiments, was already larger than the Benjamini-Hochberg threshold of $\frac{k}{m}\alpha=$ 5e-6. With a more permissive threshold of $\alpha=$ 0.01, for each of three stimuli types, our analyses suggested the existence of 5\% to 69\% of all possible directed edges in the connectivity graph, depending on the experiment (mean of 16\%). The share of edges that were independently discovered in responses to all three types of stimuli  (mean: 0.1\% of all possible edges) was on average 2.4 times larger than one would have expected for spurious discoveries (signrank $p=$ 7e-7), suggesting that three subsets of data, originating from responses to three different stimuli could be considered replications for the purposes of edge discovery. 

Note that our TE adjustment procedure could not differentiate between activity driven by shared inputs, and activity due to reliable synaptic connections that reproduced the same activation pattern in every trial. It also means that it was by design impossible for us to detect strongest looming-selective synfire chains in responses to looming stimuli, as the appearance of these strong connections would be indistinguishable from the effect of shared sensory input. This suggests that the principle of edge replication across different stimuli could not be taken literally, to avoid this masking effect.

Due to variations in staining quality, preparation shape, and focal plane alignment, we had to work with very different proportions of low-noise and high-noise neurons in different experiments, which made our edge discovery rates very uneven for any fixed threshold approach. To fix this problem, we made our edge detection procedures adaptive on the experiment-by-experiment basis. First, we relaxed criteria on edge discovery, while still giving preference to edges observed in more than one subset of responses. We included in our reconstruction only edges with geometric mean of p-values below threshold: $\prod{p_k}<\alpha^3$ , where $p_k$ are p-values for each of three subsets of data (responses to flash, crash, and scrambled stimuli). Then we looked for a value of $\alpha$ that would bring the average node degree (the ratio of network edges to network nodes, for directed graphs $=E/N$) to an arbitrary reasonable target value, which is a known approach to the analysis of noisy networks \citep{stetter2012te}. We picked a target $E/N$ value of 1.0 (number of edges equal to the number of nodes), which lead to 128$\pm$41 edges in each experiment on average (0.9\% of all possible edges); 50$\pm$21 weakly connected components, with 74$\pm$30 nodes in the largest weakly connected component. The comparison of network properties (Figures 5 and 6) did not change qualitatively in a broad range of assumed average degrees (from $\sim$ 0.5 to 1.5), but observed effects became weaker and regressed to random effects outside of this range. 

The TE approach cannot distinguish between positive and negative influence of one neuron on another, so our reconstructed edges could include a mix of excitatory and inhibitory connections. To estimate the share of putative inhibitory connections, we calculated pairwise correlations between activities of individual neurons, taken with a one frame delay, and compensated for the effect of shared inputs similar to how it was done for TE. We then looked at the sign of these correlations for pairs of neurons with statistically discernible TE. We found that 3$\pm$7\% of detected connections seemed inactivating or inhibitory, with no difference in rate between developmental stages ($d=$ 0.55, $p_t=$ 0.1). According to current understanding on tectal architecture in \textit{Xenopus}, principal tectal neurons are not expected to be inhibitory \citep{bell2011polyamines}, and moreover, the share of negative correlations tended to be lower in experiments with better signal-to-noise ratio. We therefore assumed that most, if not all observed inactivating connections may be false discoveries, and excluded all edges with negative correlation values from the analysis. For those edges that remained in the adjacency matrix, we averaged TE estimations obtained from responses to flash, scrambled, and looming stimuli, and used these averaged values as estimations of synaptic connectivity weights $w_{ji}$.

To analyze \textbf{degree distributions}, we found the sum of weights of incoming and outgoing edges for each cell; rounded these values towards nearest whole number, and calculated frequencies $F_{in}(k)$ and $F_{out}(k)$ for each degree value $k$ (Figure 4F,G). For each experiment, we then fit a regression line $y = -\gamma k + b$ to a sequence of points $[k , \, log(F(k)) ]$, for in- and out-degrees separately; estimated two power constants $\gamma_{in}$ and $\gamma_{out}$, and averaged them to arrive at one balanced estimation ($\gamma$; Figure 4H).

To quantify the share of \textbf{reciprocal connections}, we multiplied the weight matrix element-wise on itself transposed, summed these values up, and normalized the result by the sum of squared weights: $S=\sum_{ji}{w_{ji} w_{ij}} / \sum_{ji}{w_{ji}^2}$ . For positive weight matrices, this value is equal to 1 for symmetric weight matrices, 0 for matrices without reciprocal connections, and smoothly changes between these two values for "intermediate" cases.

\textbf{Network analysis}. We reviewed several lists of statistical tools applicable to weighted directed graphs \citep{rubinov2010toolbox,costa2007networks,hernandez2011metrics}, and selected a diverse set of measures to describe  different aspects of our networks, such as connectivity, unevenness of density, and global structure. We only included measures that do not erode with the inclusion or exclusion of individual weakly connected nodes, to make sure that metrics estimations would not change catastrophically from one experiment to another because of small variations in noise level, or a slightly more generous selection of regions of interests during video quantification. Examples of measures that do not satisfy this criterion are cycle order and the "small world" property, that both are sensitive to the inclusion of weak long-ranged connections \citep{papo2016beware}. We used the following list of network metrics:

\textbf{Global network efficiency} was calculated using a function from the Brain Connectivity Toolbox \citep{rubinov2010toolbox} on reciprocals for graph weights $R_{ij} = 1/w_{ij}$, and was defined as:

\[ E = \frac{1}{n} \sum_{i \neq j}^n{\frac{d_{ij}}{n-1}} \]

where $d_{ij}$ is the length of the shortest path $P_{ij}$ connecting nodes $i$ and $j$: $d_{ij} = \sum_{kl \in P_{ij}}{R_{kl}}$

\textbf{Clustering coefficient} \citep{fagiolo2007} was calculated using the Brain Connectivity Toolbox, with a function that supported weighted directed graphs:

\[ C = \frac{1}{n} \sum_i{\frac {t_i}{(k^o_i+k^i_i)(k^o_i+k^i_i-1)-2\sum_j{w_{ij}w_{ji}}}} \]

where $k^o_i$ and $k^i_i$ are out- and in-degrees of node $i$ respectively, and $t_i$ is the weighted number of directed triangles that include node $i$:

\[ t_i = \sum_{j \neq i}{\sum_{k \neq i,j}{w^{1/3}_{ij}w^{1/3}_{jk}w^{1/3}_{ki}}} \]

To estimate \textbf{network modularity}, we also used a function from the Brain Connectivity Toolbox, which calculated spectral modularity on a weighted directed graph \citep{reichardt2006community,leicht2008community}.

Our definition of \textbf{flow hierarchy} was inspired by \citep{mones2012hierarchy,czegel2015hierarchy}, but based on modified (weighted) Katz centrality \citep{katz1953original,fletcher2018katz}. To calculate Katz centrality, we assumed that each node $j$ collected flows of incoming signals through all edges $w_{ji}$ leading to this node. Activation arriving through edge $j\leftarrow i$ was proportional to the total activation $z_i$ of source node $i$, the weight of this edge $w_{ji}$, a global normalization coefficient equal to $1/\text{max}(w_{kl})$ across all edges $k\leftarrow l$ in the network, and a damping factor of $d=$ 0.9. Each node also received a small amount of constant activation $(1-d)=$ 0.1. The total activation of each node was therefore defined as:

\[ z_j = (1-d) + \frac{d}{\text{max}_{k,l}(w_{kl})} \sum_{i \neq j}{a_i w_{ji}} \]

Each node then distributed this activation to other nodes. This definition is also close to that of pagerank centrality \citep{page1999pagerank}, except that the weights are not normalized to the value of total outgoing weights for each node $i$: that is, we work with raw weights of $w_{ji}$ rather than $w_{ji}/\sum_k{w_{ki}}$. It means that a node with many outputs has a strong influence over network activation, while nodes with weak outgoing edges act as dead-ends. Similar to a standard pagerank algorithm, we solved this problem iteratively, by initializing the network with equal values of centrality, and running the equation above until convergence (typycally, $\sim$100 times). Once Katz centrality values $z_i$ were found, we used the difference between maximal Katz centrality observed in the network and mean centrality across all nodes as a measure of flow hierarchy: $h = \text{max}(z_i) - \text{mean}(z_i)$ \citep{mones2012hierarchy,czegel2015hierarchy}.

To check whether network values described above were different from values expected on a random graph, we performed \textbf{graph randomization}, using a variant of degree-preserving reshuffling \citep{maslov2002} that we generalized for directed weighted graphs. For a network with $N_E$ edges we picked $3\cdot N_E$ random pairs of nodes (nodes $i$, $j$, $k$, and $l$) that had strong connections from $i$ to $j$, and from $k$ to $l$, but weak connections or no connections from $i$ to $l$, and from $j$ to $k$ (we required $w_{ji}>w_{li}$ and $w_{lk}>w_{jk}$). We also required all four nodes to be different ($i \neq j \neq k \neq l$). Then we cross-wired these pairs of nodes, gradually randomizing network topology:

\[ \left \{ \begin{array}{l}  
w_{ji} \leftarrow w_{li} \\ 
w_{li} \leftarrow w_{ji} \\
w_{lk} \leftarrow w_{jk} \\
w_{jk} \leftarrow w_{lk}
\end{array} \right. \]

This approach to degree-preserving randomization is slightly different from the original formulation by \citep{maslov2002} in two ways. First, we explicitly don’t allow loops (self-edges) by requiring all four nodes be different. Second, we allow nodes $i$ and $k$, as well as $l$ and $j$ to be connected before the rewiring, and just swap corresponding edge weights, which seems to be a necessary adjustment for directed weighted graphs. It also means that, strictly speaking, for a weighted graph, our randomization only preserves out-degrees, but not in-degrees. Because of the requirement that $w_{ji}>w_{li}$ and $w_{lk}>w_{jk}$, for a binary directed graph our algorithm preserves in-degrees strictly, as it becomes identical to version by Maslov, while for nearly-binary graphs (bimodal or sparse), it tends to preserve in-degrees on average.

We also tested whether connectivity and positioning of selective cells within the graph was in any way peculiar, by calculating correlations between cell selectivity and several different \textbf{graph centrality measurements}. We used three centrality measures: weighted in-degree (the sum of weights of all connections to the node); Katz centrality; and clustering coefficient.

To measure whether selective cells tended to form sub-networks within the graph, we calculated \textbf{weighted assortativity}. The formula for an assortativity value (mixing coefficient) in a weighted directed network is given in \citep{farine2014weighted}, based on logic from \citep{newman2003mixing} and \citep{leung2007weighted}. The original formula from \citep{newman2003mixing} for an unweighted undirected graph defines a mixing coefficient as a Pearson correlation coefficient between properties of nodes connected by edges, taken over all edges in the graph:

\[ r=\underset{ij: a_{ij}=1}{\text{cor}}(x_i,x_j) \]

leading to the following expression:

\[ r = \frac{\frac{1}{E} \sum{x_i x_j} - [\frac{1}{E} \sum{\frac{1}{2}(x_i+x_j)}]^2} {\frac{1}{E} \sum{(x_i^2+x_j^2)}-[\frac{1}{E} \sum{\frac{1}{2}(x_i+x_j)}]^2} \]

where sums are taken over all connected edges $ij: a_{ij}=1$, and $E$ is the total number of edges.

For a weighted graph an equivalent measure can be introduced by replacing summation over edges to summation over all possible pairs of nodes $ij$, with weights $w_{ij}$ introduced in each sum. The resulting expression can be rewritten in several different forms \citep{newman2003mixing,leung2007weighted,farine2014weighted,teller2014assortative}, but instead of explicitly coding these bulky and rather confusing calculations, we used the fact that ultimately a mixing coefficient can be described as a weighted correlation across all connected directed edges $ij$ with edge values $w_{ij}$ used as correlation weights:

\[ r=\text{cor}(x_i \, , \, x_j \, , \, w_{ij}) \]

In turn, weighted correlation $\text{cor}(a,b,w)$ can be introduced through weighted covariances: 

\[ \text{cor}(a,b,w) = \frac{\text{cov}(a,b,w)}{\sqrt{\text{cov}(a,a,w) \cdot \text{cov}(b,b,w)}} \]

and weighted covariances are defined simply and intuitively as: 

\[ \text{cov}(a,b,w) = \frac{\sum_i{w_i \cdot (a_i-\bar{a})(b_i-\bar{b})}}{\sum_i{w_i}} \]

with $\bar{a}$ and $\bar{b}$ representing weighted mean values: 

\[ \bar{a}=\sum_i{w_i a_i}/\sum_i{w_i} \]

Note however that this definition may differ slightly from the one used in the Brain Connectivity Toolbox \citep{rubinov2010toolbox}.

\textbf{Unreported analyses}. For the sake of transparency, here we report all measures that were calculated, but not included in the final manuscript for being superflous or confusing: four measures of weighted directed degree assortativity (in-in, in-out, out-in, and out-out); pagerank centrality; Katz centrality on reversed graphs $\mathbf{W}^\top$; flow hierarchy for reversed graphs; node reach on direct and reversed graphs (unweighted analog of Katz centrality without attenuation); two alternative measures of cell selectivity: McFadden’s pseudo-$R$ for a logistic fit of stimulus identity to the total response of each cell, and selectivity measures calculated on peak amplitudes instead of cumulative amplitudes (the results of both calculations were qualitatively similar to those reported in the paper). We also made several attempts to estimate the prevalence of directed cycles in our networks, but decided that these measures require too much validation to be included in this manuscript. For network analysis, we also attempted to compare rewired graphs to matching random Erdos graphs, but failed to build a good generalization for a case of weighted directed graphs with an adaptive edge detection threshold.

\subsection*{Developmental Model}

The model consisted of $n$=81 cells, arranged in a 9x9 grid. The model operated in discrete time $t$, and was run for 500 epochs, 25 time steps each, or for $T$ = 12500 time steps total. At each moment of time, each cell was described by three values: its instantaneous firing rate $s_i(t)$, represented as continuous value $0 \leqslant s_i(t) \leqslant 1$; spiking threshold $h_i(t) \geqslant 0$ that slowly changed over time as described below, and a constant $\hat{s_i}$ that described the target spiking rate for each cell. Target spiking rates $\hat{s_i}$ were randomly assigned at the beginning of each simulation, and were distributed normally around 5/$n$ with a standard deviation of 1/$n$, which means that if these target spiking rates were matched, on average, at any time step, 5 out of 81 cells would be spiking.

Cells were connected to each other with "synapses" of different strengths, represented by a weight matrix $\mathbf{W}$, with weight $0 \leqslant w_{ji} \leqslant 1$. These weights were originally assigned random values, uniformly distributed between 0 and 1, except for self-connections (loops, $w_{ii}$) that were kept at 0.

At each time step we first calculated the raw activation $\mathbf{A}$ of all neurons: $\mathbf{A} = \mathbf{WS} + \mathbf{B}$, where $\mathbf{W}$ is the connectivity matrix, $\mathbf{S}$ is the vector of instantaneous spiking rates $s_i$ , and $\mathbf{B}$ is the sensory input (see below). For one cell, we have:

\[ a_i(t+1) = \sum_j{w_{ij}s_j(t)} + b_i(t) \]

These raw activation values were then adjusted down, by a formula representing global feedback inhibition, which helped to avoid run-away excitation early in development:

\[ a'_i(t+1) = \left \{ \begin{array}{l l} a_i(t+1)
& \text{if } \sum_j{s_j(t)} \leqslant \zeta \\ 
 & \\
a_i(t+1)\Big/ \Big(1 + \big(\sum_j{s_j(t)} - \zeta\big) \cdot \exp(- t/\tau_e)\Big) 
& \text{otherwise.} \end{array} \right. \]

Here $a'_i(t)$ is the final, adjusted value of activation for every cell; $\sum_j{s_j(t)}$ is the total activity in the network at the previous time step; $\zeta$ is a constant that set the level of total activity at which inhibition "turns on", and that in our case was set to \mbox{$\zeta=$ 9} (the size of the grid). The exponent $\exp(-t/\tau_e)$ served as an "easing" function that gradually "eased" the network from inhibition-dominated mode of operation to "free" operation, with a time constant $\tau_e\approx$ 2000. This “easing” formula was a practical compromise that greatly sped up our computational experiments, as it dampened network activity early on, when it was still close to randomly connected, and so prone to seizure-like activity, but allowed the simulation run on its own later in development.

The activity of each neuron $s_i(t)$ was then calculated from its total activation $a'_i(t)$:

\[ s_i(t) = f_i\big(a'_i(t)\big) \]

using a logistic activation function: 

\[ f_i(a) = 1/\Big(1+\exp\big(c\cdot(h_i(t)-a)\big)\Big) \]

where $c$ is a steepness parameter, set at $c=$ 20, and $h_i(t)$ is the current spiking threshold of cell $i$. At the beginning of each simulation, spiking thresholds $h_i(0)$ were set to random values, uniformly distributed in a narrow band between $1/(n \hat{s_i})$ and $1/(n \hat{s_i}_i)+0.1$ . During the simulation, the thresholds $h_i(t)$ were updated at each time step, to model the effect of \textbf{intrinsic homeostatic plasticity}. For this purpose, for each cell, we kept track of its running average spiking rate $\bar{s_i}(t)$, and updated both average spiking rates and spiking thresholds $h_i(t)$ by the following formulas:

\[ \bar{s_i}(t+1) = (1-\kappa)\bar{s_i}(t) + \kappa s_i(t) \]

\[ h_i(t+1) = h_i(t) - r_h(\hat{s_i} - \bar{s_i}(t)) \]

where constant $\kappa=$ 0.05 controlled the rate of averaging, and constant $r_h=$ 0.1 set the strength of homeostatic plasticity, as it set the rate at which spiking thresholds $h_i$ were allowed to adjust, to bring the discrepancy between the target spiking rate $\hat{s_i}$ and running average spiking rate $\bar{s_i}(t)$ to zero.

Once spiking activity of each neuron at the new time step $s_i(t)$ was calculated, we moved to the \textbf{spike-time dependent plasticity} (STDP) step, adjusting synaptic weights $w_{ji}$ that linked neurons in the network. In continuous time, STDP leads to an increase in weight $w_{ji}$ (from $i$ to $j$) if target neuron $j$ spikes after a spike in source neuron $i$, with a delay that is expected for spike  propagation from $i$ to $j$. If the target neuron $j$ spikes earlier than that (before, or together with neuron $i$), the weight $w_{ji}$ is decreased. The amount of weight change smoothly drops off as the delay between these two spikes increases in either direction. 

In discrete time, assuming that spike propagation always takes one time step, and neglecting the effect of smooth drop-off, STDP may be approximated by the following system of cases, with options 1 and 2 not being mutually exclusive:

\[ w_{ji}(t+1) = \left \{ \begin{array}{lll} w_{ji}(t)+\epsilon, & \text{if } s_i(t)> 0 \text{ and } s_j(t+1)> 0 \\ w_{ji}(t)-\epsilon, & \text{if } s_i(t)> 0 \text{ and } s_j(t)> 0 \\ w_{ji}(t) & \text{if } s_i(t)=0\end{array} \right. \]

where $\epsilon$ is some change in synaptic weight. 

As in our model neuronal activity $s_i(t)$ was continuous, and we wanted the synaptic change $\epsilon$ to be proportional to the overlap in neuronal activity, the non-exclusive system above may be rewritten as:

\[ w_{ji}(t+1) = w_{ji}(t) + r_w \big(s_i(t)w_{ji}(t)s_j(t+1) - s_i(t)w_{ji}(t)s_j(t)\big) \]

or

\[ w_{ji}(t+1) = w_{ji}(t)\cdot\Big(1+r_w\big(s_j(t+1)-s_j(t)\big)s_i(t)\Big) \]

where $r_w$ is a constant that controls the strength of synaptic plasticity (in this model, $r_w=$ 0.25).

Finally, we modeled \textbf{synaptic competition} by introducing a negative feedback, to limit the total sum of all inputs to each neuron, and all output from each neuron. At every time step, we used the weight matrix $\mathbf{W}$ to calculate a modified matrix $\mathbf{W}^\text{I}$, with sums of \textit{inputs} to each neuron normalized to a certain fixed value ($g=$ 1.5), and a modified weight matrix $\mathbf{W}^\text{O}$, in which the total sum of \textit{outputs} from each neuron was normalized to the same value: 

\[ w_{ji}^\text{I} = g \cdot w_{ji}/\sum_k{w_{jk}} \]
\[ w_{ji}^\text{O} = g \cdot w_{ji}/\sum_k{w_{ki}} \]

We then iteratively "moved" our actual weight matrix at each time step in the direction of the average between these two normalized matrices:

\[ w_{ji}(t+1) = 0.4 \cdot w_{ji} + 0.3 \cdot w^\text{I}_{ji} + 0.3 \cdot w^\text{O}_{ji} \]

This "sliding" approach to modeling synaptic competition was less aggressive than explicit weight normalization, and allowed for more robust model convergence.

Developing networks were activated with \textbf{simulated visual stimuli} that resembled sensory activation a real animal could have experienced when navigating in a bright-lit environment with sparsely placed black spheres. For "general" visual stimulation (used in sensitivity experiments), we repeatedly created unique collision events, with randomized original positions of a black sphere relative to the eye, final distance to the eye, and direction of movement through the visual field. We would then move the projection of this virtual sphere across the virtual retina over a course of $\tau=$ 10 time frames.  When training on looming stimuli (main series of computational experiments), we still initiated objects at random points within the visual field, with distances from the center of the visual field randomly distributed with a standard deviation of 1, but made sure that they approached the eye on a "collision trajectory", and eventually covered the entirety of the visual field. For looming and "general visual" stimuli, a projection of a sphere on the virtual retina was a solid circle, with its center moving linearly $(x,y) = (x_0,y_0)+(v_x,v_y)\cdot t/\tau$, and circle radius changing as $R(t) = R_0/(d_0 - v_z \cdot t/\tau)$. The virtual retina consisted of 81 pixels, arranged in a 9x9 grid (same dimensions as for the model network), with each pixel generating both "ON" and "OFF" responses without delay or bursting, based on the difference between two consecutive projections $\text{in}(t) = \text{XOR}(\text{img}(t),\text{img}(t-1))$. This signal was then inputted to matching nodes in the model network. When training on noise, we generated random noise with 9 pixels flicking active at any given time. 

For testing, we compared responses to flashes, crashes, and scrambled stimuli. "Crashes" were different from looming stimuli in that the change of projection radius with time was linear $R(t) = 9/2 \cdot \sqrt{2} \cdot t/\tau$, rather than realistic; this was because we used linearly expending looming stimuli in biological experiments, both in this study, and in earlier studies \citep{khakhalin2014}. The starting position of looming stimuli was slightly randomized (both $x$ and $y$ starting coordinates were normally distributed around the center of the receptive field with the standard deviation of 1), to make model responses variable, as it was necessary for our analysis. "Scrambled" stimuli were identical to "Crashes", but with all 81 pixels randomly reassigned. "Flashes" were modeled as very fast looming stimuli that took exactly 2 frames to fill the entire field of view, and with pixels reshuffled. We used this approximation instead of a simple instantaneous flash as our model was deterministic, and we needed to introduce some variability into responses to flashes, while still keeping them as close to instantaneous as possible. 

While testing networks trained on different sensory stimuli, we ran into a surprising complication: during training, different stimuli provided different levels of average activation, and so not only synaptic connections between cells were differently shaped, but also intrinsic plasticity resulted in very different activation thresholds for different neurons. This variability of excitability was however an artifact of our training method, and did not approximate real biological phenomena, as in real animals visual stimuli would happen relatively rarely, while we fed all our stimuli to the network as one intense train with no gaps. We therefore decided to let all spiking thresholds settle down before testing, to a state that was dependent only on synaptic connectivity, and not on recent stimulation history. We let the model develop for 2000 additional time steps, with only homeostatic plasticity rule on, but without STDP or synaptic scaling, while feeding neurons with Poisson random noise that activated on average 9 neurons at each time step. 

The effect of this additional adjustment step was so prominent in the model, that we hypothesize that it may be indirectly relevant for the biological tectum as well. To maintain the network \DIFdelbegin \DIFdel{of synaptic connections}\DIFdelend \DIFaddbegin \DIFadd{structure}\DIFaddend , each ensemble of synfire chains has to be regularly activated, yet the more active it is, the less excitable the neurons become, making them less likely to "win" \DIFdelbegin \DIFdel{during }\DIFdelend \DIFaddbegin \DIFadd{the }\DIFaddend competition with other ensembles during stimulus \DIFdelbegin \DIFdel{detection}\DIFdelend \DIFaddbegin \DIFadd{classification}\DIFaddend . The dynamics of plasticity in the brain would therefore pose a meta-balancing problem \citep{zenke2017temporal}\DIFdelbegin \DIFdel{: if }\DIFdelend \DIFaddbegin \DIFadd{. If }\DIFaddend intrinsic plasticity is too \DIFaddbegin \DIFadd{fast and }\DIFaddend flexible, networks that \DIFdelbegin \DIFdel{detects unusual stimuli , }\DIFdelend \DIFaddbegin \DIFadd{detect unusual stimuli would become overly excitable }\DIFaddend in the absence of these stimuli, \DIFdelbegin \DIFdel{would become overly excitable, }\DIFdelend producing high false-positive \DIFdelbegin \DIFdel{rate. Conversely, they will quickly habituate to actual stimuli , but at }\DIFdelend \DIFaddbegin \DIFadd{rates, and would habituate too fast when the stimuli are present. At }\DIFaddend the same time, \DIFdelbegin \DIFdel{they }\DIFdelend \DIFaddbegin \DIFadd{due to regular activation, these networks }\DIFaddend will have no trouble maintaining synaptic connections required for \DIFaddbegin \DIFadd{the }\DIFaddend stimulus detection \citep{litwin2014assemblies}. \DIFdelbegin \DIFdel{If however }\DIFdelend \DIFaddbegin \DIFadd{On the other hand, if }\DIFaddend intrinsic plasticity is too slow, detection networks will find it easier \DIFdelbegin \DIFdel{maintaining "optimal " }\DIFdelend \DIFaddbegin \DIFadd{to maintain optimal }\DIFaddend levels of sensitivity, \DIFdelbegin \DIFdel{but may have trouble keeping synaptic connections }\DIFdelend \DIFaddbegin \DIFadd{yet the synaptic connections that organize neurons into functional ensembles would erode and degrade due to disuse }\DIFaddend between stimulus presentations \DIFdelbegin \DIFdel{intact \mbox{%DIFAUXCMD
\citep{triplett2018emergence}}\hspace{0pt}%DIFAUXCMD
, as in the absence of spontaneous replay these synaptic connections may erode}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citep{triplett2018emergence}}\hspace{0pt}%DIFAUXCMD
}\DIFaddend . A potential solution to this problem is to cycle the network through distinct phases, with different contributions of synaptic and intrinsic plasticity, \DIFaddbegin \DIFadd{reminiscent of sleep-wake phases, and }\DIFaddend similar to how we did it in the model.

To measure the prominence of looming-resonant synfire chains in model networks we first \DIFdelbegin \DIFdel{generate }\DIFdelend \DIFaddbegin \DIFadd{generated }\DIFaddend a set of 100 slightly variable looming stimuli, as described above, and for each pair of cells $i$ and $j$ calculated the frequency at which they were activated at two consecutive time frames $L_{ji}$. If the adjacency matrix \DIFdelbegin \DIFdel{were fully shaped by imitating looming stimuli }\DIFdelend \DIFaddbegin \DIFadd{is shaped as the imitation of sensory inputs }\DIFaddend with synfire chains \DIFdelbegin \DIFdel{, we would }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citep{clopath2010stdpcoding}}\hspace{0pt}%DIFAUXCMD
, we could }\DIFaddend expect $w_{ji}$ to look like a proxy of $L_{ji}$, while if they are not related, edges of $\mathbf{W}$ and $\mathbf{L}$ would not be correlated. \DIFdelbegin \DIFdel{We therefore }\DIFdelend \DIFaddbegin \DIFadd{To quantify that, we }\DIFaddend calculated the correlation between elements of two matrices, across all edges $\text{cor}(w_{ji},L_{ji})$, and looked whether this correlation was higher than zero (Figure 8U). To estimate the contribution of synfire chain resonance to looming selectivity, we asked whether non-zero edges that seemed to contribute to selectivity calculations tended to be the ones activated consecutively during looms. To \DIFdelbegin \DIFdel{do so}\DIFdelend \DIFaddbegin \DIFadd{answer this question}\DIFaddend , we looked at a subset of edges $i \rightarrow j$ in $\mathbf{W}$ for which looming selectivity of node $j$ was higher than that of node $i$, zeroed edges for which it was not the case, and calculated a correlation between elements of this modified matrix and that of the looming stimulus representation $\mathbf{L}$. If selectivity-increasing edges within $\mathbf{W}$ \DIFdelbegin \DIFdel{were }\DIFdelend \DIFaddbegin \DIFadd{are }\DIFaddend co-aligned with edges in $\mathbf{L}$, we would expect this value of this correlation to be non-zero and positive (Figure 8V).

For \textbf{sensitivity analysis}, we either removed or greatly attenuated parts of the model, one part at a time (not cumulatively). We tried the following combinations:

\textbf{Non-looming stimuli}. In this mode, instead of training the model exclusively on looming stimuli, we used a mix of randomized transitions of a black circle across the retina, as describe above. This type of stimulation was therefore still spatially patterned, but consisted mostly of translational stimuli, with some contribution of oblique looming and oblique receding stimuli.

\textbf{Random stimulation}. The network was stimulated with random noise. Each “pixels” of the image would fire with the same probability.

\textbf{No STDP plasticity}. Instead of the spike-time-dependnet plasticity equation describe above:

\[ w_{ji}(t+1) = w_{ji}(t)\cdot\Big(1+r_w \cdot \big(s_j(t+1)-s_j(t)\big)\cdot s_i(t)\Big) \]

we used an equation with symmetrical Hebb plasticity, and no negative depression term: 

\[ w_{ji}(t+1) = w_{ji}(t)\cdot\Big(1+r_w \cdot s_j(t\DIFdelbegin \DIFdel{+1}\DIFdelend ) \cdot s_i(t)\Big) \]

\textbf{No synaptic competition}. Instead of sliding renormalization of all inputs and outputs of each neuron, we allowed synaptic weights to decay to zero: $w_{ji}(t+1) = w_{ji}(t)\cdot (1-\beta)$, where $\beta$ = 0.001.

\textbf{Weak homeostatic plasticity}. In the formula for homeostatic plasticity, instead of change coefficient $r_h$=0.1 we used $r_h$=0.01.

\section*{Acknowledgements}

My greatest gratitude is to Carlos Aizenman who encouraged me to try to publish this work as a single author, even though all experiments described here were performed on his equipment, and the materials were paid for by the money from his grant (NSF IOS-1353044)\DIFaddbegin \DIFadd{. I am very grateful to Lilach Avitan (The Hebrew University of Jerusalem) and Silas Busch (University of Chicago) for their in-depth reviews of the first version of this manuscript}\DIFaddend . I also thank Heng Xu (Shanghai Jiao Tong University) for his advice on imaging experiments; Joshua Vogelstein (John Hopkins) for his help with adaptive thresholding; Petko Bogdanov (SUNY Albany), Csilla Szabo (Skidmore College), Gerrit Ansmann (Bonn University), and Jim Belk (St Andrews University) for their help with network science and graph theory, and Sven Anderson (Bard College) for advice on model analysis.

% \section*{Declaration of Interests}

The author has no conflicts of interest to disclose.

\nolinenumbers
\bibliographystyle{apalike} % For author-year
%\bibliographystyle{unsrtnat} % For Nature-style
\bibliography{refs}

%TC:endignore
\end{document}