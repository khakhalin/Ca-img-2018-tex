\title{Khakhalin. Graph analysis of collision detection networks}

%\documentclass[twocolumn]{article}
\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[top=0.85in,left=1.5in,right=1.5in,footskip=0.75in]{geometry} % For one-column version
%\usepackage[top=0.85in,left=1in,right=1.0in,footskip=0.75in]{geometry} % For two-column version
% marginparwidth=2in


\usepackage[round, numbers, authoryear]{natbib} % Reference manager, round citations, unsorted
%\usepackage[super]{natbib} % Nature-style citations
%\setcitestyle{citesep={,}} % For nature-style, comma instead of ;

%\usepackage[switch,pagewise]{lineno} % Numbered lines for two columns
\usepackage[pagewise]{lineno} % Numbered lines for one column

%\usepackage{adjustbox} % To change margins around a table. Doesn't work?
\usepackage{tabularx} % To set table column width, if needed
\usepackage{rotating} % To write text sidewise
\usepackage{xcolor}
\renewcommand{\linenumberfont}{\normalfont\bfseries\small\color{lightgray}}
\definecolor{linkcolor}{rgb}{0.2,0.6,0.7} % Neuron-style link color
\usepackage[colorlinks=true,citecolor=linkcolor,urlcolor=blue]{hyperref}%
%\usepackage{url}

% improves typesetting in LaTeX
\usepackage{microtype}
%\DisableLigatures[f]{encoding = *, family = * }

% this package is supposed to give access to upright mu symbol via \micro
%\usepackage{siunitx} % didn't work for some reason
\usepackage{amsmath} % to enable \text tag
\usepackage{amssymb} % to enable \leqslant

% text layout
\raggedright
%\textwidth 6in 
%\textheight 9in
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

% adjust caption style
\usepackage[aboveskip=5mm,labelfont=bf,labelsep=period,singlelinecheck=off]{caption}

% this is required to include graphics
\usepackage{graphicx}

% Multicolumns
\setlength{\columnsep}{1cm}

% Titles
\usepackage{titlesec}
\titlespacing{\section}{0pc}{0.5pc}{0pc}

% Headers and footers
\usepackage{fancyhdr}
\pagestyle{fancy}
\rhead{Khakhalin AS. Graph analysis of collision detection networks. Page \thepage}
\cfoot{} % To kill footer page numbers

% Set font:
\usepackage{helvet} % Helvetica: most conservative
%\usepackage{tgheros} % TEX Gyre Heros: a bit rounder, but poor
%\usepackage{lmodern} % Fancy, nice, but unusual
\renewcommand{\familydefault}{\sfdefault} % Force this font

\begin{document} % --------------------- Document start

%TC:ignore 
% The comment above is for textcount to ignore this text.
% It will ignore everything until the endignore pair, and so on.
% We need to get down to 4500 words in the main text.

% \linenumbers % Comment to suppress line numbers

% title goes here:
%\twocolumn[
\begin{flushleft}
{\Large
\textbf\newline{Graph analysis of collision detecting networks in the tectum, and its replication in a simple computational model}
}
\newline
% authors go here:
\\
Arseny S. Khakhalin\textsuperscript{1,*}
\\
\bigskip
{1} Biology Program, Bard College, Annandale-on-Hudson, NY. 

* Correspondence: khakhalin@bard.edu


% TODO

% Fix figures

% Change Fig. to Figure everywhere

% Sort references by year, to make citations correct

\section*{Abstract}
% 179 words
Looming stimuli evoke behavioral responses in most sighted animals, yet mechanisms of looming detection in vertebrates are poorly understood. In this study, we use high-speed calcium imaging to reconstruct connectivity within small subsets of neurons in the optic tectum of \textit{Xenopus} tadpoles. We report that reconstructed networks had non-random structure, with looming selective cells serving as activation sinks, and describe differences in degree distribution and network modularity between younger and older animals. We then compare these results to predictions from a computational developmental model of the tectum, governed by spike-time-dependent plasticity, homeostatic intrinsic plasticity, and synaptic competition, and driven by structured sensory inputs. Our model developed robust looming selectivity and developed structured connectivity that replicated some effects observed in imaging experiments, including changes in modularity and degree distribution. Other effects predicted by the model (changes in hierarchy, efficiency, and spatial distribution of selective cells) were not observed in biological experiments. Finally, by comparing several reduced models, we predict which developmental rules may be most critical for the emergence of looming selectivity in the brain.
%TC:ignore 
\bigskip

\end{flushleft} % Only relevant for two-column documents, but doesn't hurt
%] % End of one-column region

\section*{Introduction}

% Create tension. Build value. Why are you reading this? What is the point? How will reading this benefit the community you belong to, or you personally? Instability (tension). Don't describe the status quo; don't build a foundation, describe the problem. Invest in tension (use: but, however, although, despite, paradox, inconsistency, unexpected, surprising, contradiction). Why does it matter? Gap in knowledge is good, but really we only care about problems that cost something.

Few sensory stimuli are as ill boding for the animal as a visual loom. A retinal projection that is small at first, but quickly grows in size, may promise a painful collision, or a meeting with a predator, and so it inherently calls for action: an avoidance maneuver, or defensive posturing, such as a blink. Moreover, to be meaningful, looming detection has to be fast. Not surprisingly, it is described in virtually every animal that uses vision, from insects to primates \citep{Pereira2016}. And yet, while our understanding of looming detection in insects has recently improved \citep{rind2016locust,von2017fruitfly}, the mechanisms that underlie collision avoidance in vertebrates are still unclear.

For vertebrates, it is well established that looming-selective circuits are located in a midbrain region known as superior colliculus in mammals, and optic tectum in all other clades \citep{frost2004review, liu2011cat, khakhalin2014, dunn2016escapesZF}. It is not known however how midbrain circuits actually perform the computations required for collision avoidance. It is also not clear whether looming detection and collision avoidance in vertebrates are innate (and so possibly hardwired), or whether they need to be learned. Finally, to navigate, animals need to calculate not just \textit{whether} a collision is about to happen, but also \textit{where} a looming stimulus is coming from. While it is known that the tectum harbors a retinotopic map of the visual field \citep{mclaughlin2003retinotopic, ruthazer2004map}, there are several competing theories about how looming detectors may be distributed within this map \citep{frost2004review}.

What type of calculation may in principle underlie collision avoidance? Across species, the logic of looming detection is varying and diverse, and includes dimming detectors \citep{ishikane2005, munch2009}, integration of opponent motion \citep{klapoetke2017looming}, and competitive spike-frequency adaptation \citep{peron2009adaptation, fotowat2011multiplexing}. Even within a single clade of anuran amphibians (frogs), animals seem to employ at least two competing mechanisms to detect looms: a non-linear response to dimming-induced retinal oscillations \citep{baranauskas2012}, and rebound of recurrent activity during edge expansion \citep{khakhalin2014, jang2016}. Moreover, at least in some cases, competing mechanisms lead to different motor responses, as described in insects \citep{card2008tradeoffs, chan2013avoidance}, fish \citep{budick2000repertoire, burgess2007twoescapes, portugues2009behaviors, temizer2015pathway, bhattacharyya2017assessment}, and tadpoles \citep{khakhalin2014}.

It may seem puzzling that a brain would combine several conflicting approaches to solve one practical problem, but this arrangement may make sense developmentally. Simple, crude ways of identifying dangerous stimuli can be used to train more sophisticated and efficient networks, capable of nuanced sensory analysis at later stages of development \citep{marblestone2016deeplearning}. We hypothesize that young aquatic animals may use "hardwired" dimming receptors \citep{baranauskas2012} both to avoid collisions \citep{dong2009}, and to "bootstrap" motion-dependent networks in the tectum. We posit that later in development, nuanced motion-detecting networks could serve as a first line of defense, identifying early phases of looming, and allowing fine course corrections \citep{khakhalin2014, bhattacharyya2017assessment}, while dimming detectors remain as a backup, mediating last-moment, less coordinated responses. Moreover, every time collision avoidance is not performed perfectly, sensorimotor networks can be refined, based on mechanosensory inputs from the lateral line and from the skin \citep{felch2016, helmbrecht2018topography}.

In this paper, we use high-speed calcium imaging and functional connectivity reconstruction to search for looming detectors within recurrent networks in the developing tectum of \textit{Xenopus} tadpoles. Tadpole tectum is uniquely suitable for studies of sensory integration, as it is excessively plastic \citep{pratt2007intrinsic, busch2019}, strongly interconnected \citep{james2015}, and develops reliable looming selectivity within about a week \citep{dong2009, khakhalin2014}, as tadpoles mature from Nieuwkoop stage 46 to stage 49. We hypothesize that looming detectors may emerge in development, with connections between tectal cells reshaped by patterned visual stimulation. Synapses in the tectum of young tadpoles exhibit spike-time-dependent plasticity (STDP) \citep{zhang1998stdp, mu2006stdp, vislay2006rf, richards2010stdp} that is known to promote the development of synfire chains: groups of neurons, sequentially connected to one another \citep{fiete2010chains, zheng2014synfire}. Synfire chains are selective for inputs that activate neurons in the same sequence in which they are connected \citep{clopath2010stdpcoding}, turning them into temporal pattern detectors, and it is easy to see how this property can be useful for looming detection.

As tadpole tectal neurons spike slowly, with broad action potentials and long refractory periods, synfire chains in the tectum would have transmission delays from one neuron to another of up to 10 ms \citep{ciarleglio2015, jang2016, busch2019}, making it possible to directly observe them with fast calcium imaging that operates at rates of $\sim$100 frames/s. If our hypothesis is true, we can expect both looming selectivity of individual cells, and statistical properties of tectal networks to change in development. We can also expect looming-sensitive architectures to spontaneously emerge in simplified models of a developing tectum \citep{gao2015simplicity, pietri2017emergence}, similarly to how it was shown for visual receptive fields \citep{bashivan2018neural}, grid cells \citep{banino2018grid}, and decision circuits \citep{haesemeyer2018convergent}. We should also be able to use modeling to explore the range of developmental rules that enable looming selectivity, thus building verifiable predictions for biological experiments \citep{linderman2017constrain, bassett2018models}. 

With this in mind, here we ask three specific questions. We test whether it is possible to reconstruct functional connectivity between tectal neurons from calcium imaging recordings. Then, we investigate whether the topology of reconstructed connectivity graphs in either younger or older tadpoles may support collision detection. Finally, we check whether looming detection emerges during reinforcement learning in a computational model, and whether the statistics of simulated looming-selective networks matches the statistics of biological networks. We show that our model predicted some, but not all aspects of topology and functionality of looming-sensitive networks, making it hard to draw simple conclusions. Nevertheless, we report several new findings, including some surprising positive and negative results.

\section*{Results}

For all mathematical methods, in the main text we provide their names and ways to interpret their results, but leave definitions for the Methods section. For statistical analyses, p-values are reported without correction, and we interpret them according to Fisher, rather than Neyman-Pearson philosophy \citep{greenland2016}. Many of our analyses are not fully independent, but complementary, as they ask related questions, but rely on differently formulated null hypotheses. At the interpretation step, we pay more attention to hypotheses supported by several alternative analyses. All code and summary data for this paper is available at:  \url{https://github.com/khakhalin/Ca-Imaging-and-Model-2018}.

% Todo: once we know where the data is hosted, update

We performed calcium imaging experiments in 14 stage 45-46 tadpoles, and 16 stage 48-49 tadpoles, recording responses from 128$\pm$40 tectal cells (between 84 and 229 in individual experiments). Here and below “$\pm$” after the mean denotes standard deviation. Unless stated otherwise, sample sizes $n=$ 14 and 16 animals for stage 46 and 49 tadpoles apply to all analyses between younger and older animals in this study. To each tadpole, we presented a sequence of three different stimuli, always in the same order: a dark-on-light "Looming" stimulus, followed by a full-field dark "Flash", followed by a spatially "Scrambled" looming stimulus (Figure 1A). Scrambled stimuli were identical to looming, except that the visual field was split into a 7x7 grid of square tiles, and these tiles were randomly rearranged in space. In total we presented 60$\pm$11 stimuli to every animal, which means that a stimulus of every type was presented 20$\pm$4 times. We recorded high speed (84 frames/s) calcium imaging signals \citep{xu2011, truszkowski2017} from one layer of “deep” principal tectal neurons in the tectum (Figure 1B,C); extracted fluorescence traces (Figure 1D,E), and inferred instantaneous spiking rate for each neuron within every frame (Figure 1F,G).

\begin{figure*}[t!]
\includegraphics[width=\linewidth]{fig1.pdf}
\caption{
Experimental design overview. \textbf{A}. Visual stimulation: four representative frames from each stimulus type. \textbf{B}. Schematic of the preparation, with stimulation fiber on the left, and microscope objective on top. \textbf{C}. View of the optic tectum during calcium imaging recording. \textbf{D}. Regions of interest for video in C, with darker cells having stronger responses. \textbf{E}. Typical fluorescence responses to flash (F), scramble (S) and looming (L) stimuli from three cells in the tectum. \textbf{F}. Spiking estimations for these fluorescence traces. \textbf{G}. Average full-brain responses to stimuli of every type, wich 95\% confidence interval band, for one representative experiment. }
\end{figure*}

\subsection*{Responses and stimulus selectivity}

As previously reported in electrophysiology experiments \citep{khakhalin2014}, responses to flashes were fast, with a sharp peak, with little recurrent activation after the peak, while responses to looming stimuli were slower, and were followed by a strong recurrent activation (Figure 1G). Responses to both looming and scrambled stimuli were highly variable from one animal to another (Figure 2A), which may indicate either an inherent variability of network configurations from animal to animal, or different levels of inhibition across preparations. We did not quantify differences in response shapes, and proceeded with the analysis of response amplitudes.

The total output of observed networks tended to be higher in response to looming stimuli than to flashes (on average, 39$\pm$29\% higher for younger; 25$\pm$25\% higher for older tadpoles; $p_{t1}=$ 2e-4 and 1e-3 respectively; Figure 2B), with no change in this preference in development ($p_t$=0.15). There was no difference in response amplitude between looming and scrambled stimuli (average difference of $-$0.03$\pm$0.15 and $-$0.01$\pm$0.20; $p_{t1}=$ 0.45 and 0.78 for younger and older animals respectively; no difference in development $p_t=$ 0.80). These results support our prior observation that the total tectal response in tadpoles depends mostly on the dynamics of visual stimuli, rather than on their geometry \citep{khakhalin2014, jang2016}.

\begin{figure*}[t!]
\includegraphics[width=\linewidth]{fig2.pdf}
\caption{
Selectivity analysis. \textbf{A}. Average brain responses (sum of activity of all recorded cells over time), for each stimulus type, in each experiment. Black lines show grand averages across experiments, separately for younger and older tadpoles. \textbf{B}. Cumulative full brain response (an integral under the curves shown in A) for each experiment, across three stimulus types. \textbf{C}. A sample selectivity map, with hue coding preference for Flash (more red), Scrambled (more green), and Looming (more blue) stimuli. \textbf{D}. Average histograms of cell selectivity distributions, for younger (violet) and older (black) tadpoles. Younger tadpoles have more cells with high selectivity. \textbf{E}. A correlation between Flash-Scramble and Flash-Looming selectivity of individual cells in a typical experiment. \textbf{F}. A correlation between Scramble-Looming and Flash-Looming selectivity of individual sells in the same experiment. \textbf{G}. Stimulus encoding in different experiments; no difference in development.}
\end{figure*}

To quantify stimulus selectivity (Figure 2C), for each tectal cell we calculated Cohen’s $d$ effect sizes between cumulative responses to different stimuli. We considered two measures of selectivity: that for "looming over flash" (a type of selectivity that may rely on both stimulus dynamics and its spatial organization), and "looming over scrambled" (that can only rely on spatial properties of stimuli, as they had the same dynamics). Looking at cumulative responses was of course a simplification, as in real life, animals respond to stimuli even as they are still unrolling \citep{peron2009adaptation, khakhalin2014}, and not after a timed 1 second-long presentation. We however had no way to justify any type of dynamic thresholding, and so opted for the simplest possible approach.

On average, tectal cells were selective for looming stimuli (Figure 2D; within-brain mean $d$ of 0.67$\pm$0.50 and 0.46$\pm$0.47 in younger and older tadpoles respectively), with no difference between stages in terms of both mean and variance ($p_t=$ 0.3, 0.3). The share of cells that responded to looming stimuli stronger than to flashes also did not change in development (84$\pm$23\%, 77$\pm$21\%; $p_t=$ 0.4). Compared to younger brains, older brains had fewer highly selective cells (Figure 2D, right side of the curve). The gap between top-selective (90th percentile) and median selective cells was larger in younger (0.75$\pm$0.26) than in older tadpoles (0.53$\pm$0.27; $p_t=$ 0.03). These results were unexpected, as older tadpoles perform better in collision avoidance tests \citep{dong2009}, and we expected them to develop a subset of looming-selective cells, as described in adult frogs \citep{nakagawa2010otneurons, baranauskas2012}, and other vertebrates \citep{wang1992pigeon, wu2005pigeon, liu2011cat}. Yet in our experiments a subpopulation of strongly selective cells not only did not emerge in older animals, but became less prominent.

We then considered a second, more computationally demanding definition of selectivity: a preference for spatially organized Looming stimuli over Scrambled stimuli. On average, tectal cells did not have a preference between these two stimuli (average selectivity of $-$0.07$\pm$0.33 in younger tadpoles, $-$0.04$\pm$0.49 in older ones; no change in development $p_t=$ 0.9). The share of cells that responded to looming stronger than to scrambled was at a chance level for both developmental stages (46$\pm$31\%, 48$\pm$37\%, $p_t=$ 0.9), and there was no change in either within-brain variance of this selectivity ($p_t=$ 0.9), or the 90$-$50 percentile asymmetry of values ($p_t=$ 0.8).

We found that selectivity for scrambled stimuli over flashes correlated with selectivity for looming stimuli over flashes (Figure 2E) in both developmental groups: average within-brain $r=$ 0.82$\pm$0.13, $p_{1t}=$3e-12 for younger animals, and 0.75$\pm$0.18, $p_{t1}=$ 3e-11 older ones, with no change in development ($p_t=$ 0.3). On the contrary, the preference for looming over flashes did not correlate with preference for looming over scrambled (Figure 2F; $r=$ 0.03$\pm$0.29, $p_{1t}=$ 0.7 for stage 46; 0.13$\pm$.30, $p_{1t}=$ 0.1 for stage 49). This further confirmed that the majority of cells in the tectum responded to stimulus dynamics, and not to its geometry.

Finally, as a holistic way to quantify tectal network selectivity, we looked at our ability to predict stimulus identity from recorded tectal responses in all cells \citep{avitan2016limitations}: a measure known as "stimulus encoding". We ran a multivariate logistic regression on one half of the data, linking amplitudes of responses in each cell to the type of stimulus used in each trial. Then we measured the quality of this linkage on the second half of recorded data (Figure 2G). The quality of prediction was rather low: 59$\pm$12\% for younger, and 62$\pm$13\% for older tadpoles, with no change in development ($p_t=$ 0.6).

% HORSE

To assess the variability of responses from one tectal cell to another within each brain, we performed exploratory factor analysis (principal component analysis, followed by promax rotation) of responses to looming stimuli within each preparation. The first and second principal components explained on average 19$\pm$7\% and 4$\pm$1\% of variance in younger tadpoles, and 24$\pm$14\% and 3$\pm$1\% in older tadpoles, with second component encoding response timing (Figure 3A). Cells with early responses to looming stimuli tended to group together within the recorded field (Figure 3B) in each of 30 experiments (see Methods). We assume that early-responding cells clumped together because of the known retinotopic arrangement of neurons in the tectum \citep{ruthazer2004map}, that made tectal activation follow and reproduce gradual unrolling of a looming stimulus on the retina. Across cells, the latency of average response correlated with distance from the retinotopy center (Figure 3C; $r=$ 0.35 $\pm$ 0.24; correlations individually significant in 25/30 experiments), despite our latency estimations being noisy, especially for low-amplitude cells (see Methods). Curiously, while visual projections to the tectum are known to be actively refined in development \citep{sakaguchi1985refinement, ruthazer2004map, munz2014hebbian}, it seems that the precision of functional retinotopic maps did not differ between younger and older tadpoles, as correlations between cell position and early component prominence did not change in development ($r=$ 0.63 $\pm$ 0.21 and 0.57 $\pm$ 0.25 respectively, $p_t= $0.5), which is similar to prior reports in Zebrafish \citep{avitan2016limitations}.

Knowing where the center of a looming stimulus was projected within the tectum, we could check whether looming-selective cells tended to be found in the center of the expanding activation area (as it would be expected if collision detectors formed a meta-retinotopic map \citep{frost2004review}), or at the periphery. We found that selectivity for Looming over Flash tended to decrease with distance from the projection center (Figure 3C) for both stage 45 (average $r=-$0.37$\pm$0.27; individual correlations $p_r<$0.05 in 12/14 experiments), and stage 49 tadpoles (average $r=-$0.09$\pm$0.35; $p_r<$0.05 in 12/16 experiments), indicating that looming-selective cells tended to be in the center of the emerging spatial response. Similarly, at both developmental stages, selectivity decreased with response latency (stage 46: $p_r<$0.05 in 11/14 animals, average $r=-$0.29$\pm$0.11; stage 49: $p_r<$0.05 in 10/16 animals, average $r=-$0.16$\pm$0.21). Both correlations were weaker in older tadpoles ($p_t=$ 0.02 for distance-selectivity, $p_t=$ 0.03 for latency-selectivity), suggesting a more uniform distribution of looming-selective cells within the network.

\begin{figure*}[t!]
\includegraphics[width=\linewidth]{fig3.pdf}
\caption{
Spatiotemporal organization of responses. \textbf{A}. Two first components identified in tectal responses (Principal Component Analysis with promax rotation).  \textbf{B}. Cells in a sample experiment, colored by the contribution of early (green) and late (purple) response components. Black cross shows the estimated position of the retinotopy center. \textbf{C}. Average change of Flash-Looming selectivity with distance from the retinotopy center, for stage 46 (left) and stage 49 (right) tadpoles. \textbf{D}. Adjusted correlation matrix for instantaneous activation of different neurons across all stimuli in a sample experiment (same as in B). \textbf{E}. Ensembles identified from the correlation matrix (same sample experiment). \textbf{F}. The number of ensembles, identified in younger and older tadpoles. \textbf{G}. Maximal modularity of ensemble partition. \textbf{H}. Ensemble spatial compactness.}
\end{figure*}

\subsection*{Variability and neuronal ensembles}

We then checked whether \textbf{trial-to trial variability} of tectal responses increased in development, as it happens to spontaneous activity in the tadpole tectum \citep{xu2011}. We performed a principal component analysis of trial by trial population responses, and looked at the total number of components needed to describe 80\% of variance across cells \citep{avitan2017spontaneous}. We found that this number was similar in stage 46 and 49 tadpoles, with a possible mild increase in response richness in older animals (insignificant for each stimulus alone, but consistent across stimuli: 51$\pm$14 and 65$\pm$28, $p_t$=0.1 for looming stimuli; 49$\pm$12 and 62$\pm$32, $p_t$=0.2 for flashes; 51$\pm$14 and 64$\pm$26, $p_t$=0.1 for scrambled).

To better describe and visualize network activation variability, we used spectral clustering to find \textbf{ensembles} of tectal cells that were active or silent together on a trial-by-trial basis \citep{thompson2016ensembles}. Unlike for spontaneous activity, we could not easily aggregate activity states into clusters \citep{avitan2017spontaneous}, as activation in our networks was driven by shared sensory inputs. Instead, we subtracted average responses of each cell from its responses in individual trials, and calculated pairwise correlations on the remaining “anomalies” of trial-by-trial activation (Figure 3D). We turned these correlations into pairwise distances in a multidimensional space, ran a series of spectral clustering partitions \citep{ng2002spectral}, and of all possible partitions, picked the one that maximized spectral modularity on a similarity graph \citep{newman2006modularity, gomez2009community} (Figure 3E; see Methods for details). We found that the number of ensembles did not differ between younger and older tadpoles (Figure 3F; 10$\pm$5 in stage 45, 11$\pm$11 in stage 49; $p_t$=0.9), but in older tadpoles, ensembles were more coordinated with each other,  producing lower values of network modularity (Figure 3G; 0.14$\pm$0.05 to 0.09$\pm$0.06, $p_t$=0.03). Tectal ensembles also tended to be spatially localized (Figure 3H), as cells within each ensemble were closer to each other on average than to cells from different ensembles, both in younger (29$\pm$9\% closer) and older animals (25$\pm$10\% closer; no difference in development $p_t$=0.3).

\subsection*{Network reconstruction}

\begin{figure*}[t!]
\includegraphics[width=\linewidth]{fig4.pdf}
\caption{
Connectivity reconstruction.  \textbf{A}. Delayed correlations between activities of different neurons. \textbf{B}. Adjusted transfer entropy estimations. \textbf{C}. Estimated weighted adjacency matrix. \textbf{D}. Connectivity reconstruction for one experiment, with location of each cell preserved. Node color indicates its selectivity. \textbf{E}. Same graph, optimized for graph presentation. Position of individual nodes no longer represents the position of tectal cells. \textbf{F}. In-degree distribution for younger (violet) and older (black) tadpoles, shown in log-scale. Significant changes for individual degrees ($p_t<$ 0.05) are marked with asterisks. \textbf{G}. Same as C, but for out-degree distributions. \textbf{H}. Network power increased in development, as older tadpoles had a slightly sharper slope to their degree distributions. }
\end{figure*}

The high speed of imaging used in this study (84 frames/s) allowed us to look not just at instantaneous correlations between activation of individual neurons, but at the \textit{propagation} of signals through the network. To reconstruct network connectivity, we calculated pairwise transfer entropy \citep{gourevitch2007te, stetter2012te} between activity traces of individual cells. Intuitively, for each pair of neurons $i$ and $j$ we quantified the amount of additional information that past activity of neuron $i$ can offer to predict current activity of neuron $j$. This is conceptually similar to calculating a cross-correlation between the activity of neuron $i$ at each frame $t$, and the activity of neuron $j$ at each frame $t+1$ (Figure 4A), except that transfer entropy has higher power, lower noise, and does not make assumptions about the type of influence neuron $i$ has over neuron $j$ (\citealt{stetter2012te}; Figure 4B).

In our experiments, all tectal neurons received shared inputs from the eye, that recruited them in a similar manner in every trial. This complicated connectivity inference, as neurons could spike in a sequence both because they were connected, and because they received innervation from sequentially activated areas of the retina \citep{mehler2018lure}. To compensate for shared inputs, we randomly reshuffled trials for every neuron, calculated average transfer entropy on reshuffled data, and subtracted it from the value obtained on trial-matched data \citep{gourevitch2007te, wollstadt2014te}. In essence, it is similar to working with deviations from an average response, and quantifying whether these deviations tended to propagate through the network, from one neuron to another. The reshuffling step also allowed us to calculate $p$-values for each pair of neurons, and quantify how unusual the value of transfer entropy was, compared to a value arising from shared inputs alone.

We interpreted transfer entropy values as approximations of weights in a connectivity matrix $\textbf{W}$, with $w_{ji}$ describing the strength of connection from neuron $i$ to neuron $j$ (but see \citealt{mehler2018lure}). We calculated $\textbf{W}$ and corresponding $p$-values independently on looming, flash, and scrambled stimuli, and used these independent estimations to ensure some level of internal replication within each experiment (see Methods). Of all possible edges, only 1.6$\pm$1.4\% were found to be non-zero in all three independent analyses, which was significantly higher than 0.6$\pm$0.09\% expected if edges were assigned at random (paired t-test $p_{tp}=$ 9e-9). In each experiment, we then introduced cut-offs on edge $p$-values, to exclude weak noisy edges from the connectivity graph (Figure 4C). As noise levels varied across recordings, we adjusted these cut-offs \citep{stetter2012te}, making the number of non-zero edges in each reconstruction equal to the number of recorded cells (Figure 4D,E). The effective $p$-value cut-offs were between 0.001 and 0.007 depending on the experiment (median of 0.004).

The simplest statistical property of a network is its \textbf{degree distribution}: the share of nodes with different number of incoming ($k_{in}$) and outgoing ($k_{out}$) connections. We compared rounded degree distributions between networks detected in younger and older tadpoles (Figure 4F,G), and found that more developed networks contained fewer unconnected cells ($k_{in}=$ 0, $p_t<$0.03) and fewer cells with high number of connections ($k_{in}=$ 5, $k_{out}=$ 6, $p_t=$ 0.01 in both cases), but more cells with intermediate number of connections ($k_{in}=$ 2, $p_t=$ 0.001, and $k_{out}=$ 2, $p_t=$ 0.04). When we approximated degree distributions (excluding $k=$ 0) with a power law (Figure 4H), the power constant $\gamma$ was smaller in younger (1.48$\pm$0.19) than in older tadpoles (1.82$\pm$0.25, $p_t=$ 2e-4), consistent with a steeper drop from the rate of occurrence of weakly connected to highly connected cells. It suggests that older tectal networks had more chains of connected neurons (degree $k=$ 1) and forks ($k=$ 2), while younger neurons had more hyperconnected hubs ($k>$5) and unconnected nodes ($k_{in}=$ 0), which matches the expectation for STDP-driven networks \citep{fiete2010chains}.

An unusual feature of our protocol, compared to most calcium imaging techniques, was that the signal-to-noise ratio varied greatly from one cell to another, depending on how far it was from the focal plane, and how much dye it absorbed through the partially exposed membrane during staining. As a result, the share of cells with weak signals that appeared unconnected to the rest of the network varied across experiments, and was dependent on extraneous circumstances, such as the physical curvature of the preparation. To ensure that poorly resolved cells did not bias our conclusions too strongly, we restricted further network analysis to the largest weakly connected component of each network. There was no difference in the number of weakly connected components detected in younger and older tadpoles (50$\pm$14 and 50$\pm$26 for stages 45 and 49 respectively, $p_t=$ 0.9), but in older animals the largest weakly connected component included a higher share of observed cells (50$\pm$6\% and 64$\pm$12\% for younger and older networks respectively; $p_t=$ 4e-4), which was to be expected given the change in degree distributions described above.

A known theoretical prediction for networks dominated by spike-time-dependent plasticity (STDP) is that with time neuronal connections would become highly asymmetric \citep{pratt2008recurrent, richards2010stdp}. Indeed, if cells $i$ and $j$ are reciprocally connected, every time $j$ spikes after $i$, STDP would increase the weight $w_{ij}$ , but decrease $w_{ji}$ \citep{abbott1996ltpsequence, fiete2010chains}. We found that in our data, \textbf{the share of bidirectional edges} (with both $w_{ij}$ and $w_{ji}>$0) among all detected edges was smaller (0.3$\pm$0.3\%) than expected for random edge assignment in graphs of our size (0.4$\pm$0.1\%, paired $p_t=$ 0.02), indicating asymmetric information flow in the tectum. Moreover, the share of bidirectional edges decreased in development, from 0.4$\pm$0.3\% in younger animals to 0.2$\pm$0.2\% in older animals ($p_t=$ 0.03), suggesting that STDP was still shaping emerging network topology at these developmental stages.

We then looked at whether connected cells were more likely to be located spatially closer to each other. We found that the \textbf{average distance between connected cells} was indeed shorter than the average distance on a randomized graph: 18$\pm$10\% shorter for stage 45, and 17$\pm$8\% shorter for stage 49 tadpoles (individually significant with $p_t<$0.05 for 13/14 and 16/16 experiments respectively). Contrary to our expectations, and in contrast to what is known about visual inputs to the tectum \citep{tao2005refinement}, the intra-tectal connectivity did not become more compact in development ($p_t$=0.7). This may suggest that tectal networks rely on far-reaching recurrent connections to integrate visual information across the visual field \citep{baginskas2009recurrent, liu2016jumbo, jang2016}.

\subsection*{Network properties}

For each connectivity graph reconstructed from tadpole tecta, we calculated several standard network properties, and compared them between developmental stages. We also checked whether our results were statistically unusual for a network of this size, by comparing network properties of our graphs to that of matching randomized graphs \citep{ansmann2012surrogate}. To do so, we randomly rewired edges between nodes, while keeping the distribution of edge weights $w_{ji}$, and the number of non-zero edges adjacent to each node (node degree) fixed, as  generalization of degree-preserving rewiring \citep{maslov2002} for weihted directed graphs. 

\begin{figure*}[t!]
\includegraphics[width=\linewidth]{fig5.pdf}
\caption{
Global network properties. Filled markers show values from experiments; empty markers show values for matching rewired networks; black squares show averages. Network diagrams to the right of each plot present sample graphs with low and high values of respective network measure. \textbf{A}. Global network efficiency was lower than expected for stage 46 tadpoles. \textbf{B}. Global clustering was higher than expected by chance, for both developmental stages. \textbf{C}. Network modularity increased in development.  \textbf{D}. Network hierarchy was higher than expected by chance, for both developimental stages. }
\end{figure*}

The measure of average "connectedness" in the graph, known as \textbf{network efficiency}, is defined as the average shortest path connecting two random nodes in the network \citep{latora2001efficiency, rubinov2010toolbox}. This value is high when short paths made of high-weight edges tend to connect any two randomly chosen nodes in the graph, making it easy for signals to propagate within the network; the value is low when some pairs of nodes are far from each other on a graph (Figure 5A). We found that network efficiency (0.004$\pm$0.002 for stage 46, 0.002$\pm$0.002 for stage 49 tadpoles) was slightly lower than expected for a randomized network with matching degree distribution ($d=-$0.3, paired $p_t=$ 0.04 and $d=-$0.3, paired $p_t=$ 0.06 for younger and older tadpoles respectively). It was not clear whether efficiency changed with age ($d=-$0.8, $p_t=$ 0.06).

The \textbf{global clustering coefficient} describes the small-scale heterogeneity in the network \citep{fagiolo2007}, and is defined as the relative frequency of two neighboring nodes being a part a triangle with a third node connected to both of them (Figure 5B). The value of clustering coefficient in our networks was very small (2.4$\pm$2.5 e-3 for stage 46, 1.5$\pm$1.6 e-3 for stage 49 animals), but slightly larger than expected in a randomly rewired network with same degree distribution ($d=$ 0.5 and 0.6, paired $p_t=$ 0.01 and 0.02 for younger and older animals). This means that neurons with more than two connections were likely to form clusters. There was no change in clustering in development ($d$=$-$0.4, $p_t$=0.3, Fig.).

Network \textbf{modularity} is the most commonly used measure of mesoscale network heterogeneity \citep{newman2006modularity, leicht2008community}. A network with high modularity can be split into a set of sub-networks, with higher density of connections within each sub-network, and weaker connections between them (Figure 5C). In our experiments, reconstructed tectal networks seemed slightly more modular than randomly rewired networks ($d=$ 0.2 and 0.3, paired $p_t=$ 0.2 and 0.06, for stages 46 and 49 respectively), and network modularity increased in development ($d=$ 1.0, $p_t =$ 0.01).

\textbf{Flow hierarchy} is a measure of structural hierarchy in the network \citep{mones2012hierarchy}, assessed through flows of activation propagating through it. We based our measure of hierarchy on the distribution of Katz centrality values (\citealt{katz1953original, fletcher2018katz}; see below for definitions). Intuitively, hierarchy is high when a network has groups of "input" and "output" nodes, with connections between them largely pointing in the same direction, as it happens in layered feed-forward networks; hierarchy is weak in random networks, or networks that consist on cycles with no clear inputs and outputs \citep{czegel2015hierarchy}. We hypothesized that a network of dedicated looming detectors may exhibit flow hierarchy, with more edges leading from "feeder neurons" to "detector neurons". Indeed, tectal networks were more hierarchical than randomized networks with matching degrees distributions (Figure 5D; $d=$ 1.5 and 1.1, paired $p_t=$ 1e-04 and 1e-03 for younger and older tadpoles respectively). There was no difference in flow hierarchy in development ($d=-$0.3, $p_t=$ 0.4).

\subsection*{Selectivity Mechanisms}

Even though the exact architecture of collision-detecting tectal networks is unknown, it is safe to assume that looming-selective neurons have to integrate streams of information coming from different parts of the visual field. We therefore hypothesized that the position of selective neurons within connectivity graphs would not be random \citep{timme2016degree}. To verify that, for each reconstructed network we tested correlations between looming selectivity of each neuron and values that quantify its place within the network, known as measures of \textbf{node centrality}.

\begin{figure*}[t!]
\includegraphics[width=\linewidth]{fig6.pdf}
\caption{
Local network properties for cells selective for looming stimuli. \textbf{A}. Cells with higher Katz centrality had a weak tendency to be selective for looming stimuli. A diagram inset illustrates the concept of Katz centrality: for this graph, cells with low centrality (sources) are pale, while cells with higher centrality (sinks) are darker. \textbf{B}. Correlation coefficients for Katz centrality of each cell and its selectivity for looming stimuli, calculated for each experiment, and shown by developmental stage. \textbf{C}. Correlation coefficients for node in-degree and its selectivity for looming stimuli, in each experiment, shown by developmental stage. \textbf{D}. Correlation coefficients (Spearman) for average activation of each cell, and its selectivity for looming stimuli, calculated for each experiment, and shown by developmental stage. }
\end{figure*}

To identify information sinks, for each cell we calculated its \textbf{Katz centrality} within the graph \citep{katz1953original, fletcher2018katz}. By definition, nodes with high Katz centrality have many paths leading to them, so a spike originating at random within a graph is more likely to eventually cause activation of these nodes, compared to nodes with low Katz centrality. We found that when all cells from all experiments are considered (Figure 6A), there was a very weak but significant correlation between looming selectivity of cells and their Katz centrality ($r =$ 0.02, $p_{r} =$ 1e-6, $n=$2487). More convincingly, correlation coefficients between Katz centrality and looming selectivity were positive in 19/30 experiments (Figure 6B; average $r=$ 0.09$\pm$0.20 $p_{t1}=$ 0.03); there was no change of this effect in development ($p_t=$ 0.5).

A node may have high Katz centrality for two reasons: it may receive a higher number of incoming connections (higher in-degree), or have chains of directed edges leading to it. To see which of these patterns may be at work here, we checked whether looming-selective cells tended to receive more incoming connections, compared to non-selective cells \citep{litwin2014assemblies}. When all points were considered, there was no correlation between in-degree and cell selectivity ($p_{r}=$0.3, $n=$ 2487), but for individual experiments correlation coefficients between in-degree and selectivity were positive in 20/30 of cases (Figure 6C, $p_{t1}=$ 0.03). There was no change of this correlation in development ($p_t=$ 0.5).

As cells with higher Katz centrality tend to be activated more often \citep{fletcher2018katz}, we checked whether selectivity for looming stimuli correlated with cell \textbf{average spiking activity} in our recordings. We found that both in younger and older animals, actively spiking cells tended to have stronger looming selectivity (for stage 46 average $r_S=$ 0.34$\pm$0.20, $r>$ 0 with $p_{t1}=$ 2e-5; for stage 49 $r_S=$ 0.14$\pm$0.26, $p_{t1}=$ 0.04, indicating a significant decrease with development $p_t=$ 4e-4; across all cells $r_S=$ 0.07, $p_r=$ 4e-4). This matches predictions from our studies of intrinsic excitability in the tectum \citep{busch2019}. For this analysis we used Spearman rather than Pearson correlation, as 4 experiments had single neurons (one per experiment) with extremely high activity levels that acted as influential points. Excluding these 4 neurons and using Pearson correlation yielded very similar results.

If looming-selective cells gather information from the network, it was also plausible to expect that they would be connected to other selective cells more often than to non-selective ones. To test this, we looked into \textbf{assortativity of selectivity}: a weighted correlation between selectivity scores of cells connected by edges, with strength of these edges taken as weights (see Methods). We found that in both younger and older tadpoles, selectivity values for connected cells correlated (for stage 46: $r=$ 0.07$\pm$0.13, $p_{t1}=$ 0.04, individual $r>$ 0 in 9/14 experiments; for stage 49: $r=$ 0.07$\pm$0.10, $p_{t1}=$ 0.02; individual $r>$ 0 in 10/16 experiments). This suggests that similarly selective cells indeed tended to be connected to each other. There was no change in this effect over development ($p_t=$ 0.9).

% Plot for assortativity shows perfectly no change. Is it worth showing it here?

Finally, we checked whether higher average activity of looming-selective cells linked them into tight clusters, or "rich clubs". We calculated \textbf{local clustering coefficient} for each cell, and checked whether it correlated with cell selectivity. We found that local clustering coefficient did not correlate with cell selectivity both when all cells were pooled together ($p_r=$ 0.6, n=2487), or across individual reconstructions (correlation coefficients across experiments were not different from zero; $p_t1=$ 0.25, n=30). This shows that while selective cells tended to be connected to each other, they did not form tight clusters.

Together these results suggest that the distribution of selective cells in tectal networks was not random.

%We also hypothesized that selectivity for looming stimuli may gradually “accumulate” as the signal propagates through the network, and so “downstream” cells on the receiving end of a strong edge within a graph would, on average, be more selective than “upstream” cells. This hypothesis proved to be wrong in both younger and older tadpoles: across all experiments, 52$\pm$7\% of strong edges (top 25\% of $w_{ji}$ values) led from cells with less to more selective cells, which is not different from chance rate ($p_{t1}$=0.1 for analysis across experiments, $n$=30). A weighted average increase in selectivity between two cells connected by an edge was 0.02$\pm$0.07 (not different from zero: $p_{t1}$=0.2, $n$=30), and there was no change in this value in development ($p_t$=0.2, $n$=14, 16).

\subsection*{Developmental Model}

As our recordings were noisy, and sample sizes relatively small, it was hard to expect that we would uncover clear links between selectivity and connectivity of tectal cells in experimental results alone. To compensate for these limitations, and provide a counterpoint to experimental results, we built a mathematical model of the developing tectum, and ran its results through \textit{exactly same} set of analyses that we used for our experiments. The model consisted of 81 artificial neurons, arranged in a 9$\times$9 grid, that were all originally connected to each other (every neuron to every neuron) with random positive (excitatory) synaptic weights (Figure 7A). The model operated in discrete time, in 10 ms increments, and we interpreted the output of each neuron at each moment of time as its instantaneous firing rate. At each time step we looked at the network activity in the previous step, calculated total synaptic inputs to each neuron, and used a sliding logistic function to translate these synaptic inputs to postsynaptic spiking (see Methods for details).

\begin{figure*}[!t]
\includegraphics[width=\linewidth]{fig7.pdf}
\caption{
Developmental model. \textbf{A}. Model diagram (see text). \textbf{B}. Typical adjacency matrix before simulation (random seed). \textbf{C}. Typical adjacency matrix by the end of simulation. \textbf{D}. Visualization of neuronal connections in space, with cell positions properly represented. Node color shows its looming selectivity. \textbf{E}. Same connectivity graph as in C, but rearranged to reveal its structure. \textbf{F}. Evolution of all input weights to one sample cell, over the course of simulation. \textbf{G}. Evolution of spiking threshold for 9 sample cells, over the course of simulation. \textbf{H}. Response amplitudes to three testing stimuli (Flash, Scramble, Looming) for all cells in one simulation. }
\end{figure*}

We introduced three simple developmental rules: spike-time-dependent plasticity (STDP), homeostatic plasticity, and synaptic competition. Our implementation of STDP approximated biological STDP observed in the tectum \citep{zhang1998stdp, mu2006stdp}: if two cells were active one after another in two consecutive time frames, and they were connected with a synapse, the weight of this synapse was increased. Conversely, if two cells were active within the same time frame, the weight of a synapse connecting them was decreased, as it means that one cell would try to activate another cell immediately after it had spiked. The homeostatic plasticity rule adjusted excitability thresholds, trying to keep spiking output of each neuron constant on average \citep{pratt2007intrinsic, turrigiano2011}. Synaptic competition attempted to keep as close to constant as possible both the sum of synaptic inputs to each neuron, and the sum of outputs from it. In practice, it means that every time a synapse connecting neurons $i$ and $j$ increased in strength, all output synapses of neuron $i$ and all input synapses of nuron $j$ were scaled down \citep{cohen2002synreview, munz2014hebbian, hamodi2016nmda}.

With these three developmental rules at play, we exposed the model to patterned sensory stimulation that imitated retinotopic inputs from the eye. We hypothesized that during collision avoidance in real tadpoles, STDP-driven changes in the tectum may be controlled and amplified by a global learning signal that arrives after unsuccessful collision avoidance \citep{savin2014stdpreward, aswolinskiy2015stdpreward}, and originates either in dimming receptors in the retina \citep{baranauskas2012}, or in mechanosensory systems of the hindbrain \citep{pratt2009multisens, felch2016, truszkowski2017}. This hypothesis is known as the eligibility trace model, in which changes in synaptic weights do not happen immediately, but are first "remembered" by each cell as potential changes, and are only implemented in response to a timely reinforcement signal \citep{seung2003trace}. To reflect this assumption, in the main set of simulations we only exposed our model to looming stimuli, as if STDP-driven changes were only implemented during looming responses (but see sensitivity analyses below). The network was allowed to develop for 12500 time steps (at least 500 looming stimuli), and we saved its topology at five equally spaced time points during this process, from a naive network, to the final state. We ran independent simulations 50 times, and for each connectivity snapshot we analyzed its weighted graph, as well as network responses to model visual stimuli that included a looming stimulus, a scrambled stimulus, and a full-field flash (same as in biological experiments).

\begin{figure*}[t!]
\includegraphics[width=\linewidth]{fig8.pdf}
\caption{
Model results. Each plot shows how one network measure changed as the model developed. Each color of line and markers represents one model type: red for the default model, other colors for sensitivity analyses. See text for details.}
\end{figure*}

The summary of modeling results is shown in Figure 8 (red lines and markeres for the base model). In development, the network became selective for looming stimuli, both in terms of the total response (by the end of development it responded to looming 99$\pm$9\% stronger than to flash; Figure 8A) and average selectivity (mean Cohen $d$ of responses = 1.09$\pm$0.10; Figure 8B). The share of cells selective for looming stimuli also increased (Figure 8C), and then saturated at $\sim$98\% level, as did the selectivity of the top 10\% of most selective cells (Figure 8D).

We then analyzed the quality of stimulus encoding, or whether in our model the identity of visual stimuli could be reconstructed from network activation. For a dataset consisting of equal shares of looming and non-looming stimuli, stimulus encoding increased in development, and plateaued at the prediction accuracy level of $\sim$95\% (Figure 8E). The fact that with multivariate logistic regression we could identify looming stimuli in 95\% of cases suggests that a retinotopic STDP-driven network can achieve reliable looming detection, as long as it is equipped with a prooperly tuned output layer. It is possible that in biological tadpoles, projections of tectal neurons to motor neurons in the hindbrain may play a role of this output layer \citep{helmbrecht2018topography}, with its weights tuned in development via reinforcement learning.

Unlike in biological experiments, the model was selective for looming stimuli over scrambled stimuli at the full network level (Figure 8F): by the end of the training, responses to looming stimulus were 43$\pm$7\% stronger than to scrambled. Mean selectivity of individual cells was 0.48$\pm$0.07 (Figure 8G); and 84\% of cells were selective for looming stimuli (Figure 8H), which was also different from what we observed in biological experiments. The selectivity for looming over flash correlated with selectivity for scrambled over flash on a cell by cell basis ($r$= 0.17$\pm$0.10). A subset of highly selective cells did not emerge in the model, and the difference between 90th and 50th percentiles of selectivity were rather low ($\sim$ 0.9 ; Figure 8I).

The position of selective cells within the network was different in the model, compared to biological experiments. While in tadpoles, selective cells tended to be located in the middle of the retinotopic field, in the model they tended to be located on the periphery, and selectivity for looming stimuli positively correlated with the distance from the network center (Figure 8J). Similar to tadpoles, however, this correlation disappeared in development (from $r\sim$ 0.75 in a naive network, to $r\sim$0.25 in a trained network). Similarly to biological networks, selective cells tended to be closer to each other (down to 71$\pm$3\% of distance expected for random connections; Figure 8K) than expected by chance, yet unlike for biological networks, this locality of connections was refined in development.

We then looked at topological and functional correlates of looming selectivity in model networks. Selective cells tended to be more spiky ($r$=0.34$\pm$0.12; Figure 8L, red line), and usually were not a part of a cluster (Figure 8M; final correlation with local clustering coefficient $r$=$-$0.26$\pm$0.12). Unlike in biological networks, in the model selective sells were not special in terms of their Katz centrality ($r$=0.02$\pm$0.13; Figure 8N), and they did not tend to receive an unusual number of incoming connections ($r=$0.00$\pm$0.12; Figure 8O). As in biological experiments, selective cells tended to be connected to each other (weighted assortativity of 0.24$\pm$0.07; Figure 8P). %In naive networks, the majority ($\sim$85\%) of strong edges (top 50\% of edges by weight) tended to lead from less selective sells to more selective ones, but in developed networks this share was reduced to chance value (51$\pm$0.03), matching our results from biological networks.

The variability of responses to looming stimuli over time, quantified as the number of principal components required to describe 80\% of response variability, mildly increased with network development, from 28$\pm$1 to 37$\pm$1 (Figure 8Q). The number of detected network ensembles did not change in development, staying around 15 (Figure 8R), but the share of variance in network responses explained by the involvement of different ensembles increased from $\sim$35\% in naive networks, to $\sim$50\% by the end of learning. As in biological networks, cells that formed an ensemble were slightly more likely to be connected to each other (Figure 8S), and were about 40\% closer to each other than any two random cells in the network (Figure 8T).

The distribution of degrees in the model was similar to that in biological experiments: the share of weakly connected cells (weighted in-degree $<$ 0.5) plummeted from $\sim$50\% in naive networks to 9$\pm$2\% by the end of training. On the contrary, the share of cells with degrees of 1 and 2 increased from $\sim$40\% to 83$\pm$2\%. With this changes, the power constant for the degree distribution changed from $\gamma \sim -$1.5 to $-$1.96$\pm$0.03 for both in- and out-degrees (Figure 8U), which also qualitatively matched changes in biological experiments. The share of reciprocally connected cells also decreased in development (Figure 8V).

Finally, we observed that most network measures changed with network development: efficiency (Figure 8W) and modularity (Figure 8X) increased, while clustering (Figure 8Y) decreased. In all three cases, the changes were mainly due to changes in weight and degree distribution, as they persisted if calculated on networks randomized with degree-preserving rewiring. The flow hierarchy (Figure 8Z) increased mildly in development, which was entirely due to structured changes in network topology, as the effect was not present in rewired graphs. A robust increase in modularity may seem to be in contradiction with a stable number of activation ensembles in the network, as network modules may be expected to form activity ensembles. We assume that the reason for this difference was that both in biological and computational experiments we identified ensembles from activity in in response to highly structured sensory inputs, rather than from long recordings of spontaneous activity \citep{triplett2018emergence}.

To conclude whether predictions of our model matched biological experiments, we formulated a list of “atomic”, elementary statements about each of the measures we analyzed, looking at whether they changed in development, whether they increased or decreased, and whether they differed from similar measures in a randomized network (Table 1, first two columns). Overall, the model and the experiments showed similar selectivity for looming over flash, but different selectivity to looming over scrambled stimuli. The interplay between cell position and connectivity was similar, except for the spatial distribution of looming-selective cells within the retinotopic map, which was peripheral in the model, but central in tadpoles. Changes in degree distributions were well matched, yet, with a possible exception of modularity, none of other network measures matched. When correlations between different types of node centrality and cell selectivity were considered, some of them matched, but most did not.

%\newgeometry{left=1in} % This page will have smaller margins
\begin{table}
    \input{table1.tex}
    \caption{A summary of network phenomena observed in biological experiments, in comparison with simulation results for the \textbf{base} model, and several reduced models (see main text). In the table, we use $\checkmark$ for "yes", $\times$ for "no", $\land$ for "increase", $\lor$ for "decrease", $\land \lor$ for "increase followed by decrease", and $=$ for "no change". FL stands for "Flash-Looming" selectivity; SL - for Scrambled-Looming selectivity; "cor" abbreviates "correlation".}
\end{table}
%\restoregeometry


\subsection*{Sensitivity analysis}

While a comparison with one faithfully constructed model is important, a better approach is to consider a family of models, and see which assumptions are critical for the replication of biological results, and which ones are not essential \citep{linderman2017constrain, pauli2018repro}. For example, we wondered whether it was important to assume that  plasticity was stronger during actual collisions, or whether  looming selectivity would develop if instead of looming stimuli we used more general visual stimuli. We also wondered whether structured sensory flow is even necessary for the emergence of selectivity \citep{triplett2018emergence}. To answer these questions, we rerun our model, excluding different parts of it at every set of runs. First we removed explicit synaptic competition, by replacing it with synapse weight decay (Figure 8, brown lines). In a different set of modeling experiments we greatly decreased the amount of intrinsic plasticity present in the system (Figure 8, green lines), and in yet another set we replaced STDP with simple symmetrical Hebbian plasticity (Figure 8, teal lines). Finally, in last two series of model runs, we let the model develop either while exposed to random visual noise (Figure 8, blue), or randomized oblique translational stimuli (Figure 8, pink).

We found that training on looming stimuli was not critical (in terms of replicating predictions of a full model), but training on structured stimuli was. When looming stimuli were replaced by non-collision visual stimuli (Table 1, column "Visual"), almost all network parameters still developed similarly to how they did in the base model. Looming selectivity also showed a similar trajectory, but reached 20-50\% lower selectivity values (pink lines follow red lines in Figure 8A-J). In contrast, when patterned stimuli were replaced with uncorrelated noise (Table 1, column "Noise"), selectivity did not improve with time (blue lines are flat and low in Figure 8A-J), even though network toplogy still changed in development (Figure 8W-Z).

Disruption of individual developmental rules led to very different changes in network development. When STDP was replaced by simple Hebbian plasticity (Table 1, column "No STDP"; teal lines in Figure 8), looming selectivity was similar or better than with STDP (top lines in Figure8A,B,F,G), and the network generally developed similarly, except that modularity was higher (Figure 8X), neuronal ensembles were more strongly interconnected (Figure 8S). 

When synaptic competition was replaced with synaptic strength decay (Table 1, column "No Syn. Comp."), the degree distribution was very different (got flatter rather than sharper; Figure 8U, brown lines); selectivity for looming stimuli was weakened (Figure 8C); the pattern of interactions between selectivity node centrality became unlike what was observed in the base series of experiments (Figure 8M-O), and the network became strongly hierarchical (Figure 8Z). The main reason for these differences appears to be that without synaptic competition, chains of connected neurons were allowed to lead to "dead-ends" within the graph, while with competition the total output of each neuron remained constant, leading to the development of cycles. 

Finally, when intrinsic plasticity was weakened (Table 1, column "No Intrinsic"; Figure 8, green lines), looming stimuli was completely disrupted (green in Figure 8A-C,F-H), and networks had simpler response structure, both in terms of PCA results (Figure 8Q), and ensemble analysis (Figure 8R). This confirms that agile intrinsic plasticity is critical for learning, as without it the network was likely to be primarily driven by spontaneous activity, that structured it incorrectly.

\section*{Discussion}

In the first half of this study, we reconstruct functional connectivity in the tectum of \textit{Xenopus} tadpoles from high-speed calcium imaging recordings, and describe novel aspects of tectal network topology. We show that tectal networks become more openwork with development, approaching a scale-free statistics. We also show that cells selective for looming stimuli tend to be located in the middle of the receptive field for a looming stimuli, and serve as "information sinks", collecting more inputs from the rest of the network, compared to non-selective cells.

In the second part of this paper, we hypothesized that a developing network governed by spike-time-dependent plasticity, synaptic competition, and stimulated by patterned visual inputs, would 1) spontaneously acquire selectivity for looming stimuli, and 2) develop a non-random network structure. We also hoped that these effects would be robust enough to be replicated in biological experiments. The support for this hypothesis is mixed. The model did develop selectivity for looming stimuli (in terms of both average preference, and stimulus encoding), and this increase in selectivity was resilient to changes in model assumptions. Yet this result were not truly replicated in biological experiments, as we observed no improvement in looming detection over development, and neither average selectivity, stimulus encoding, nor cell specialization differed between younger and older tadpoles. This was particularly surprising in view of a known improvement with age in collision detection in tadpoles \citep{dong2009}.

As predicted, our model networks developed non-random structure, with a scale-free degree distribution, low clustering, and high modularity. Of predictions related to network structure, most were replicated in biological experiments (Table 1, compare columns "Imaging" and "Model / Base"): most notably, changes in network degree distribution, a decrease in bidirectional connections, and all statements related to neuronal ensembles. This match between the model and the experiments suggests that at the very least, our model captured the nature of network development under the influence of synaptic competition and spike-time-dependent plasticity (STDP). Synaptic competition promoted connectivity in weakly connected neurons, while "punishing" overconnected cells, which created light-frame, openwork graph structures \citep{fiete2010chains}, while STDP coordinated activity within sub-networks, increasing modularity \citep{stam2010modular, litwin2014assemblies}, similar to how it was previously described for classic Hebbian plasticity \citep{damicelli2018topomod, triplett2018emergence}. We did not observe changes in the \textit{number} of neuronal ensembles \citep{avitan2017spontaneous, pietri2017emergence}, but we believe this is because our experiments were overall not suited for ensemble detection, as we worked with strong shared inputs that reliably activated almost every neuron in the network. Thus, our approach was very different from a case of spontaneous activity, where different sub-networks get activated randomly, with activity propagating within modules more readily than between them \citep{avitan2017spontaneous}.

At the same time, most model predictions about how network properties were supposed to change in development did not replicate in biological experiments. There are four possible explanations for this discrepancy. First, while neurons in stage 46 and 49 tadpoles have different synaptic and intrinsic properties \citep{ciarleglio2015}, and while retinal inputs to the tectum are known to be refined at these developmental stages \citep{tao2005refinement, munz2014hebbian}, the patterns of internal tectal connectivity may be relatively settled by stage 46. In our model, most network measures plateaued, or even reversed late in development, which means that even for a qualitative comparison between the model and the experiment we have to make a critical assumption about whether developmental stage 46 corresponds to a mid-point of network maturation, or falls on the developmental plateau. The absence of improvement in stimulus identity prediction from neuronal activity in older tadpoles, as well as a known difference in STDP between very young (stage 42) and older (stage 48) tadpoles \citep{richards2010stdp, tsui2010developmental}, suggest that both stages included in this study may indeed fall on the “plateau”. If true, this would mean that differences in collision avoidance between stage 46 and stage 49 tadpoles \citep{dong2009} may be due to maturation of sensorimotor projections from the tectum to the hindbrain, which we did not assess in this study.

Second, a poor fit between model predictions and biological experiments may be a consequence of low statistical power of this study. With respectively 14 and 16 networks reconstructed for each developmental stage, we could only hope to detect large changes in network parameters (Cohen $d \approx$ 1.0, assuming $p_t<$ 0.05 threshold and 80\% power). Moreover, based on available imaging studies, we can estimate that at stage 49, one side of a tadpole tectum contains about 10-15 thousand neurons, as it is about 40 cells across \citep{hiramoto2009}, and packed 6-10 cells deep in its thickest part \citep{hewapathirane2008vivo}, while tapering towards the edges \citep{bollmann2009}. On the other hand, here we reconstructed connectivity within the top layer of 128$\pm$40 cells, in a field of about 12 by 12 neurons, which means that our reconstructions covered only about 1\% of a full tectal network, and 0.01\% of all connections. With a coverage so sparse, our parameter estimations are expected to be noisy, further lowering our test power.

Third, one can question the validity of our connectivity reconstructions, as we did not have an opportunity to compare these reconstructions to a "ground truth" connectivity (but see \citealt{xu2011}). The best way to address this concern would be to run a set of control experiment, analyzing transfer entropy between pairs of cells proven to be either connected or disconnected, to estimate the power of graph reconstruction from Ca imaging recordings. Unfortunately, these experiments are currently beyond our technical ability, so we have to rely on indirect criteria for successful network reconstruction. Two most important observations that support the validity of our results are the fact that the share of reciprocal connections decreased in development; and that we observed a consistent non-randomness of almost all network measurements in reconstructed networks, compared to rewired networks. Also, we were comforted by a good replication of tectal response shapes (compared to \citealt{khakhalin2014}), a good internal replication of edge detection between stimuli types (see Methods), and an observation of retinotopy during responses to looming stimuli.

To add to that, the fact that we recorded network activity in response to shared inputs is of little concern for this particular type of analysis, as during transfer entropy calculations we ignored all patterns that were shared across trials. This means that, if anything, we were at risk of not detecting strongest connections within the network (if they were strong enough to be activated in every trial), rather than detecting non-existent connections.

Finally, the fourth way to explain a relatively poor fit between our model and imaging experiments is to assume that the mechanisms of looming selectivity in the tectum are after all different from that in the model. In our simulations, looming selectivity was largely mediated by synfire chains \citep{cohen2002synreview, zheng2014synfire} that were guided by structured sensory activation \citep{vislay2006rf, clopath2010stdpcoding}, and thus encoded activation patterns typical for looming stimuli \citep{pratt2008recurrent, richards2010stdp}. Later, when looming stimuli were presented to a model, they "resonated" with matching synfire chains, causing stronger network response. It may be that in the biological tectum, enhanced responses to looming stimuli are due to either delayed recurrent integration \citep{khakhalin2014, jang2016}, dynamic inactivation of neurons \citep{fotowat2011multiplexing}, or some other non-linear effects \citep{baginskas2009recurrent}. Two notable discrepancies between the model and the experiments are the position of selective cells within the network (central in tadpoles, peripheral in the model), and the difference in centrality measurements related to local signal integration (slightly higher in-degree and Katz rank in biological experiments, but no similar effect in the model). At the same time, the difference in the position of looming-selective cells may be due to explicit spatial assignment of output neurons in biological tecta that obviously was not included in the model, while the difference in Katz centrality may be explained by exaggerated synaptic competition in our model. Indeed, in simulations we forced every neuron to have outputs \textit{within} the tectal network, which led to the development of cycles, while in the real tectum selective cells may lack local intratectal outputs, projecting only to other brain regions. This hypothesis is indirectly supported by high Katz centrality of looming-selective cells in simulations without synaptic competition (Figure 8N,O).

Despite an incomplete match with the data, our model yielded interesting general predictions. The sensitivity analysis showed that of all modeling assumptions, two most important ones are the presence of fast intrinsic plasticity, and the exposure to patterned stimulation. The model converged to a state suitable for looming detection even if either STDP or synaptic competition were disrupted, but it failed if intrinsic plasticity was slow compared to synaptic plasticity, or if sensory inputs were kept random. Moreover, it was not critical for the model to be trained on looming stimuli: it performed almost as well with a mix of randomized translating, receding, and oblique looming stimuli, suggesting that it is edge continuity that mattered the most. This suggests that proper tectal organization can emerge from responses to retinal waves alone, provided that their statistics is similar to that of behaviorally relevant visual stimuli \citep{huberman2008waves}. Our model also predicts disrupted tectal organization in enucleated or dark-reared animals, which matches experimental research in tadpoles \citep{xu2011}, but curiously seems to contradict experimental observations in Zebrafish \citep{pietri2017emergence}.

At the same time, to use a subset of tectal outputs to a functional looming detector, as we did while estimating stimulus encoding (Figure 8E), a developing brain would need access to a learning signal. As a far-fetched hypothesis, we propose that in aquatic vertebrates this learning signal may come from both dimming receptors in the retina \citep{ishikane2005, baranauskas2012}, and lateral line receptors in the body \citep{pratt2009multisens, felch2016, truszkowski2017}. These inputs can then facilitate plasticity in tectal projections to the reticulospinal neurons in the hindbrain, strengthening inputs from a subset of tectal neurons that were most active immediately before a collision. Moreover, during random spatial encounters, different parts of the retina would be dimmed, and different segments of the lateral line would be activated in each individual collision, theoretically allowing animals to build several overlapping subnetworks, selective for collisions of different geometry, and projecting to different subsets of motor neurons \citep{frost2004review, helmbrecht2018topography}. This type of learning could lead to the development of spatially nuanced escape responses to optimize avoidance behaviors, as described in both tadpoles \citep{khakhalin2014} and fish \citep{bhattacharyya2017assessment}. Based on this hypothesis, we predict that tadpoles that are raised individually in empty arenas (devoid of objects to collide with) should have normal vision, and normal dark-startles, but would not be able to perform proper collision avoidance.

To sum up, we show that a combination of simple developmental rules with patterned sensory inputs can quickly shape a random network into a structured retinotopic system, able to support collision detection. Our results suggest that collision detection in small aquatic animals may rely on resonant subnetworks of spatially organized synfire chains that integrate information about stimulus position, speed, and size change, and transform it into an appropriate motor response. We hypothesize that this sensorimotor transformation may develop through a form of reinforcement learning in the hindbrain, and hope to test this hypothesis in the future.

\section*{Methods}

% All code for his paper is available at: \url{https://github.com/khakhalin/Ca-Imaging-and-Model-2018}

\subsection*{Statistics and reporting}

Unless stated otherwise, all values are reported as mean $\pm$ standard deviation. For most common tests, test type is indicated by the subscript for its reported p-value: $p_t$ for a two-sample t-test with two tails and unequal variances; $p_{t1}$ for a one-sample two-tail t-test, and $p_r$ for a Pearson correlation test.

When working with weight matrices, we write them as it is usually done in computational neuroscience, where $w_{ji}$ is a weight of an edge coming from node $i$ to node $j$, which is different from how adjacency matrices are presented in graph theory, where $A_{ji}$ would typically mean an edge from node $j$ to node $i$ (so our $W = A^\top$).

\subsection*{Experiments}

Overall, we followed calcium imaging protocols previously described in \citep{xu2011, truszkowski2017}, but combined it with visual stimulation modeled after \citep{khakhalin2014}. Experiments were performed at Brown University, in accordance with university IACUC protocols. Unless noted otherwise, chemicals were purchased from Sigma. Tadpoles were kept in Steinberg’s solution, on a 12/12 light cycle, at 18$^\circ$ C for 10-20 days, until they reached Nieuwkoop-Faber developmental stages of either 45-46 or 48-49. In each experiment, we anesthetized a tadpole with 0.02\% tricainemethane sulfonate (MS-222) solution for 5 minutes, then paralyzed it by immersion in 20 mM solution of tubacurarine for 5 minutes, and pinned it down to a carved Sylgard block within the recording chamber filled with artificial cerebro-spinal fluid solution (ACSF: 115 mM NaCl, 4 mM KCl, 5 mM HEPES, 10 uM glycine, 10 mM glucose). The optic tectum was exposed, and ventricular membrane was removed on one side of the tectum. Tadpoles were pinned down tilted, at an angle of 10-20$^\circ$, to keep exposed tectal surface flat for imaging. We then surrounded the tadpole with a small circular enclosure 15 mm in diameter, made of a thicker part of a standard plastic transfer pipette, to achieve higher concentration of Ca-sensitive dye in the solution. We dissolved 50 ug of AM ester cell permeable Oregon Green 488 nm Bapta-1 (OGB1 $\#$06807, Molecular Probes, Waltham, MA) in 30 ul of medium consisting of 4\% F-127 detergent in 96\% DMSO by weight; agitated this solution in a sonicator for 15 minutes, then added 30 ul of ACSF to the vial, and sonicated for another 10 minutes. The solution was then mixed with 4 ml of ACSF to the final concentration of 10 uM, transferred to the chamber, and the chamber was placed in the dark for 1 hour. After staining, the circular enclosure was removed; the preparation was gently washed with 10 ml of ACSF 3 times; the chamber was filled with 10 ml of fresh ACSF, and transferred under the scope.

This staining protocol with a BAPTA-conjugated dye proved to be challenging, and had a high failure rate. As staining procedure involved a detergent, and called for high concentrations of dye, most successful preparations were those that received the highest possible exposure to dye that did not yet kill the cells. A large share of preparations however either fell short of optimal staining, and had a weak fluorescence signal, and so a low signal-to-noise ratio, or got overexposed, leading to strong fluorescence, but weak responses to stimulation, as neurons grew increasingly unhealthy. This variability in signal-to-noise ratio led to  differences in edge detection certainty from one experiment to another, which complicated our network analysis (see below).

Visual stimulation was provided with a previously described setup \citep{khakhalin2014}, consisting of an LCD screen (Kopin Corporation, Taunton, MA, USA) illuminated by a blue LED (LXHL-LB3C, 490 nm; Lumileds Lighting, USA), with image projected to an optic multifiber (600 um, Fujikura Ltd, Tokyo, Japan). The other end of the fiber was brought to the left eye of the tadpole, and placed 400 um away from the lens, on the axis of the eye, to have the image projected to the center of the retina. The stimulation sequence consisted of three stimuli: Looming stimulus (in which a circle appeared in the center of the field, its radius growing linearly from 0 to full-field within 1 second), full-field Flash, and spatially "Scrambled" stimulus. For Scrambled stimuli, we divided the field of view into a grid of 17 by 17 squares, and randomly reassigned these squares within the image. The result was a stimulus that was identical to Looming in terms of its total brightness at every time step, and presented fragments of a moving edge locally (within every square in a reshuffling grid), but lacked high-level spatial organization. The permutation of squares was randomized for each experiment, but was consistent within all trials within every experiment. Stimuli were delivered every 20 s, in the same sequence of "Looming, Flash, Scrambled”, typically for the total of 60 or 72 stimuli. The stimuli were generated in Matlab (Mathworks), using Psychtoolbox \citep{kleiner2007psychtoolbox}. Excitation light for imaging was turned on one second before the onset of visual stimulation, and kept on for 5 seconds, which was shown not to interfere with responses \citep{xu2011}.

Fluorescent responses in the tectum were imaged using a Nikon Eclipse FN1 microscope with a 60x water-immersion objective and ANDOR 860 EM-CCD camera (Andor Technologies). NIS-elements software (Nikon) was used to record the activity. We used binning with 8x8 pixels per bin, resulting in a 130x130 image covering the field of view of 1130 um. The data was acquired with 10 ms auto-exposure, leading to actual frame rate of 84 frames per second (11.9 ms per frame). For each preparation, we found a focal plane that produced images of as many cells as possible, which usually meant a plane focused "in-between" topmost and bottom-most cells within the field of view. To keep the signal-to-noise ratio consistent throughout the experiment despite the ongoing bleaching of the Ca sensor, we started with relatively weak illumination (with neutral density filter ND4 engaged) and no signal amplification by the camera (EM gain of 0). We then increased the EM gain level gradually after every 12 stimuli, to keep the signal level approximately constant. Once EM gain setting reached the value of 7, we increased illumination strength by disengaging one of the density filters, reduced EM gain back to 0, and repeated the process.

Videos were processed offline; circular regions of interest of equal size (21 binned pixels per region) were manually positioned over neurons with well defined Ca responses (based on the visualization of fluorescence variability in time, as provided by NIS-elements software). Average fluorescence within each region of interest was quantified, and exported to Matlab. We then processed fluorescence traces using non-negative deconvolution algorithm \citep{vogelstein2010oopsi}, and used its output without thresholding, interpreting it as a probabilistic estimation of instantaneous spike rate for each cell. We chose not to threshold the signal, as depending on the overlap each cell body had with the focal plane, as well as the amount of dye sequestered by the cell during staining, different neurons had very different signal-to-noise ratios even within each preparation, which complicated the matter of finding a single threshold. This decision also shaped all further steps of our analysis, as in our dataset poorly resolved cells with low spiking activity were represented not by spike traces that were mostly silent, but by traces that were noisy, and approached uniform distribution of estimator values. %Reference cells, required for the deconvolution algorithm, were selected automatically, as the cell with 5th highest amplitude fluorescence response.

We did not attempt to match inferred spike trains to "ground-truth" electrophysiology recordings, as the validity of this calcium imaging protocol was justified previously \citep{xu2011, truszkowski2017}. We also did not perform background subtraction \citep{truszkowski2017}, as most effects of background fluorescence were expected to be cancelled out during analysis. The main risk of not subtracting the background is that unsubtracted traces may contain a superposition of axonal spiking and synaptic activation in the neuropil. Judging from the spatial distribution of fluorescence signals, in our experiments neuropil fibers were not stained, as sensitive dye had no access to structures below the top, exposed level of primary tectal cells. Moreover, our signal acquisition was focused on fluorescence sources within the focal plane, meaning that any  neuropil signals were both attenuated, and spatially averaged across regions of interest. Finally, neuropil activation was expected to be similar in each trial, as same stimuli were presented to the tadpole in each trial. As deconvolution operation is close to linear, and we did not perform spike thresholding, any shared neuropil signal would be deconvolved, “hidden” in inferred spike-trains, and later cancelled out during trial-reshuffling (see below). Similarly, we did not address motion artifacts, as in our preparation they were synchronous in all cells (manifested as parallel displacement of signal sources from fixed ROIs), and therefore only introduced a fixed bias to all connectivity estimations.

\subsection*{Analysis}

\textbf{Basic analysis} To quantify response amplitudes, we used reconstructed responses between 250 and 2000 ms into the recording, as this window included full visual responses, but excluded artifacts caused by the excitation light. As a measure of cell selectivity for stimuli of a specific type, and in some cases as a measure of total network response selectivity, we used Cohen’s $d$ effect size for the difference between responses to Looming and Flash, or Looming and Scrambled stimuli:

\[ d = (m_L-m_F)/ \sqrt{ \big((n_L-1) s^2_L + (n_F-1) s^2_F)/(n_L + n_F - 2)} = \]
\[ =(m_L-m_F)/\sqrt{\big(s^2_L+s^2_F\big)/2} \]

In case of equal sample sizes $n_L=n_F=n$. Here $m_L$ and $m_F$ are mean responses to looming and flash stimuli respectively, and $s_L$ , $s_F$ are standard deviations for both groups.

To find the \textbf{retinotopy center}, we concatenated all responses of every cell to looming stimuli into one vector, ran a principal component analysis on these vectors, then rotated two first components using promax rotation, and made sure that the 1st component $c^1$ is the one with shorter latency, and that it is positive (by swapping and flipping the components if necessary). We then ran a non-linear optimization, looking for a pair of coordinates $(x,y)$ within the field of view, that would maximize the absolute value of correlation between distances of each cell to this center and the relative prominence of the short-latency component for this cell:

\[ r = \text{cor}\big(\sqrt{(x_i-x)^2+(y_i-y)^2}\ ,\ c^1_i/(c^1_i + c^2_i), \big) \]

We interpreted these $(x,y)$ coordinates as our best guess for the possible position of the "retinotopy center" for each recording. The fits were robust, with $p<$ 0.05 observed in every experiment (30/30), and average achieved correlations of $r=$ 0.59$\pm$0.23. To assess possible overfitting, we performed identical optimization fitting  after reshuffling cell identities 5 times for each experiment, which yielded average $r$ values of only 0.13$\pm$0.06, and $p_r<$ 0.05 in 21\% of experiments. From this we concluded that cells with early responses to looming stimuli were indeed clustered together, and that this clustering was not an artifact of our analysis, even though the $r$-values were probably exaggerated due to overfitting.

For \textbf{response latency} calculations, we looked at each response $y(t)$, and found the position of its maximum $(x_M, y_M)$. We then used the least squares fit with non-linear solved to approximate the segment between the beginning of the response and $x_M$ with a piecewise linear function:

\[ f(x) = \left \{ \begin{array}{cll} 0 & \text{for} & 0 \leqslant x<x_L \\
a (x-x_L) & \text{for} & x_L\leqslant x < x_M \end{array} \right. \]

optimizing for $x_L$ and $a$, where $x_L$ is the response latency, and $a$ is an amplitude-like parameter we did not use for subsequent analysis. This approach worked well for isolated responses with low noise, but got increasingly noisy with weak signals. To quantify the retinotopy, we therefore used the results of factor analysis, as described above, and only used response latencies for results verification.

\textbf{Ensemble analysis}. To find ensembles of cells that tended to be co-active together, we used a modified spectral clustering procedure \citep{ng2002spectral} and the definitinon of spectral modularity \citep{newman2006modularity}, generalized to weighted oriented graphs. First, for each stimulus type, for each cell $i$, and separately for each experimental trial $k$, we unbiased and normalized each activity response $a^k_i(t)$, by subtracting its mean, and dividing the result over standard deviation:

\[ a^k_i(t)' = \big(a^k_i(t)-b^k_i\big)/\sigma^k_i \]

where $b^k_i = \sum_{t=1}^T{a^k_i(t)}$ and $\sigma^k_i = \frac{1}{T-1}\sum_{t=1}^T{(a^k_i(t) - b^k_i))}$ .

Then, for each cell, we calculated the average response across all trials of the same type: 

\[ \overline{a_i}(t) = \frac{1}{n}\sum_{k=1}^n{a^k_i(t)} \]

and subtracted these average responses from each trial, which resulted in a vector of a trial-by-trial deviations from the average response:

\[ a^k_i(t)'' = a^k_i(t)' - \overline{a_i}(t) \]

We then concatenated these vectors of deviations from the mean $a^k_i(t)''$ across all trials, and used them to calculate a cross-correlation matrix, to see which cells tended to be active and inactive together:

\[ c_{ij} = \text{cor}\big(a''_i(t)\, , \, a''_j(t)\big) \]

We calculated adjusted correlations $c_{ij}$ separately for each of three types of stimuli (flash, scramble, and looming), and averaged these three estimations $c_{ij}^s$, to arrive at a less noisy estimation of adjusted cross-correlation. We then removed negative correlations, replacing them with zeroes.

\[ c'_{ij} = \text{max}\big(0 \, , \, \frac{1}{3} \sum_{s}{c_{ij}^s}\big) \]

We then roughly followed the spectral clustering procedure by \citep{ng2002spectral}, with adjustments that seemed appropriate for ensemble detection. We first transformed our correlation matrix $c_{ij}$ into a matrix of pairwise Euclidean distances:

\[ \varphi_{ij} = 2(1-c_{ij}) \]

and then to affinity matrix $\textbf{A}$:

\[ a_{ij} = \text{exp}(-\varphi_{ij}/\sigma) \]

where $\sigma$ is a free parameter that we set at 10000. We then calculated a diagonal degree matrix $\textbf{D}$ such that $d_{ij} = 0$ for $i \neq j$ , and $d_{ii} = \sum_k{a_ik}$ otherwise. We used $\textbf{D}$ to build a Laplacian matrix $L$, such that:

\[ L_{ij} = a_{ij}/\sqrt{d_{ii}\cdot d_{jj}} \]

and found eigenvectors $x_1$ .. $x_n$ of this matrix $L$. Then we started to look for a good number of ensembles $k$, by going through all values from 1 (no ensembles) and up to the number of cells (each cell as a separate ensemble). For each $k$, we found first $k$ largest eigenvectors of $L$, stacked them in columns, and renormalized each row of this matrix to give it unit length:

\[ u_{lm} = \frac{x_{lm}}{\sqrt{\sum_{z=1}^{k}{x_{lz}^2}}} \]

where $x_{lm}$ is an $m$-th element of $l$-th eigenvector of $L$. We then used k-means clustering on rows of $\textbf{U}$ as points in $\mathbb{R}^k$, looking for $k$ clusters. Once rows of $\textbf{U}$ (and so cells in the original data) were assigned to $k$ clusters, we calculated spectral modularity of this partition on the original matrix $w_{ij}$, using a weighted directed modification of classic formula from \citep{newman2006modularity}:

\[ Q_k = \frac{1}{4m}\sum_{ij}{\delta_{ij}\Big(w_{ij}-\frac{d^{out}_i d^{in}_j}{2m}}\Big) \]

Here $d^{out}_i$ and $d^{in}_j$ are weighted out- and in-degrees for nodes $i$ and $j$ respectively: $d^{out}_i = \sum_k{w_{ik}}$ , and $d^{in}_j = \sum_k{w_{kj}}$ ; $m$ is the total number of edges involved: $m = \sum_{ij}{w_{ij}}/2$ , and $\delta_{ij}$ is a signal matrix with $\delta_{ij}=1$ for nodes $i$ and $j$ that belong to the same cluster, and $\delta_{ij} = 0$ otherwise. 

Finally, we selected the number of clusters $K$ that, after spectral clustering, produced highest modularity $Q_K$ across all $Q_k$, and we used $K$ as an estimation of the number of ensembles in the network, and corresponding cluster allocation - as the allocation of cells to these ensembles.

\textbf{Network reconstruction}. For network reconstruction, we used a modified Transfer Entropy (TE) calculation, adapted from \citep{gourevitch2007te, stetter2012te}. Fast Ca imaging recordings, as used in this study, provide a middle ground between commonly used slower Ca imaging data and multielectrode recordings. In most Ca imaging recordings, the frame acquisition time (100 ms) is an order of magnitude longer than the transmission time between neurons (2 ms), which biases analyses towards co-activation analysis. In our data, the high rate of acquisition (12 ms per frame) was close to typical cell-to-cell activation transmission time in the tectum, so we restricted our analysis to interactions between the activity of each cell at a frame i and their activity at the next frame i+1, ignoring both longer (multiframe), and same-frame interactions.

For each cell, we binned its activity trace at 3 levels, classifying every frame as either a high, medium, or low activity frame. For each cell, we used 1/3 and 2/3 quantiles of its inferred activity values as thresholds, so that all three types of frames were equally frequent, to maximize information. Then for each pair of neurons $i$ and $j$ we calculated the probability $P(g_j^1,g_j^0,g_i^0)$, which showed the conditional probability of neuron $j$ being in state $g_j^1$ (either 1, 2, or 3) at moment $t$, if this neuron was in a state $g_j^0$ at the previous frame $t-1$, and  input neuron $i$ was in state $g_i^0$ at the same frame $t-1$. From this set of probabilities, we calculated conditional probabilities of $P(g_j^1 \mid g_j^0)$, and finally calculated the total transfer entropy as

\[ T_{ij} = \sum_{l,m,n=1}^3{P(g_j^1=l,g_j^0=m,g_i^0=n)}\cdot \log\left(\frac{P(g_j^1=l \, \mid \, g_j^0=m, \, g_i^0=n)}{P(g_j^1=l \, \mid \, g_j^0=m)}\right) \]

For this project, a common sensory drive (visual inputs from the retina) presented a unique problem. If the hypothesis of this paper is true, and the detection of looming stimuli in the tectum is actually mediated by sensory activation of matching synfire chains, we can expect the pattern of this sensory activation to be synchronized with causal transfer of excitation from one neuron to another. Because of that, common sensory inputs cannot be eliminated by methods that rely on the comparison of delays \citep{wollstadt2014te}. Instead, we eliminated the effects of common drive by randomly reshuffling our data within each stimulus type, and pairing activation history of each cell with activation history of other cells from non-matching trials. For each experiment, we calculated 1000 randomly reshuffled transfer entropy estimations, and then subtracted the average of these reshuffled TE estimations from raw TE estimation, arriving at adjusted TE \citep{gourevitch2007te}:

\[ T'_{ij} = T_{ij} - T^\text{shuffled}_{ij} \]

This approach is similar to the idea of analyzing subtle variations in activation from one response to another, as opposed to the analysis of activation traces themselves. As presented stimuli were same in every trial, the effect of common sensory drive over time was expected to be shared across all trials. This means that if a connection between cells $i$ and $j$ was suggested equally strongly by the analysis of real, and reshuffled data, these cells were probably sequentially driven by a common input, rather than by a true causal connection between them.

For each TE estimation, we also calculated a corresponding p-value that quantified whether actually observed TE was unusual enough (significantly different), compared to TE estimations obtained on reshuffled data, that corresponded to a null hypothesis of no causal connections. With the computational power available to us, we could only generate 1000 surrogate reshuffled networks for every TE calculation, which made it impossible to use the false discovery rate correction, as it is sometimes recommended for large-scale studies of brain connectivity \citep{lindner2011trentool, vicente2011te}. With $\sim$10$^2$ neurons and 10$^4$ connections the smallest possible non-zero p-value of 0.001, corresponding to finding a more extreme TE value in one out of 1000 surrogate experiments, was already larger than the Benjamini-Hochberg threshold of $\frac{k}{m}\alpha=$ 5e-6. With a more permissive threshold of $\alpha=$ 0.01, for each of three stimuli types, our analyses suggested the existence of 5\% to 69\% of all possible directed edges in the connectivity graph, depending on the experiment (mean of 16\%). The share of edges that were independently discovered in responses to all three types of stimuli  (mean: 0.1\% of all possible edges) was on average 2.4 times larger than one would have expected for spurious discoveries (signrank $p=$ 7e-7), suggesting that three subsets of data, originating from responses to three different stimuli could be considered replications for the purposes of edge discovery. 

Note that our TE adjustment procedure could not differentiate between activity driven by shared inputs, and activity due to reliable synaptic connections that reproduced the same activation pattern in every trial. It also means that it was by design impossible for us to detect strongest looming-selective synfire chains in responses to looming stimuli, as the appearance of these strong connections would be indistinguishable from the effect of shared sensory input. This suggests that the principle of edge replication across different stimuli could not be taken literally, to avoid this masking effect.

Due to variations in staining quality, preparation shape, and focal plane alignment, we had to work with very different proportions of low-noise and high-noise neurons in different experiments, which made our edge discovery rates very uneven for any fixed threshold approach. To fix this problem, we made our edge detection procedures adaptive on the experiment-by-experiment basis. First, we relaxed criteria on edge discovery, while still giving preference to edges observed in more than one subset of responses. We included in our reconstruction only edges with geometric mean of p-values below significance threshold: $\prod{p_k}<\alpha^3$ , where $p_k$ are p-values for each of three subsets of data (responses to flash, crash, and scrambled stimuli). Then we looked for a value of $\alpha$ that would bring the average node degree (the ratio of network edges to network nodes, for directed graphs $=E/N$) to an arbitrary reasonable target value, which is a known approach to the analysis of noisy networks \citep{stetter2012te}. We picked a target $E/N$ value of 1.0 (number of edges equal to the number of nodes), which lead to 128$\pm$41 edges in each experiment on average (0.9\% of all possible edges); 50$\pm$21 weakly connected components, with 74$\pm$30 nodes in the largest weakly connected component. The comparison of network properties (Figures 5 and 6) did not change qualitatively in a broad range of assumed average degrees (from $\sim$ 0.5 to 1.5), but observed effects became weaker and regressed to random effects outside of this range. 

The TE approach cannot distinguish between positive and negative influence of one neuron on another, so our reconstructed edges could include a mix of excitatory and inhibitory connections. To estimate the share of putative inhibitory connections, we calculated pairwise correlations between activities of individual neurons, taken with a one frame delay, and compensated for the effect of shared inputs similar to how it was done for TE. We then looked at the sign of these correlations for pairs of neurons with significant TE. We found that 3$\pm$7\% of detected connections seemed inactivating or inhibitory, with no difference in rate between developmental stages ($d=$ 0.55, $p_t=$ 0.1). According to current understanding on tectal architecture in \textit{Xenopus}, principal tectal neurons are not expected to be inhibitory \citep{bell2011polyamines}, and moreover, the share of negative correlations tended to be lower in experiments with better signal-to-noise ratio. We therefore assumed that most, if not all observed inactivating connections may be false discoveries, and excluded all edges with negative correlation values from the analysis. For those edges that remained in the adjacency matrix, we averaged TE estimations obtained from responses to Flash, Scrambled, and Looming stimuli, and used these averaged values as estimations of synaptic connectivity weights $w_{ji}$.

To analyze \textbf{degree distributions}, we found the sum of weights of incoming and outgoing edges for each cell; rounded these values towards nearest whole number, and calculated frequencies $F_{in}(k)$ and $F_{out}(k)$ for each degree value $k$ (Figure 4F,G). For each experiment, we then fit a regression line $y = -\gamma k + b$ to a sequence of points $[k , \, log(F(k)) ]$, for in- and out-degrees separately; estimated two power constants $\gamma_{in}$ and $\gamma_{out}$, and averaged them to arrive at one balanced estimation ($\gamma$; Figure 4H).

To quantify the share of \textbf{reciprocal connections}, we multiplied the weight matrix element-wise on itself transposed, summed these values up, and normalized the result by the sum of squared weights: $S=\sum_{ji}{w_{ji} w_{ij}} / \sum_{ji}{w_{ji}^2}$ . For positive weight matrices, this value is equal to 1 for symmetric weight matrices, 0 for matrices without reciprocal connections, and smoothly changes between these two values for "intermediate" cases.

\textbf{Network analysis}. We reviewed several lists of statistical tools applicable to weighted directed graphs \citep{rubinov2010toolbox,costa2007networks,hernandez2011metrics}, and selected a diverse set of measures to describe  different aspects of our networks, such as connectivity, unevenness of density, and global structure. We only included measures that do not erode with the inclusion or exclusion of individual weakly connected nodes, to make sure that metrics estimations would not change catastrophically from one experiment to another because of small variations in noise level, or a slightly more generous selection of regions of interests during video quantification. Examples of measures that do not satisfy this criterion are cycle order and the "small world" property, that both are sensitive to the inclusion of weak long-ranged connections \citep{papo2016beware}. We used the following list of network metrics:

\textbf{Global network efficiency} was calculated using a function from the Brain Connectivity Toolbox \citep{rubinov2010toolbox} on reciprocals for graph weights $R_{ij} = 1/w_{ij}$, and was defined as:

\[ E = \frac{1}{n} \sum_{i \neq j}^n{\frac{d_{ij}}{n-1}} \]

where $d_{ij}$ is the length of the shortest path $P_{ij}$ connecting nodes $i$ and $j$: $d_{ij} = \sum_{kl \in P_{ij}}{R_{kl}}$

\textbf{Clustering coefficient} \citep{fagiolo2007} was calculated using the Brain Connectivity Toolbox, with a function that supported weighted directed graphs:

\[ C = \frac{1}{n} \sum_i{\frac {t_i}{(k^o_i+k^i_i)(k^o_i+k^i_i-1)-2\sum_j{w_{ij}w_{ji}}}} \]

where $k^o_i$ and $k^i_i$ are out- and in-degrees of node $i$ respectively, and $t_i$ is the weighted number of directed triangles that include node $i$:

\[ t_i = \sum_{j \neq i}{\sum_{k \neq i,j}{w^{1/3}_{ij}w^{1/3}_{jk}w^{1/3}_{ki}}} \]

To estimate \textbf{network modularity}, we also used a function from the Brain Connectivity Toolbox, which calculated spectral modularity on a weighted directed graph \citep{reichardt2006community,leicht2008community}.

Our definition of \textbf{flow hierarchy} was inspired by \citep{mones2012hierarchy,czegel2015hierarchy}, but based on modified (weighted) Katz centrality \citep{katz1953original,fletcher2018katz}. To calculate Katz centrality, we assumed that each node $j$ collected flows of incoming signals through all edges $w_{ji}$ leading to this node. Activation arriving through edge $j\leftarrow i$ was proportional to the total activation $z_i$ of source node $i$, the weight of this edge $w_{ji}$, a global normalization coefficient equal to $1/\text{max}(w_{kl})$ across all edges $k\leftarrow l$ in the network, and a damping factor of $d=$ 0.9. Each node also received a small amount of constant activation $(1-d)=$ 0.1. The total activation of each node was therefore defined as:

\[ z_j = (1-d) + \frac{d}{\text{max}_{k,l}(w_{kl})} \sum_{i \neq j}{a_i w_{ji}} \]

Each node then distributed this activation to other nodes. This definition is also close to that of pagerank centrality \citep{page1999pagerank}, except that the weights are not normalized to the value of total outgoing weights for each node $i$: that is, we work with raw weights of $w_{ji}$ rather than $w_{ji}/\sum_k{w_{ki}}$. It means that a node with many outputs has a strong influence over network activation, while nodes with weak outgoing edges act as dead-ends. Similar to a standard pagerank algorithm, we solved this problem iteratively, by initializing the network with equal values of centrality, and running the equation above until convergence (typycally, $\sim$100 times). Once Katz centrality values $z_i$ were found, we used the difference between maximal Katz centrality observed in the network and mean centrality across all nodes as a measure of flow hierarchy: $h = \text{max}(z_i) - \text{mean}(z_i)$ \citep{mones2012hierarchy,czegel2015hierarchy}.

To check whether network values described above were different from values expected on a random graph, we performed \textbf{graph randomization}, using a variant of degree-preserving reshuffling \citep{maslov2002} that we generalized for directed weighted graphs. For a network with $N_E$ edges we picked $3\cdot N_E$ random pairs of nodes (nodes $i$, $j$, $k$, and $l$) that had strong connections from $i$ to $j$, and from $k$ to $l$, but weak connections or no connections from $i$ to $l$, and from $j$ to $k$ (we required $w_{ji}>w_{li}$ and $w_{lk}>w_{jk}$). We also required all four nodes to be different ($i \neq j \neq k \neq l$). Then we cross-wired these pairs of nodes, gradually randomizing network topology:

\[ \left \{ \begin{array}{l}  
w_{ji} \leftarrow w_{li} \\ 
w_{li} \leftarrow w_{ji} \\
w_{lk} \leftarrow w_{jk} \\
w_{jk} \leftarrow w_{lk}
\end{array} \right. \]

This approach to degree-preserving randomization is slightly different from the original formulation by \citep{maslov2002} in two ways. First, we explicitly don’t allow loops (self-edges) by requiring all four nodes be different. Second, we allow nodes $i$ and $k$, as well as $l$ and $j$ to be connected before the rewiring, and just swap corresponding edge weights, which seems to be a necessary adjustment for directed weighted graphs. It also means that, strictly speaking, for a weighted graph, our randomization only preserves out-degrees, but not in-degrees. Because of the requirement that $w_{ji}>w_{li}$ and $w_{lk}>w_{jk}$, for a binary directed graph our algorithm preserves in-degrees strictly, as it becomes identical to version by Maslov, while for nearly-binary graphs (bimodal or sparse), it tends to preserve in-degrees on average.

We also tested whether connectivity and positioning of selective cells within the graph was in any way peculiar, by calculating correlations between cell selectivity and several different \textbf{graph centrality measurements}. We used three centrality measures: weighted in-degree (the sum of weights of all connections to the node); Katz centrality; and clustering coefficient.

To measure whether selective cells tended to form sub-networks within the graph, we calculated \textbf{weighted assortativity}. The formula for an assortativity value (mixing coefficient) in a weighted directed network is given in \citep{farine2014weighted}, based on logic from \citep{newman2003mixing} and \citep{leung2007weighted}. The original formula from \citep{newman2003mixing} for an unweighted undirected graph defines a mixing coefficient as a Pearson correlation coefficient between properties of nodes connected by edges, taken over all edges in the graph:

\[ r=\underset{ij: a_{ij}=1}{\text{cor}}(x_i,x_j) \]

leading to the following expression:

\[ r = \frac{\frac{1}{E} \sum{x_i x_j} - [\frac{1}{E} \sum{\frac{1}{2}(x_i+x_j)}]^2} {\frac{1}{E} \sum{(x_i^2+x_j^2)}-[\frac{1}{E} \sum{\frac{1}{2}(x_i+x_j)}]^2} \]

where sums are taken over all connected edges $ij: a_{ij}=1$, and $E$ is the total number of edges.

For a weighted graph an equivalent measure can be introduced by replacing summation over edges to summation over all possible pairs of nodes $ij$, with weights $w_{ij}$ introduced in each sum. The resulting expression can be rewritten in several different forms \citep{newman2003mixing,leung2007weighted,farine2014weighted,teller2014assortative}, but instead of explicitly coding these bulky and rather confusing calculations, we used the fact that ultimately a mixing coefficient can be described as a weighted correlation across all connected directed edges $ij$ with edge values $w_{ij}$ used as correlation weights:

\[ r=\text{cor}(x_i \, , \, x_j \, , \, w_{ij}) \]

In turn, weighted correlation $\text{cor}(a,b,w)$ can be introduced through weighted covariances: 

\[ \text{cor}(a,b,w) = \frac{\text{cov}(a,b,w)}{\sqrt{\text{cov}(a,a,w) \cdot \text{cov}(b,b,w)}} \]

and weighted covariances are defined simply and intuitively as: 

\[ \text{cov}(a,b,w) = \frac{\sum_i{w_i \cdot (a_i-\bar{a})(b_i-\bar{b})}}{\sum_i{w_i}} \]

with $\bar{a}$ and $\bar{b}$ representing weighted mean values: 

\[ \bar{a}=\sum_i{w_i a_i}/\sum_i{w_i} \]

Note however that this definition may differ slightly from the one used in the Brain Connectivity Toolbox \citep{rubinov2010toolbox}.

\textbf{Unreported analyses}. For the sake of transparency, here we report all measures that were calculated, but not included in the final manuscript for being superflous or confusing: four measures of weighted directed degree assortativity (in-in, in-out, out-in, and out-out); pagerank centrality; Katz centrality on reversed graphs $W^\top$; flow hierarchy for reversed graphs; node reach on direct and reversed graphs (unweighted analog of Katz centrality without attenuation); two alternative measures of cell selectivity: McFadden’s pseudo-$R$ for a logistic fit of stimulus identity to the total response of each cell, and selectivity measures calculated on peak amplitudes instead of cumulative amplitudes (the results of both calculations were qualitatively similar to those reported in the paper). We also made several attempts to estimate the prevalence of directed cycles in our networks, but decided that these measures require too much validation to be included in this manuscript. For network analysis, we also attempted to compare rewired graphs to matching random Erdos graphs, but failed to build a good generalization for a case of weighted directed graphs with an adaptive edge detection threshold.

\subsection*{Developmental Model}

The model consisted of $n$=81 cells, arranged in a 9x9 grid. The model operated in discrete time $t$, and was run for 500 epochs, 25 time steps each, or for $T$ = 12500 time steps total. At each moment of time, each cell was described by three values: its instantaneous firing rate $s_i(t)$, represented as continuous value $0 \leqslant s_i(t) \leqslant 1$; spiking threshold $h_i(t) \geqslant 0$ that slowly changed over time as described below, and a constant $\hat{s_i}$ that described the target spiking rate for each cell. Target spiking rates $\hat{s_i}$ were randomly assigned at the beginning of each simulation, and were distributed normally around 5/$n$ with a standard deviation of 1/$n$, which means that if these target spiking rates were matched, on average, at any time step, 5 out of 81 cells would be spiking.

Cells were connected to each other with "synapses" of different strengths, represented by a weight matrix \textbf{W}, with weight $0 \leqslant w_{ji} \leqslant 1$. These weights were originally assigned random values, uniformly distributed between 0 and 1, except for self-connections (loops, $w_{ii}$) that were kept at 0.

At each time step we first calculated the raw activation \textbf{A} of all neurons: $\textbf{A} = \textbf{WS} + \textbf{B}$, where \textbf{W} is the connectivity matrix, \textbf{S} is the vector of instantaneous spiking rates $s_i$ , and \textbf{B} is the sensory input (see below). For one cell, we have:

\[ a_i(t+1) = \sum_j{w_{ij}s_j(t)} + b_i(t) \]

These raw activation values were then adjusted down, by a formula representing global feedback inhibition, which helped to avoid run-away excitation early in development:

\[ a'_i(t+1) = \left \{ \begin{array}{l l} a_i(t+1)
& \text{if } \sum_j{s_j(t)} \leqslant \zeta \\ 
 & \\
a_i(t+1)\Big/ \Big(1 + \big(\sum_j{s_j(t)} - \zeta\big) \cdot \exp(- t/\tau_e)\Big) 
& \text{otherwise.} \end{array} \right. \]

Here $a'_i(t)$ is the final, adjusted value of activation for every cell; $\sum_j{s_j(t)}$ is the total activity in the network at the previous time step; $\zeta$ is a constant that set the level of total activity at which inhibition "turns on", and that in our case was set to \mbox{$\zeta=$ 9} (the size of the grid). The exponent $\exp(-t/\tau_e)$ served as an "easing" function that gradually "eased" the network from inhibition-dominated mode of operation to "free" operation, with a time constant $\tau_e\approx$ 2000. This “easing” formula was a practical compromise that greatly sped up our computational experiments, as it dampened network activity early on, when it was still close to randomly connected, and so prone to seizure-like activity, but allowed the simulation run on its own later in development.

The activity of each neuron $s_i(t)$ was then calculated from its total activation $a'_i(t)$:

\[ s_i(t) = f_i\big(a'_i(t)\big) \]

using a logistic activation function: 

\[ f_i(a) = 1/\Big(1+\exp\big(c\cdot(h_i(t)-a)\big)\Big) \]

where $c$ is a steepness parameter, set at $c=$ 20, and $h_i(t)$ is the current spiking threshold of cell $i$. At the beginning of each simulation, spiking thresholds $h_i(0)$ were set to random values, uniformly distributed in a narrow band between $1/(n \hat{s_i})$ and $1/(n \hat{s_i}_i)+0.1$ . During the simulation, the thresholds $h_i(t)$ were updated at each time step, to model the effect of \textbf{intrinsic homeostatic plasticity}. For this purpose, for each cell, we kept track of its running average spiking rate $\bar{s_i}(t)$, and updated both average spiking rates and spiking thresholds $h_i(t)$ by the following formulas:

\[ \bar{s_i}(t+1) = (1-\kappa)\bar{s_i}(t) + \kappa s_i(t) \]

\[ h_i(t+1) = h_i(t) - r_h(\hat{s_i} - \bar{s_i}(t)) \]

where constant $\kappa=$ 0.05 controlled the rate of averaging, and constant $r_h=$ 0.1 set the strength of homeostatic plasticity, as it set the rate at which spiking thresholds $h_i$ were allowed to adjust, to bring the discrepancy between the target spiking rate $\hat{s_i}$ and running average spiking rate $\bar{s_i}(t)$ to zero.

Once spiking activity of each neuron at the new time step $s_i(t)$ was calculated, we moved to the \textbf{spike-time dependent plasticity} (STDP) step, adjusting synaptic weights $w_{ji}$ that linked neurons in the network. In continuous time, STDP leads to an increase in weight $w_{ji}$ (from $i$ to $j$) if target neuron $j$ spikes after a spike in source neuron $i$, with a delay that is expected for spike  propagation from $i$ to $j$. If the target neuron $j$ spikes earlier than that (before, or together with neuron $i$), the weight $w_{ji}$ is decreased. The amount of weight change smoothly drops off as the delay between these two spikes increases in either direction. 

In discrete time, assuming that spike propagation always takes one time step, and neglecting the effect of smooth drop-off, STDP may be approximated by the following system of cases, with options 1 and 2 not being mutually exclusive:

\[ w_{ji}(t+1) = \left \{ \begin{array}{lll} w_{ji}(t)+\epsilon, & \text{if } s_i(t)> 0 \text{ and } s_j(t+1)> 0 \\ w_{ji}(t)-\epsilon, & \text{if } s_i(t)> 0 \text{ and } s_j(t)> 0 \\ w_{ji}(t) & \text{if } s_i(t)=0\end{array} \right. \]

where $\epsilon$ is some change in synaptic weight. 

As in our model neuronal activity $s_i(t)$ was continuous, and we wanted the synaptic change $\epsilon$ to be proportional to the overlap in neuronal activity, the non-exclusive system above may be rewritten as:

\[ w_{ji}(t+1) = w_{ji}(t) + r_w \big(s_i(t)w_{ji}(t)s_j(t+1) - s_i(t)w_{ji}(t)s_j(t)\big) \]

or

\[ w_{ji}(t+1) = w_{ji}(t)\cdot\Big(1+r_w\big(s_j(t+1)-s_j(t)\big)s_i(t)\Big) \]

where $r_w$ is a constant that controls the strength of synaptic plasticity (in this model, $r_w=$ 0.25).

Finally, we modeled \textbf{synaptic competition} by introducing a negative feedback, to limit the total sum of all inputs to each neuron, and all output from each neuron. At every time step, we used the weight matrix $\textbf{W}$ to calculate a modified matrix $\textbf{W}^\text{I}$, with sums of \textit{inputs} to each neuron normalized to a certain fixed value ($g=$ 1.5), and a modified weight matrix $\textbf{W}^\text{O}$, in which the total sum of \textit{outputs} from each neuron was normalized to the same value: 

\[ w_{ji}^\text{I} = g \cdot w_{ji}/\sum_k{w_{jk}} \]
\[ w_{ji}^\text{O} = g \cdot w_{ji}/\sum_k{w_{ki}} \]

We then iteratively "moved" our actual weight matrix at each time step in the direction of the average between these two normalized matrices:

\[ w_{ji}(t+1) = 0.4 \cdot w_{ji} + 0.3 \cdot w^\text{I}_{ji} + 0.3 \cdot w^\text{O}_{ji} \]

This "sliding" approach to modeling synaptic competition was less aggressive than explicit weight normalization, and allowed for more robust model convergence.

Finally, developing networks were activated with \textbf{simulated visual stimuli} that resembled sensory activation a real animal could have experienced when navigating in a bright-lit environment with sparsely placed black spheres. For "general" visual stimulation (used in sensitivity experiments), we repeatedly created unique collision events, with randomized original positions of a black sphere relative to the eye, final distance to the eye, and direction of movement through the visual field. We would then move the projection of this virtual sphere across the virtual retina over a course of $\tau=$ 10 time frames.  When training on looming stimuli (main series of computational experiments), we still initiated objects at random points within the visual field, but made sure that they approached the eye on a "collision trajectory", and eventually covered the entirety of the visual field. For looming and "general visual" stimuli, a projection of a sphere on the virtual retina was a solid circle, with its center moving linearly $(x,y) = (x_0,y_0)+(v_x,v_y)\cdot t/\tau$, and circle radius changing as $R(t) = R_0/(d_0 - v_z \cdot t/\tau)$. The virtual retina consisted of 81 pixels, arranged in a 9x9 grid (same dimensions as for the model network), with each pixel generating both "ON" and "OFF" responses without delay or bursting, based on the difference between two consecutive projections $\text{in}(t) = \text{XOR}(\text{img}(t),\text{img}(t-1))$. This signal was then inputted to matching nodes in the model network. When training on noise, we generated random noise with 9 pixels flicking active at any given time. 

For testing, we compared responses to flashes, crashes, and scrambled stimuli. "Crashes" were different from looming stimuli in that the change of projection radius with time was linear $R(t) = 9/2 \cdot \sqrt{2} \cdot t/\tau$, rather than realistic; this was because we used linearly expending looming stimuli in biological experiments, both in this study, and in earlier studies \citep{khakhalin2014}. "Scrambled" stimuli were identical to "Crashes", but with all 81 pixels randomly reassigned. "Flashes" were modeled as very fast looming stimuli that took exactly 2 frames to fill the entire field of view, and with pixels reshuffled. We used this approximation instead of a simple instantaneous flash as our model was deterministic, and we needed to introduce some variability into responses to flashes, while still keeping them as close to instantaneous as possible. 

While testing networks trained on different sensory stimuli, we ran into a surprising complication: during training, different stimuli provided different levels of average activation, and so not only synaptic connections between cells were differently shaped, but also intrinsic plasticity resulted in very different activation thresholds for different neurons. This variability of excitability was however an artifact of our training method, and did not approximate real biological phenomena, as in real animals visual stimuli would happen relatively rarely, while we fed all our stimuli to the network as one intense train with no gaps. We therefore decided to let all spiking thresholds settle down before testing, to a state that was dependent only on synaptic connectivity, and not on recent stimulation history. We let the model develop for 2000 additional time steps, with only homeostatic plasticity rule on, but without STDP or synaptic scaling, while feeding neurons with Poisson random noise that activated on average 9 neurons at each time step. 

The effect of this additional adjustment step was so prominent in the model, that we hypothesize that it may be indirectly relevant for the biological tectum as well. To maintain the network of synaptic connections, each ensemble of synfire chains has to be regularly activated, yet the more active it is, the less excitable the neurons become, making them less likely to "win" during competition with other ensembles during stimulus detection. The dynamics of plasticity in the brain would therefore pose a meta-balancing problem \citep{zenke2017temporal}: if intrinsic plasticity is too flexible, networks that detects unusual stimuli, in the absence of these stimuli, would become overly excitable, producing high false-positive rate. Conversely, they will quickly habituate to actual stimuli, but at the same time, they will have no trouble maintaining synaptic connections required for stimulus detection \citep{litwin2014assemblies}. If however intrinsic plasticity is too slow, detection networks will find it easier maintaining "optimal" levels of sensitivity, but may have trouble keeping synaptic connections between stimulus presentations intact \citep{triplett2018emergence}, as in the absence of spontaneous replay these synaptic connections may erode. A potential solution to this problem is to cycle the network through distinct phases, with different contributions of synaptic and intrinsic plasticity, similar to how we did it in the model.

For \textbf{sensitivity analysis}, we either removed or greatly attenuated parts of the model, one part at a time (not cumulatively). We tried the following combinations:

\textbf{Non-looming stimuli}. In this mode, instead of training the model exclusively on looming stimuli, we used a mix of randomized transitions of a black circle across the retina, as describe above. This type of stimulation was therefore still spatially patterned, but consisted mostly of translational stimuli, with some contribution of oblique looming and oblique receding stimuli.

\textbf{Random stimulation}. The network was stimulated with random noise. Each “pixels” of the image would fire with the same probability.

\textbf{No STDP plasticity}. Instead of the spike-time-dependnet plasticity equation describe above:

\[ w_{ji}(t+1) = w_{ji}(t)\cdot\Big(1+r_w \cdot \big(s_j(t+1)-s_j(t)\big)\cdot s_i(t)\Big) \]

we used an equation with symmetrical Hebb plasticity, and no negative depression term: 

\[ w_{ji}(t+1) = w_{ji}(t)\cdot\Big(1+r_w \cdot s_j(t+1) \cdot s_i(t)\Big) \]

\textbf{No synaptic competition}. Instead of sliding renormalization of all inputs and outputs of each neuron, we allowed synaptic weights to decay to zero: $w_{ji}(t+1) = w_{ji}(t)\cdot (1-\beta)$, where $\beta$ = 0.001.

\textbf{Weak homeostatic plasticity}. In the formula for homeostatic plasticity, instead of change coefficient $r_h$=0.1 we used $r_h$=0.01.

\section*{Acknowledgements}

My greatest gratitude is to Carlos Aizenman who encouraged me to try to publish this work as a single author, even though all experiments described here were performed on his equipment, and the materials were paid for by the money from his grant (NSF IOS-1353044). I also thank Heng Xu (Shanghai Jiao Tong University) for his advice on imaging experiments; Joshua Vogelstein (John Hopkins) for his help with adaptive thresholding; Petko Bogdanov (SUNY Albany), Csilla Szabo (Skidmore College), Gerrit Ansmann (Bonn University), and Jim Belk (St Andrews University) for their help with network science and graph theory, and Sven Anderson (Bard College) for advice on model analysis.

% \section*{Declaration of Interests}

The author has no conflicts of interest to disclose.

\nolinenumbers
\bibliographystyle{apalike} % For author-year
%\bibliographystyle{unsrtnat} % For Nature-style
\bibliography{refs}

%TC:endignore
\end{document}